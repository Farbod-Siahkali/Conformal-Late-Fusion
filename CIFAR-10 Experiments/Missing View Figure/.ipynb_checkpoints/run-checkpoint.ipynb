{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd082fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Config:\n",
      "CIFARConfig(alpha=0.1, K0=6, num_classes=10, num_simulations=10, epochs_per_view=40, lr=0.001, batch_size=512, max_iter_lr=1000, train_seed_base=42, train_frac=0.5, cal_frac_of_temp=0.3, fuse_train_frac_of_rest=0.7, viewdrop_min_missing=0, viewdrop_max_missing=5, viewdrop_prob=0.7, eval_min_missing=0, max_missing_to_eval=4, patterns_per_m=6, out_dir='results_missing_views')\n",
      "\n",
      "=== Simulation 1/10 ===\n",
      "  [View 1/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 2/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 3/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 4/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 5/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 6/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  -> Missing 0 (keep 6) patterns: 1\n",
      "  -> Missing 1 (keep 5) patterns: 6\n",
      "  -> Missing 2 (keep 4) patterns: 6\n",
      "  -> Missing 3 (keep 3) patterns: 6\n",
      "  -> Missing 4 (keep 2) patterns: 6\n",
      "Reference (avg probs) accuracy: 46.57%\n",
      "\n",
      "=== Simulation 2/10 ===\n",
      "  [View 1/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 2/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 3/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 4/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 5/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 6/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  -> Missing 0 (keep 6) patterns: 1\n",
      "  -> Missing 1 (keep 5) patterns: 6\n",
      "  -> Missing 2 (keep 4) patterns: 6\n",
      "  -> Missing 3 (keep 3) patterns: 6\n",
      "  -> Missing 4 (keep 2) patterns: 6\n",
      "Reference (avg probs) accuracy: 45.89%\n",
      "\n",
      "=== Simulation 3/10 ===\n",
      "  [View 1/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 2/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 3/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 4/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 5/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 6/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  -> Missing 0 (keep 6) patterns: 1\n",
      "  -> Missing 1 (keep 5) patterns: 6\n",
      "  -> Missing 2 (keep 4) patterns: 6\n",
      "  -> Missing 3 (keep 3) patterns: 6\n",
      "  -> Missing 4 (keep 2) patterns: 6\n",
      "Reference (avg probs) accuracy: 46.21%\n",
      "\n",
      "=== Simulation 4/10 ===\n",
      "  [View 1/6] training...\n",
      "  epoch 20/40\n",
      "  epoch 40/40\n",
      "  [View 2/6] training...\n",
      "  epoch 20/40\n"
     ]
    }
   ],
   "source": [
    "# Jupyter-friendly: CIFAR-10 Multi-View CP with Missing-View Sweep (CLF vs baselines)\n",
    "# - Per-subset recalibration to keep CLF coverage valid\n",
    "# - View-drop augmentation for robustness to missing views\n",
    "# - Saves CSVs + a two-panel figure (Coverage / Set Size) using scienceplots\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "from __future__ import annotations\n",
    "import warnings, itertools, os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import scienceplots  # noqa: F401\n",
    "plt.style.use(['science', 'no-latex'])\n",
    "\n",
    "# -----------------------------\n",
    "# Config dataclass\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class CIFARConfig:\n",
    "    alpha: float = 0.1\n",
    "    K0: int = 6                      # choose 4 or 6\n",
    "    num_classes: int = 10\n",
    "    num_simulations: int = 1         # increase for variance bars\n",
    "    # training\n",
    "    epochs_per_view: int = 40        # raise if you have GPU time\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 512\n",
    "    max_iter_lr: int = 1000\n",
    "    train_seed_base: int = 42\n",
    "    # data split fractions\n",
    "    train_frac: float = 0.5\n",
    "    cal_frac_of_temp: float = 0.3\n",
    "    fuse_train_frac_of_rest: float = 0.7\n",
    "    # fusion training robustness (augmentation)\n",
    "    viewdrop_min_missing: int = 0\n",
    "    viewdrop_max_missing: int = 3    # <= K0-1\n",
    "    viewdrop_prob: float = 0.7\n",
    "    # evaluation sweep\n",
    "    eval_min_missing: int = 0        # set 1 to skip the no-missing case\n",
    "    max_missing_to_eval: int = None  # None -> up to K0-1\n",
    "    patterns_per_m: int = 6          # random subsets per missing count m\n",
    "    # IO\n",
    "    out_dir: str = \"results_missing_views\"\n",
    "\n",
    "# -----------------------------\n",
    "# Device & Data (Loaded once)\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=_transform)\n",
    "test_dataset  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=_transform)\n",
    "\n",
    "X_train_full = train_dataset.data.astype(np.float32) / 255.0\n",
    "Y_train_full = np.array(train_dataset.targets)\n",
    "X_test_full  = test_dataset.data.astype(np.float32) / 255.0\n",
    "Y_test_full  = np.array(test_dataset.targets)\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-view (patch) utilities\n",
    "# -----------------------------\n",
    "def split_image_into_k_patches(image: torch.Tensor, k: int) -> List[torch.Tensor]:\n",
    "    C, H, W = image.shape\n",
    "    if k == 4:\n",
    "        patches = []\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                patch = image[:, i*16:(i+1)*16, j*16:(j+1)*16]\n",
    "                patches.append(patch)\n",
    "        return patches\n",
    "    else:\n",
    "        base_width = W // k\n",
    "        remainder = W % k\n",
    "        patches, start = [], 0\n",
    "        for idx in range(k):\n",
    "            width = base_width + (1 if idx < remainder else 0)\n",
    "            patch = image[:, :, start:start+width]\n",
    "            patches.append(patch)\n",
    "            start += width\n",
    "        return patches\n",
    "\n",
    "class PatchesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images: np.ndarray, labels: np.ndarray, k: int, view: int):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.k = k\n",
    "        self.view = view\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx: int):\n",
    "        img = self.images[idx].transpose((2, 0, 1))\n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "        patch = split_image_into_k_patches(img, self.k)[self.view]\n",
    "        return patch, int(self.labels[idx])\n",
    "\n",
    "# -----------------------------\n",
    "# Simple CNN per view\n",
    "# -----------------------------\n",
    "class PredictorCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.num_classes = num_classes\n",
    "    def forward_features(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        return x\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.fc1 is None:\n",
    "            b, c, h, w = x.shape\n",
    "            self.fc1 = nn.Linear(c*h*w, 128).to(x.device)\n",
    "            self.fc2 = nn.Linear(128, self.num_classes).to(x.device)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def train_model(model: nn.Module, train_loader, num_epochs=100, lr=1e-3):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt  = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    for ep in range(num_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = torch.tensor(yb, dtype=torch.long, device=device)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if (ep + 1) % 20 == 0 or ep == num_epochs - 1:\n",
    "            print(f\"  epoch {ep+1}/{num_epochs}\")\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Conformal utilities\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def compute_nonconformity_scores(model: nn.Module, loader) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    scores, labels = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        probs = F.softmax(model(xb), dim=1)\n",
    "        idx = torch.arange(probs.size(0), device=probs.device)\n",
    "        true_p = probs[idx, torch.tensor(yb, dtype=torch.long, device=probs.device)]\n",
    "        s = (1 - true_p).detach().cpu().numpy()\n",
    "        scores.extend(s)\n",
    "        labels.extend(yb.numpy())\n",
    "    return np.asarray(scores, float), np.asarray(labels, int)\n",
    "\n",
    "def classwise_scores(scores: np.ndarray, labels: np.ndarray, L: int) -> Dict[int, np.ndarray]:\n",
    "    out = {c: [] for c in range(L)}\n",
    "    for s, y in zip(scores, labels):\n",
    "        out[int(y)].append(float(s))\n",
    "    return {c: np.asarray(v, float) for c, v in out.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def per_view_pvalues_and_probs(\n",
    "    model: nn.Module, class_scores: Dict[int, np.ndarray], loader, L: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    probs_all = []\n",
    "    for xb, _ in loader:\n",
    "        xb = xb.to(device)\n",
    "        probs = F.softmax(model(xb), dim=1).detach().cpu().numpy()\n",
    "        probs_all.append(probs)\n",
    "    probs_all = np.vstack(probs_all)  # (n, L)\n",
    "    n = probs_all.shape[0]\n",
    "    pvals = np.zeros((n, L))\n",
    "    for y in range(L):\n",
    "        cal = class_scores.get(y, np.array([]))\n",
    "        if cal.size == 0:\n",
    "            pvals[:, y] = 1.0\n",
    "        else:\n",
    "            s_test = 1 - probs_all[:, y]\n",
    "            counts = np.sum(cal[:, None] >= s_test[None, :], axis=0)\n",
    "            pvals[:, y] = (1 + counts) / (len(cal) + 1)\n",
    "    return pvals, probs_all\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_probs(model: nn.Module, loader) -> np.ndarray:\n",
    "    model.eval()\n",
    "    probs_all = []\n",
    "    for xb, _ in loader:\n",
    "        xb = xb.to(device)\n",
    "        probs = F.softmax(model(xb), dim=1).detach().cpu().numpy()\n",
    "        probs_all.append(probs)\n",
    "    return np.vstack(probs_all)\n",
    "\n",
    "# -----------------------------\n",
    "# Fusion utilities & baselines\n",
    "# -----------------------------\n",
    "def build_fusion_features_with_masks(pvals_list: List[np.ndarray], probs_list: List[np.ndarray], masks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Stack per-view [pvals, probs, mask] blocks. masks shape: (n, K) with 1 for present, 0 for missing.\"\"\"\n",
    "    blocks = []\n",
    "    K = len(pvals_list)\n",
    "    for k in range(K):\n",
    "        n, L = pvals_list[k].shape\n",
    "        mk = masks[:, [k]]  # (n,1)\n",
    "        mask_block = np.repeat(mk, repeats=2*L, axis=1)\n",
    "        blk = np.hstack([pvals_list[k], probs_list[k]])\n",
    "        blk = blk * mask_block\n",
    "        blk = np.hstack([blk, mk])  # add 1 indicator per view\n",
    "        blocks.append(blk)\n",
    "    return np.hstack(blocks)\n",
    "\n",
    "def min_p_value_fusion(P_all_subset: np.ndarray) -> np.ndarray:\n",
    "    Kp = P_all_subset.shape[0]\n",
    "    return Kp * np.min(P_all_subset, axis=0)\n",
    "\n",
    "def fisher_fusion(P_all_subset: np.ndarray) -> np.ndarray:\n",
    "    eps = 1e-12\n",
    "    p = np.clip(P_all_subset, eps, 1.0)\n",
    "    T = -2 * np.sum(np.log(p), axis=0)\n",
    "    df = 2 * P_all_subset.shape[0]\n",
    "    return 1 - chi2.cdf(T, df=df)\n",
    "\n",
    "def adjusted_fisher_params(P_train_subset: np.ndarray, y_train: np.ndarray, L: int):\n",
    "    Kp, ntr, _ = P_train_subset.shape\n",
    "    eps = 1e-12\n",
    "    params = {}\n",
    "    for y in range(L):\n",
    "        idx = np.where(y_train == y)[0]\n",
    "        if idx.size < 5:\n",
    "            params[y] = (\"fallback\", None, None)\n",
    "            continue\n",
    "        P_cls = np.clip(P_train_subset[:, idx, y], eps, 1.0)\n",
    "        W = -2 * np.log(P_cls)\n",
    "        Wc = W - W.mean(axis=1, keepdims=True)\n",
    "        Sigma = (Wc @ Wc.T) / max(W.shape[1] - 1, 1)\n",
    "        var_T = np.sum(Sigma)\n",
    "        if not np.isfinite(var_T) or var_T <= 0:\n",
    "            var_T = 4 * Kp\n",
    "        f_y = (8.0 * Kp * Kp) / var_T\n",
    "        c_y = var_T / (4 * Kp)\n",
    "        params[y] = (\"ok\", f_y, c_y)\n",
    "    return params\n",
    "\n",
    "def adjusted_fisher_fusion_subset(P_train_subset: np.ndarray, y_train: np.ndarray, P_test_subset: np.ndarray, L: int) -> np.ndarray:\n",
    "    eps = 1e-12\n",
    "    nte = P_test_subset.shape[1]\n",
    "    out = np.zeros((nte, L))\n",
    "    params = adjusted_fisher_params(P_train_subset, y_train, L)\n",
    "    for y in range(L):\n",
    "        status, f_y, c_y = params[y]\n",
    "        if status != \"ok\":\n",
    "            out[:, y] = fisher_fusion(P_test_subset)[:, y]\n",
    "            continue\n",
    "        P_t = np.clip(P_test_subset[:, :, y], eps, 1.0)\n",
    "        T_t = -2 * np.sum(np.log(P_t), axis=0)\n",
    "        out[:, y] = 1 - chi2.cdf(T_t / c_y, df=f_y)\n",
    "    return out\n",
    "\n",
    "def weighted_average_fusion_subset(P_all_subset: np.ndarray, y_train: np.ndarray):\n",
    "    \"\"\"Returns (apply_fn, weights). apply_fn(P_subset)->(n,L)\"\"\"\n",
    "    Kp, ntr, L = P_all_subset.shape\n",
    "    pv_tr_concat = np.concatenate([P_all_subset[k] for k in range(Kp)], axis=1)  # (ntr, K'*L)\n",
    "    lr = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=500, random_state=0)\n",
    "    lr.fit(pv_tr_concat, y_train)\n",
    "    B = lr.coef_  # (L, K'*L)\n",
    "    imps = []\n",
    "    for k in range(Kp):\n",
    "        block = B[:, k*L:(k+1)*L]\n",
    "        imps.append(np.linalg.norm(block, ord=\"fro\"))\n",
    "    w = np.array(imps, float)\n",
    "    w = np.maximum(w, 1e-12)\n",
    "    w = w / w.sum()\n",
    "    def apply_weights(P_subset):\n",
    "        return np.tensordot(w, P_subset, axes=(0,0))  # (n,L)\n",
    "    return apply_weights, w\n",
    "\n",
    "def fused_class_cal_scores(y_cal: np.ndarray, fused_probs_cal: np.ndarray, L: int) -> Dict[int, np.ndarray]:\n",
    "    s = 1 - fused_probs_cal[np.arange(len(y_cal)), y_cal]\n",
    "    out = {c: [] for c in range(L)}\n",
    "    for sc, yy in zip(s, y_cal):\n",
    "        out[int(yy)].append(float(sc))\n",
    "    return {c: np.asarray(v, float) for c, v in out.items()}\n",
    "\n",
    "def fused_p_values_from_cal(fused_probs: np.ndarray, cal_class_scores: Dict[int, np.ndarray]) -> np.ndarray:\n",
    "    n, L = fused_probs.shape\n",
    "    out = np.zeros((n, L))\n",
    "    for y in range(L):\n",
    "        cal = cal_class_scores.get(y, np.array([]))\n",
    "        if cal.size == 0:\n",
    "            out[:, y] = 1.0\n",
    "        else:\n",
    "            s_test = 1 - fused_probs[:, y]\n",
    "            counts = np.sum(cal[:, None] >= s_test[None, :], axis=0)\n",
    "            out[:, y] = (1 + counts) / (len(cal) + 1)\n",
    "    return out\n",
    "\n",
    "def evaluate_sets(P: np.ndarray, y_true: np.ndarray, alpha: float) -> Tuple[float, float]:\n",
    "    C = (P > alpha)\n",
    "    cov = float(np.mean(C[np.arange(len(y_true)), y_true]))\n",
    "    size = float(np.mean(C.sum(axis=1)))\n",
    "    return cov, size\n",
    "\n",
    "# -----------------------------\n",
    "# View-drop augmentation helpers\n",
    "# -----------------------------\n",
    "def random_missing_mask(n: int, K: int, min_missing: int, max_missing: int, prob_apply: float, rng: np.random.RandomState):\n",
    "    mask = np.ones((n, K), dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        if rng.rand() < prob_apply:\n",
    "            m = rng.randint(min_missing, max_missing+1)\n",
    "            miss = rng.choice(K, size=m, replace=False)\n",
    "            mask[i, miss] = 0.0\n",
    "    return mask\n",
    "\n",
    "def apply_mask_to_perview_arrays(arr_list: List[np.ndarray], mask: np.ndarray) -> List[np.ndarray]:\n",
    "    K = len(arr_list)\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        arr = arr_list[k].copy()\n",
    "        arr[mask[:,k] == 0] = 0.0\n",
    "        out.append(arr)\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# Main experiment with missing-view sweep\n",
    "# -----------------------------\n",
    "def run_missing_view_sweep(cfg: CIFARConfig):\n",
    "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "    all_rows = []\n",
    "    for sim in range(cfg.num_simulations):\n",
    "        print(f\"\\n=== Simulation {sim+1}/{cfg.num_simulations} ===\")\n",
    "        seed = cfg.train_seed_base + sim\n",
    "        rng  = np.random.RandomState(seed)\n",
    "\n",
    "        # Splits\n",
    "        X_trP, X_tmp, y_trP, y_tmp = train_test_split(\n",
    "            X_train_full, Y_train_full,\n",
    "            test_size=1 - cfg.train_frac, stratify=Y_train_full, random_state=seed\n",
    "        )\n",
    "        X_cal, X_rest, y_cal, y_rest = train_test_split(\n",
    "            X_tmp, y_tmp,\n",
    "            test_size=1 - cfg.cal_frac_of_temp, stratify=y_tmp, random_state=seed\n",
    "        )\n",
    "        X_fuse_tr, X_fuse_cal, y_fuse_tr, y_fuse_cal = train_test_split(\n",
    "            X_rest, y_rest,\n",
    "            test_size=1 - cfg.fuse_train_frac_of_rest, stratify=y_rest, random_state=seed\n",
    "        )\n",
    "        X_te, y_te = X_test_full, Y_test_full\n",
    "\n",
    "        K0 = cfg.K0\n",
    "        num_views = 4 if K0 == 4 else K0\n",
    "\n",
    "        # Build loaders per view\n",
    "        loaders = {}\n",
    "        for v in range(num_views):\n",
    "            loaders[v] = dict(\n",
    "                train=torch.utils.data.DataLoader(PatchesDataset(X_trP,      y_trP,      K0, v), batch_size=cfg.batch_size, shuffle=True),\n",
    "                cal  =torch.utils.data.DataLoader(PatchesDataset(X_cal,      y_cal,      K0, v), batch_size=cfg.batch_size, shuffle=False),\n",
    "                ftr  =torch.utils.data.DataLoader(PatchesDataset(X_fuse_tr,  y_fuse_tr,  K0, v), batch_size=cfg.batch_size, shuffle=False),\n",
    "                fcal =torch.utils.data.DataLoader(PatchesDataset(X_fuse_cal, y_fuse_cal, K0, v), batch_size=cfg.batch_size, shuffle=False),\n",
    "                te   =torch.utils.data.DataLoader(PatchesDataset(X_te,       y_te,       K0, v), batch_size=cfg.batch_size, shuffle=False),\n",
    "            )\n",
    "\n",
    "        # Train per-view models\n",
    "        models, cal_classwise = [], []\n",
    "        for v in range(num_views):\n",
    "            print(f\"  [View {v+1}/{num_views}] training...\")\n",
    "            m = PredictorCNN(num_classes=cfg.num_classes)\n",
    "            m = train_model(m, loaders[v][\"train\"], num_epochs=cfg.epochs_per_view, lr=cfg.lr)\n",
    "            models.append(m)\n",
    "            sc, lab = compute_nonconformity_scores(m, loaders[v][\"cal\"])\n",
    "            cal_classwise.append(classwise_scores(sc, lab, cfg.num_classes))\n",
    "\n",
    "        # Per-view p/probs for fusion train/cal/test (full K0)\n",
    "        pv_tr, pr_tr = [], []\n",
    "        pv_cal, pr_cal = [], []\n",
    "        pv_te,  pr_te  = [], []\n",
    "        for v in range(num_views):\n",
    "            p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"ftr\"], cfg.num_classes)\n",
    "            pv_tr.append(p); pr_tr.append(pr)\n",
    "            p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"fcal\"], cfg.num_classes)\n",
    "            pv_cal.append(p); pr_cal.append(pr)\n",
    "            p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"te\"],  cfg.num_classes)\n",
    "            pv_te.append(p);  pr_te.append(pr)\n",
    "\n",
    "        n_ftr = pv_tr[0].shape[0]\n",
    "        n_fcal = pv_cal[0].shape[0]\n",
    "        n_te = pv_te[0].shape[0]\n",
    "        L = cfg.num_classes\n",
    "\n",
    "        # === Train CLF (LogisticRegression) with view-drop augmentation ===\n",
    "        mask_train = random_missing_mask(\n",
    "            n=n_ftr, K=K0,\n",
    "            min_missing=cfg.viewdrop_min_missing,\n",
    "            max_missing=min(cfg.viewdrop_max_missing, K0-1),\n",
    "            prob_apply=cfg.viewdrop_prob, rng=rng\n",
    "        )\n",
    "        X_ftr = build_fusion_features_with_masks(\n",
    "            apply_mask_to_perview_arrays(pv_tr, mask_train),\n",
    "            apply_mask_to_perview_arrays(pr_tr, mask_train),\n",
    "            mask_train\n",
    "        )\n",
    "        fusion_lr = LogisticRegression(max_iter=cfg.max_iter_lr, multi_class=\"multinomial\",\n",
    "                                       solver=\"lbfgs\", random_state=seed)\n",
    "        fusion_lr.fit(X_ftr, y_fuse_tr)\n",
    "\n",
    "        # Precompute full training P for baselines\n",
    "        P_train_full = np.stack(pv_tr, axis=0)   # (K0, ntr, L)\n",
    "        P_test_full  = np.stack(pv_te,  axis=0)  # (K0, nte, L)\n",
    "\n",
    "        # ---- helper: evaluate a subset with PER-SUBSET RECALIBRATION ----\n",
    "        def eval_on_subset(keep_views: List[int], tag_missing: int):\n",
    "            \"\"\"\n",
    "            Recalibrate the fused model on the same subset/mask to keep exchangeability.\n",
    "            Baselines already recomputed on subset.\n",
    "            \"\"\"\n",
    "            Kp = len(keep_views)\n",
    "\n",
    "            # (1) Build masked CALIB features for THIS subset\n",
    "            mask_cal_sub = np.zeros((n_fcal, K0), dtype=np.float32)\n",
    "            mask_cal_sub[:, keep_views] = 1.0\n",
    "            X_fcal_sub = build_fusion_features_with_masks(\n",
    "                [pv_cal[k] for k in range(K0)],\n",
    "                [pr_cal[k] for k in range(K0)],\n",
    "                mask_cal_sub\n",
    "            )\n",
    "            fused_probs_cal_sub = fusion_lr.predict_proba(X_fcal_sub)\n",
    "            fused_cal_scores_sub = fused_class_cal_scores(y_fuse_cal, fused_probs_cal_sub, L)\n",
    "\n",
    "            # (2) Build masked TEST features for THIS subset\n",
    "            mask_te = np.zeros((n_te, K0), dtype=np.float32)\n",
    "            mask_te[:, keep_views] = 1.0\n",
    "            X_ftest = build_fusion_features_with_masks(\n",
    "                [pv_te[k] for k in range(K0)],\n",
    "                [pr_te[k] for k in range(K0)],\n",
    "                mask_te\n",
    "            )\n",
    "            fused_probs_test = fusion_lr.predict_proba(X_ftest)\n",
    "            P_cf = fused_p_values_from_cal(fused_probs_test, fused_cal_scores_sub)\n",
    "\n",
    "            # (3) Baselines on subset\n",
    "            P_train_sub = P_train_full[keep_views, :, :]  # (Kp, ntr, L)\n",
    "            P_test_sub  = P_test_full[keep_views,  :, :]\n",
    "            P_min  = min_p_value_fusion(P_test_sub)\n",
    "            P_fi   = fisher_fusion(P_test_sub)\n",
    "            P_adjF = adjusted_fisher_fusion_subset(P_train_sub, y_fuse_tr, P_test_sub, L)\n",
    "            apply_wavg, w_learned = weighted_average_fusion_subset(P_train_sub, y_fuse_tr)\n",
    "            P_wavg = apply_wavg(P_test_sub)\n",
    "\n",
    "            # (4) Metrics\n",
    "            cov_cf, set_cf = evaluate_sets(P_cf, y_te, cfg.alpha)\n",
    "            cov_min, set_min = evaluate_sets(P_min, y_te, cfg.alpha)\n",
    "            cov_fi,  set_fi  = evaluate_sets(P_fi,  y_te, cfg.alpha)\n",
    "            cov_afi, set_afi = evaluate_sets(P_adjF,y_te, cfg.alpha)\n",
    "            cov_wl,  set_wl  = evaluate_sets(P_wavg,y_te, cfg.alpha)\n",
    "\n",
    "            row = dict(\n",
    "                Sim=sim, K0=K0, Kept=Kp, Missing=tag_missing, KeepViews=str(keep_views),\n",
    "                CLF_Cov=100*cov_cf, CLF_Set=set_cf,\n",
    "                MinPV_Cov=100*cov_min, MinPV_Set=set_min,\n",
    "                Fisher_Cov=100*cov_fi, Fisher_Set=set_fi,\n",
    "                AdjF_Cov=100*cov_afi, AdjF_Set=set_afi,\n",
    "                WAvgL_Cov=100*cov_wl, WAvgL_Set=set_wl\n",
    "            )\n",
    "            return row\n",
    "\n",
    "        # Sweep missing views\n",
    "        start_m = cfg.eval_min_missing\n",
    "        max_m = cfg.max_missing_to_eval if cfg.max_missing_to_eval is not None else (K0-1)\n",
    "        max_m = min(max_m, K0-1)\n",
    "        rng2 = np.random.RandomState(seed+12345)\n",
    "        for m in range(start_m, max_m+1):\n",
    "            Kp = K0 - m\n",
    "            all_subsets = list(itertools.combinations(range(K0), Kp))\n",
    "            if len(all_subsets) > cfg.patterns_per_m:\n",
    "                keep_sets = rng2.choice(len(all_subsets), size=cfg.patterns_per_m, replace=False)\n",
    "                subsets = [list(all_subsets[i]) for i in keep_sets]\n",
    "            else:\n",
    "                subsets = [list(s) for s in all_subsets]\n",
    "            print(f\"  -> Missing {m} (keep {Kp}) patterns: {len(subsets)}\")\n",
    "            for keep in subsets:\n",
    "                row = eval_on_subset(keep, m)\n",
    "                all_rows.append(row)\n",
    "\n",
    "        # ref accuracy (full K0)\n",
    "        avg_probs = np.mean(np.stack(pr_te, axis=0), axis=0)\n",
    "        acc_ref = accuracy_score(y_te, np.argmax(avg_probs, axis=1)) * 100.0\n",
    "        print(f\"Reference (avg probs) accuracy: {acc_ref:.2f}%\")\n",
    "\n",
    "        # cleanup\n",
    "        for m in models:\n",
    "            m.to(\"cpu\")\n",
    "        del models\n",
    "\n",
    "    df = pd.DataFrame(all_rows).sort_values([\"Sim\", \"Missing\", \"Kept\"])\n",
    "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "    out_csv = os.path.join(cfg.out_dir, f\"missing_view_sweep_K{cfg.K0}_revised.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    # quick summaries\n",
    "    summary = df.groupby([\"Missing\"]).agg({\n",
    "        \"CLF_Cov\":\"mean\",\"CLF_Set\":\"mean\",\n",
    "        \"MinPV_Cov\":\"mean\",\"MinPV_Set\":\"mean\",\n",
    "        \"Fisher_Cov\":\"mean\",\"Fisher_Set\":\"mean\",\n",
    "        \"AdjF_Cov\":\"mean\",\"AdjF_Set\":\"mean\",\n",
    "        \"WAvgL_Cov\":\"mean\",\"WAvgL_Set\":\"mean\",\n",
    "    }).reset_index()\n",
    "    sum_csv = os.path.join(cfg.out_dir, f\"missing_view_summary_K{cfg.K0}_revised.csv\")\n",
    "    summary.to_csv(sum_csv, index=False)\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(f\" - {out_csv}\")\n",
    "    print(f\" - {sum_csv}\")\n",
    "    print(\"\\nSummary (means across patterns per #missing):\")\n",
    "    print(summary.to_string(index=False))\n",
    "\n",
    "    # ---- Plot + display inline ----\n",
    "    plot_missing_view_summary(k0=cfg.K0, csv_dir=cfg.out_dir)\n",
    "    plt.show()\n",
    "\n",
    "def plot_missing_view_summary(k0=6, csv_dir=\"results_missing_views\", out_prefix=None):\n",
    "    csv_path = os.path.join(csv_dir, f\"missing_view_sweep_K{k0}_revised.csv\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # metrics to plot\n",
    "    cov_cols = [\"CLF_Cov\",\"MinPV_Cov\",\"Fisher_Cov\",\"AdjF_Cov\",\"WAvgL_Cov\"]\n",
    "    set_cols = [\"CLF_Set\",\"MinPV_Set\",\"Fisher_Set\",\"AdjF_Set\",\"WAvgL_Set\"]\n",
    "\n",
    "    # group by Missing\n",
    "    g = df.groupby(\"Missing\")\n",
    "    mean_cov = g[cov_cols].mean()\n",
    "    std_cov  = g[cov_cols].std().fillna(0.0)\n",
    "    mean_set = g[set_cols].mean()\n",
    "    std_set  = g[set_cols].std().fillna(0.0)\n",
    "\n",
    "    xs = mean_cov.index.values\n",
    "\n",
    "    # pretty labels\n",
    "    label_map = {\n",
    "        \"CLF\": \"CLF (ours)\",\n",
    "        \"MinPV\": \"Min p-Value\",\n",
    "        \"Fisher\": \"Fisher\",\n",
    "        \"AdjF\": \"Adjusted Fisher\",\n",
    "        \"WAvgL\": \"Weighted Avg (learned)\"\n",
    "    }\n",
    "\n",
    "    # order for legend/lines\n",
    "    order_cov = [\"CLF_Cov\",\"AdjF_Cov\",\"MinPV_Cov\",\"WAvgL_Cov\",\"Fisher_Cov\"]\n",
    "    order_set = [\"CLF_Set\",\"AdjF_Set\",\"MinPV_Set\",\"WAvgL_Set\",\"Fisher_Set\"]\n",
    "\n",
    "    styles = {\n",
    "        \"CLF\":   dict(marker=\"o\"),\n",
    "        \"AdjF\":  dict(marker=\"s\"),\n",
    "        \"MinPV\": dict(marker=\"^\"),\n",
    "        \"WAvgL\": dict(marker=\"v\"),\n",
    "        \"Fisher\":dict(marker=\"D\"),\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.5), constrained_layout=True)\n",
    "\n",
    "    # --- Coverage subplot ---\n",
    "    ax = axes[0]\n",
    "    for key in order_cov:\n",
    "        base = key.replace(\"_Cov\",\"\")\n",
    "        lbl = label_map[base]\n",
    "        style = styles.get(base, {})\n",
    "        ax.plot(xs, mean_cov[key].values, label=lbl, **style)\n",
    "        ax.fill_between(xs,\n",
    "                        (mean_cov[key] - std_cov[key]).values,\n",
    "                        (mean_cov[key] + std_cov[key]).values,\n",
    "                        alpha=0.15)\n",
    "    ax.axhline(90.0, linestyle=\"--\", linewidth=1, label=\"Target 1-α\", alpha=0.7)\n",
    "    ax.set_xlabel(\"# Missing Views\")\n",
    "    ax.set_ylabel(\"Coverage (%)\")\n",
    "    ax.set_title(f\"Coverage vs. Missing Views (K0={k0})\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(frameon=True)\n",
    "\n",
    "    # --- Set size subplot ---\n",
    "    ax = axes[1]\n",
    "    for key in order_set:\n",
    "        base = key.replace(\"_Set\",\"\")\n",
    "        lbl = label_map[base]\n",
    "        style = styles.get(base, {})\n",
    "        ax.plot(xs, mean_set[key].values, label=lbl, **style)\n",
    "        ax.fill_between(xs,\n",
    "                        (mean_set[key] - std_set[key]).values,\n",
    "                        (mean_set[key] + std_set[key]).values,\n",
    "                        alpha=0.15)\n",
    "        ax.set_ylim(bottom=0)\n",
    "    ax.set_xlabel(\"# Missing Views\")\n",
    "    ax.set_ylabel(\"Average Set Size\")\n",
    "    ax.set_title(f\"Set Size vs. Missing Views (K0={k0})\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(frameon=True)\n",
    "\n",
    "    if out_prefix is None:\n",
    "        out_prefix = os.path.join(csv_dir, f\"fig_missing_view_sweep_K{k0}_revised\")\n",
    "    png_path = out_prefix + \".png\"\n",
    "    pdf_path = out_prefix + \".pdf\"\n",
    "    plt.savefig(png_path, dpi=220)\n",
    "    plt.savefig(pdf_path)\n",
    "    print(f\"Saved figure: {png_path}\")\n",
    "    print(f\"Saved figure: {pdf_path}\")\n",
    "\n",
    "# ===========================\n",
    "# Configure & Run (Notebook)\n",
    "# ===========================\n",
    "# You can edit these, then re-run the cell:\n",
    "K_LIST = [6]            # set to [4], [6], or [4,6] to run both sequentially\n",
    "EPOCHS = 40             # lower for quick dry run (e.g., 10); higher for better accuracy\n",
    "SIMS   = 10              # increase for error bars\n",
    "EVAL_MIN_MISSING = 0    # start sweep at 1 missing view; set 0 to include the no-missing point\n",
    "EVAL_MAX_MISSING = 4    # end sweep at 3 missing views; set to None for up to K0-1\n",
    "PATTERNS_PER_M  = 6     # random subsets per missing-count\n",
    "\n",
    "for K0 in K_LIST:\n",
    "    cfg = CIFARConfig(\n",
    "        K0=K0,\n",
    "        epochs_per_view=EPOCHS,\n",
    "        num_simulations=SIMS,\n",
    "        eval_min_missing=EVAL_MIN_MISSING,\n",
    "        max_missing_to_eval=EVAL_MAX_MISSING,\n",
    "        patterns_per_m=PATTERNS_PER_M,\n",
    "        viewdrop_min_missing=0,\n",
    "        viewdrop_max_missing=min(5, K0-1),\n",
    "        viewdrop_prob=0.7,\n",
    "        out_dir=\"results_missing_views\"\n",
    "    )\n",
    "    print(\"\\nConfig:\")\n",
    "    print(cfg)\n",
    "    run_missing_view_sweep(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paper-ready plot: unified grid (matches α-sweep), color order, WITH circle markers, single legend on left ---\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# ── Style & typography (editable text in vector exports)\n",
    "plt.style.use(['science', 'no-latex'])\n",
    "\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype']  = 42\n",
    "mpl.rcParams.update({\n",
    "    # keep text editable in vector exports\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "    # print-appropriate sizes\n",
    "    \"font.size\": 7,\n",
    "    \"axes.titlesize\": 8,\n",
    "    \"axes.labelsize\": 7,\n",
    "    \"xtick.labelsize\": 6,\n",
    "    \"ytick.labelsize\": 6,\n",
    "    \"legend.fontsize\": 4,\n",
    "    # lines\n",
    "    \"lines.linewidth\": 0.8,\n",
    "    \"axes.linewidth\": 0.5,\n",
    "})\n",
    "\n",
    "# ── Unified grid style (explicit so it matches your other figure)\n",
    "GRID_KW = {\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.color\": \"#9e9e9e\",\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.linewidth\": 0.5,\n",
    "    \"grid.alpha\": 0.6,\n",
    "}\n",
    "mpl.rcParams.update(GRID_KW)\n",
    "\n",
    "\n",
    "def plot_missing_view_summary_colors_markers(\n",
    "    k0=6,\n",
    "    csv_dir=\"results_missing_views\",\n",
    "    out_prefix=None,\n",
    "    pad=0.28,           # slight x padding so points aren't on the frame\n",
    "    figsize=(3.54, 2),     # change to (3.54, 2) if you want the small paper size\n",
    "    dpi=300\n",
    "):\n",
    "    # load results\n",
    "    csv_path = os.path.join(csv_dir, f\"missing_view_sweep_K{k0}_revised.csv\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    cov_cols = [\"CLF_Cov\",\"AdjF_Cov\",\"Fisher_Cov\",\"MinPV_Cov\",\"WAvgL_Cov\"]\n",
    "    set_cols = [\"CLF_Set\",\"AdjF_Set\",\"Fisher_Set\",\"MinPV_Set\",\"WAvgL_Set\"]\n",
    "    for col in cov_cols + set_cols + [\"Missing\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"CSV missing expected column: {col}\")\n",
    "\n",
    "    # group means/stds by # missing\n",
    "    g = df.groupby(\"Missing\")\n",
    "    xs = g.size().index.values\n",
    "    mean_cov = g[cov_cols].mean()\n",
    "    std_cov  = g[cov_cols].std().fillna(0.0)\n",
    "    mean_set = g[set_cols].mean()\n",
    "    std_set  = g[set_cols].std().fillna(0.0)\n",
    "\n",
    "    # legend order (keep exactly)\n",
    "    order_cov = [\n",
    "        (\"CLF_Cov\",   \"CLF\"),\n",
    "        (\"AdjF_Cov\",  \"Adjusted Fisher's\"),\n",
    "        (\"Fisher_Cov\",\"Fisher's\"),\n",
    "        (\"MinPV_Cov\", \"Min p-Value\"),\n",
    "        (\"WAvgL_Cov\", \"Weighted Average\"),\n",
    "    ]\n",
    "    order_set = [\n",
    "        (\"CLF_Set\",   \"CLF\"),\n",
    "        (\"AdjF_Set\",  \"Adjusted Fisher's\"),\n",
    "        (\"Fisher_Set\",\"Fisher's\"),\n",
    "        (\"MinPV_Set\", \"Min p-Value\"),\n",
    "        (\"WAvgL_Set\", \"Weighted Average\"),\n",
    "    ]\n",
    "\n",
    "    # requested color order across the legend: blue, orange, green, red, purple\n",
    "    palette = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\"]\n",
    "    color_map = {\n",
    "        \"CLF\":               palette[0],  # blue\n",
    "        \"Adjusted Fisher's\": palette[1],  # orange\n",
    "        \"Fisher's\":          palette[2],  # green\n",
    "        \"Min p-Value\":       palette[3],  # red\n",
    "        \"Weighted Average\":  palette[4],  # purple\n",
    "        \"Target 1-alpha\":    \"#444444\",\n",
    "    }\n",
    "\n",
    "    # marker styling (open circles)\n",
    "    mk = dict(marker='o', markersize=3.5, markerfacecolor='white', markeredgewidth=0.8)\n",
    "\n",
    "    fig, (axL, axR) = plt.subplots(1, 2, figsize=figsize, dpi=dpi)\n",
    "\n",
    "    # ----- Coverage (left) -----\n",
    "    handles = []\n",
    "    for (key, label) in order_cov:\n",
    "        y  = mean_cov[key].values\n",
    "        lo = (mean_cov[key] - std_cov[key]).values\n",
    "        hi = (mean_cov[key] + std_cov[key]).values\n",
    "        ln, = axL.plot(xs, y, color=color_map[label], label=label, **mk)\n",
    "        handles.append(ln)\n",
    "        if np.any(std_cov[key].values > 0):\n",
    "            axL.fill_between(xs, lo, hi, color=color_map[label], alpha=0.12, linewidth=0)\n",
    "\n",
    "    # target 1-alpha line\n",
    "    target = axL.axhline(90.0, linestyle=\"-.\", linewidth=0.9, color=color_map[\"Target 1-alpha\"])\n",
    "\n",
    "    # your titles/labels kept as-is\n",
    "    axL.set_xlabel(\"Number of Missing Views\")\n",
    "    axL.set_ylabel(\"Coverage (%)\")\n",
    "    axL.set_title(f\"Coverage vs. Missing (K={k0})\")\n",
    "    axL.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    axL.set_xlim(xs.min() - pad, xs.max() + pad)\n",
    "    axL.margins(y=0.03)\n",
    "\n",
    "    # single legend on left (exact order + target)\n",
    "    legend_labels  = [lbl for _, lbl in order_cov] + [r\"Target 1-$\\alpha$\"]\n",
    "    legend_handles = handles + [target]\n",
    "    axL.legend(legend_handles, legend_labels, frameon=True, ncol=1, loc=\"lower left\", framealpha=0.9)\n",
    "\n",
    "    # ----- Set size (right) -----\n",
    "    for (key, label) in order_set:\n",
    "        y  = mean_set[key].values\n",
    "        lo = (mean_set[key] - std_set[key]).values\n",
    "        hi = (mean_set[key] + std_set[key]).values\n",
    "        axR.plot(xs, y, color=color_map[label], **mk)\n",
    "        if np.any(std_set[key].values > 0):\n",
    "            axR.fill_between(xs, lo, hi, color=color_map[label], alpha=0.12, linewidth=0)\n",
    "\n",
    "    axR.set_xlabel(\"Number of Missing Views\")\n",
    "    axR.set_ylabel(\"Average Set Size\")\n",
    "    axR.set_title(f\"Set Size vs. Missing (K={k0})\")\n",
    "    axR.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    axR.set_xlim(xs.min() - pad, xs.max() + pad)\n",
    "    axR.margins(y=0.03)\n",
    "\n",
    "    # ── Make grids match exactly & disable minor grids on both subplots\n",
    "    for ax in (axL, axR):\n",
    "        ax.grid(True, which=\"major\", axis=\"both\")\n",
    "        ax.grid(False, which=\"minor\")\n",
    "\n",
    "    # layout & save\n",
    "    if out_prefix is None:\n",
    "        out_prefix = os.path.join(csv_dir, f\"fig_missing_view_sweep_K{k0}_colors_markers\")\n",
    "    png_path = out_prefix + \".png\"\n",
    "    pdf_path = out_prefix + \".pdf\"\n",
    "    svg_path = out_prefix + \".svg\"\n",
    "\n",
    "    plt.tight_layout(w_pad=1.0)\n",
    "    plt.savefig(png_path, dpi=dpi)\n",
    "    plt.savefig(pdf_path)\n",
    "    plt.savefig(svg_path)\n",
    "    print(f\"Saved: {png_path}\")\n",
    "    print(f\"Saved: {pdf_path}\")\n",
    "    print(f\"Saved: {svg_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# run (set k0=4 if your CSV is for K0=4)\n",
    "plot_missing_view_summary_colors_markers(k0=6, csv_dir=\"results_missing_views\", pad=0.28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e66e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
