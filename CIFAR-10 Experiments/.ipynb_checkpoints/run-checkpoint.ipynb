{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e28738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "=== Simulation 1/10 ===\n",
      "\n",
      "  -> K = 2\n",
      "    [View 1/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 4\n",
      "    [View 1/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 5/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 6/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "=== Simulation 2/10 ===\n",
      "\n",
      "  -> K = 2\n",
      "    [View 1/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 4\n",
      "    [View 1/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 5/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 6/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "=== Simulation 3/10 ===\n",
      "\n",
      "  -> K = 2\n",
      "    [View 1/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 5/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 4\n",
      "    [View 1/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 5/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 6/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 4\n",
      "    [View 1/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 5/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 6/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 4\n",
      "    [View 1/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 5/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 6/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "=== Simulation 7/10 ===\n",
      "\n",
      "  -> K = 2\n",
      "    [View 1/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 4\n",
      "    [View 1/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/4] training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 5/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "=== Simulation 8/10 ===\n",
      "\n",
      "  -> K = 2\n",
      "    [View 1/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 4\n",
      "    [View 1/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 5/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 6/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "=== Simulation 9/10 ===\n",
      "\n",
      "  -> K = 2\n",
      "    [View 1/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 100/100\n",
      "    [View 2/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 5/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 6/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "=== Simulation 10/10 ===\n",
      "\n",
      "  -> K = 2\n",
      "    [View 1/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 4\n",
      "    [View 1/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 5/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 6/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "\n",
      "=== Coverage (raw rows) ===\n",
      "   Sim  K  Conformal Fusion  Min p-Value  Fisher  Adjusted Fisher  \\\n",
      "0    0  2             90.08        91.68   87.66            90.02   \n",
      "1    0  3             90.72        92.32   86.26            90.39   \n",
      "2    0  4             90.85        92.33   85.33            90.37   \n",
      "3    0  6             90.72        94.37   82.22            90.41   \n",
      "4    1  2             90.05        91.42   87.11            90.15   \n",
      "\n",
      "   Weighted Avg (learned)  MVCP (theirs)  \n",
      "0                   94.26          90.30  \n",
      "1                   96.16          90.65  \n",
      "2                   97.61          89.83  \n",
      "3                   97.03          90.69  \n",
      "4                   93.81          90.99  \n",
      "\n",
      "=== Set Size (raw rows) ===\n",
      "   Sim  K  Conformal Fusion  Min p-Value  Fisher  Adjusted Fisher  \\\n",
      "0    0  2            3.5544       4.3050  3.3309           3.6672   \n",
      "1    0  3            3.6696       4.7038  3.2398           3.8669   \n",
      "2    0  4            3.4142       4.8201  2.9841           3.6952   \n",
      "3    0  6            3.6631       5.5205  2.8553           3.9866   \n",
      "4    1  2            3.7392       4.3395  3.3735           3.8252   \n",
      "\n",
      "   Weighted Avg (learned)  MVCP (theirs)  \n",
      "0                  4.6114         3.7395  \n",
      "1                  5.3174         3.8842  \n",
      "2                  5.9118         3.6031  \n",
      "3                  5.8189         4.0029  \n",
      "4                  4.6105         4.0697  \n",
      "\n",
      "Saved:\n",
      "  cifar_summary_coverage.csv / .tex\n",
      "  cifar_summary_setsize.csv  / .tex\n",
      "  cifar_summary_final.csv    / .tex\n",
      "  cifar_accuracy_summary.csv / .tex\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 multi-view conformal fusion + MVCP (their method)\n",
    "# - Per-view CNNs produce classwise probabilities\n",
    "# - Baselines: Min-p, Fisher, Adjusted Fisher, Weighted Avg (learned)\n",
    "# - Late fusion: LR on [p, prob]; conformalize fused scores\n",
    "# - Added: MVCP (quantile-envelope over multivariate per-view scores)\n",
    "# - Produces mean (std) tables across simulations & K; saves CSV/LaTeX\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# Config\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class CIFARConfig:\n",
    "    # core\n",
    "    alpha: float = 0.1\n",
    "    Ks: Tuple[int, ...] = (2, 3, 4, 6)\n",
    "    num_classes: int = 10\n",
    "    num_simulations: int = 10\n",
    "\n",
    "    # training\n",
    "    epochs_per_view: int = 100      # adjust for runtime\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 512\n",
    "    max_iter_lr: int = 1000         # for sklearn LR (fusion & weight-learning)\n",
    "    train_seed_base: int = 42\n",
    "\n",
    "    # data split fractions (similar structure to synthetic)\n",
    "    train_frac: float = 0.5         # predictor training from full train set\n",
    "    cal_frac_of_temp: float = 0.3   # portion of temp used as calibration for per-view\n",
    "    fuse_train_frac_of_rest: float = 0.7  # remaining split into fusion_train/cal\n",
    "\n",
    "    # MVCP params (their method)\n",
    "    mvcp_env_frac: float = 0.2      # fraction of fuse-cal used to define envelope S_C^(1)\n",
    "    mvcp_num_dirs: int = 32         # number of projection directions M\n",
    "    mvcp_eps: float = 5e-3          # tolerance around (1 - alpha) for coverage in S_C^(1)\n",
    "    mvcp_rng_seed: int = 2024       # random seed for direction sampling\n",
    "\n",
    "# =============================================================================\n",
    "# Torch / Data\n",
    "# =============================================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# seeds (optional, for reproducibility)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load once\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "X_train_full = train_dataset.data.astype(np.float32) / 255.0   # (50000, 32, 32, 3)\n",
    "Y_train_full = np.array(train_dataset.targets)\n",
    "X_test_full  = test_dataset.data.astype(np.float32) / 255.0    # (10000, 32, 32, 3)\n",
    "Y_test_full  = np.array(test_dataset.targets)\n",
    "\n",
    "# =============================================================================\n",
    "# Multi-view (patch) utilities\n",
    "# =============================================================================\n",
    "\n",
    "def split_image_into_k_patches(image: torch.Tensor, k: int) -> List[torch.Tensor]:\n",
    "    # image: (C, H, W) = (3,32,32)\n",
    "    C, H, W = image.shape\n",
    "    if k == 4:\n",
    "        # 2x2 grid\n",
    "        patches = []\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                patch = image[:, i*16:(i+1)*16, j*16:(j+1)*16]\n",
    "                patches.append(patch)\n",
    "        return patches\n",
    "    else:\n",
    "        # vertical stripes\n",
    "        base_width = W // k\n",
    "        remainder = W % k\n",
    "        patches, start = [], 0\n",
    "        for idx in range(k):\n",
    "            width = base_width + (1 if idx < remainder else 0)\n",
    "            patch = image[:, :, start:start+width]\n",
    "            patches.append(patch)\n",
    "            start += width\n",
    "        return patches\n",
    "\n",
    "class PatchesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images: np.ndarray, labels: np.ndarray, k: int, view: int):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.k = k\n",
    "        self.view = view\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img = self.images[idx].transpose((2, 0, 1))   # (3,32,32)\n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "        patches = split_image_into_k_patches(img, self.k)\n",
    "        patch = patches[self.view]\n",
    "        label = int(self.labels[idx])\n",
    "        return patch, label\n",
    "\n",
    "# =============================================================================\n",
    "# Simple CNN per view\n",
    "# =============================================================================\n",
    "\n",
    "class PredictorCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1   = None\n",
    "        self.fc2   = None\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.fc1 is None:\n",
    "            b, c, h, w = x.shape\n",
    "            self.fc1 = nn.Linear(c*h*w, 128).to(x.device)\n",
    "            self.fc2 = nn.Linear(128, self.num_classes).to(x.device)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def train_model(model: nn.Module, train_loader, num_epochs=100, lr=1e-3):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt  = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    for ep in range(num_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), torch.tensor(yb, dtype=torch.long, device=device)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if (ep+1) % 25 == 0:\n",
    "            print(f\"  epoch {ep+1}/{num_epochs}\")\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# Conformal utilities (torch models)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_nonconformity_scores(model: nn.Module, loader) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    scores, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs  = F.softmax(logits, dim=1)\n",
    "            idx    = torch.arange(probs.size(0), device=probs.device)\n",
    "            true_p = probs[idx, torch.tensor(yb, dtype=torch.long, device=probs.device)]\n",
    "            s = (1 - true_p).detach().cpu().numpy()\n",
    "            scores.extend(s)\n",
    "            labels.extend(yb.numpy())\n",
    "    return np.asarray(scores, float), np.asarray(labels, int)\n",
    "\n",
    "def classwise_scores(scores: np.ndarray, labels: np.ndarray, L: int) -> Dict[int, np.ndarray]:\n",
    "    out = {c: [] for c in range(L)}\n",
    "    for s, y in zip(scores, labels):\n",
    "        out[int(y)].append(float(s))\n",
    "    return {c: np.asarray(v, float) for c, v in out.items()}\n",
    "\n",
    "def per_view_pvalues_and_probs(\n",
    "    model: nn.Module, class_scores: Dict[int, np.ndarray], loader, L: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return p-values (n,L) and probs (n,L) for a single view.\"\"\"\n",
    "    model.eval()\n",
    "    probs_all = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs  = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "            probs_all.append(probs)\n",
    "    probs_all = np.vstack(probs_all)  # (n, L)\n",
    "\n",
    "    n = probs_all.shape[0]\n",
    "    pvals = np.zeros((n, L))\n",
    "    for y in range(L):\n",
    "        cal = class_scores.get(y, np.array([]))\n",
    "        if cal.size == 0:\n",
    "            pvals[:, y] = 1.0\n",
    "        else:\n",
    "            s_test = 1 - probs_all[:, y]\n",
    "            counts = np.sum(cal[:, None] >= s_test[None, :], axis=0)\n",
    "            pvals[:, y] = (1 + counts) / (len(cal) + 1)\n",
    "    return pvals, probs_all\n",
    "\n",
    "# =============================================================================\n",
    "# Fusion utilities\n",
    "# =============================================================================\n",
    "\n",
    "def build_fusion_features(pvals_list: List[np.ndarray], probs_list: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Horizontally stack [pvals, probs] for each view -> (n, K*2L)\"\"\"\n",
    "    blocks = [np.hstack([pvals_list[k], probs_list[k]]) for k in range(len(pvals_list))]\n",
    "    return np.hstack(blocks)\n",
    "\n",
    "def min_p_value_fusion(P_all: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"K * min_k p_k^y. P_all: (K,n,L) -> (n,L)\"\"\"\n",
    "    K = P_all.shape[0]\n",
    "    return K * np.min(P_all, axis=0)\n",
    "\n",
    "def fisher_fusion(P_all: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Standard Fisher.\"\"\"\n",
    "    eps = 1e-12\n",
    "    p = np.clip(P_all, eps, 1.0)\n",
    "    T = -2 * np.sum(np.log(p), axis=0)\n",
    "    df = 2 * P_all.shape[0]\n",
    "    return 1 - chi2.cdf(T, df=df)\n",
    "\n",
    "def adjusted_fisher_fusion(P_train: np.ndarray, y_train: np.ndarray, P_test: np.ndarray, L: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Moment-matched Fisher per class: fit variance of T_y = sum_k -2log p_k^y on per-class train,\n",
    "    then use scaled-chi-square CDF.\n",
    "    \"\"\"\n",
    "    K, _, _ = P_train.shape\n",
    "    n_test = P_test.shape[1]\n",
    "    eps = 1e-12\n",
    "    out = np.zeros((n_test, L))\n",
    "    for y in range(L):\n",
    "        idx = np.where(y_train == y)[0]\n",
    "        if idx.size < 5:\n",
    "            out[:, y] = fisher_fusion(P_test)[:, y]\n",
    "            continue\n",
    "        P_cls = np.clip(P_train[:, idx, y], eps, 1.0)  # (K, n_y)\n",
    "        W = -2 * np.log(P_cls)                          # (K, n_y)\n",
    "        Wc = W - W.mean(axis=1, keepdims=True)\n",
    "        Sigma = (Wc @ Wc.T) / max(W.shape[1] - 1, 1)    # (K, K)\n",
    "        var_T = np.sum(Sigma)\n",
    "        if not np.isfinite(var_T) or var_T <= 0:\n",
    "            var_T = 4 * K\n",
    "        f_y = (8.0 * K * K) / var_T\n",
    "        c_y = var_T / (4 * K)\n",
    "\n",
    "        P_t = np.clip(P_test[:, :, y], eps, 1.0)\n",
    "        T_t = -2 * np.sum(np.log(P_t), axis=0)\n",
    "        out[:, y] = 1 - chi2.cdf(T_t / c_y, df=f_y)\n",
    "    return out\n",
    "\n",
    "def weighted_average_fusion(P_all: np.ndarray, weights: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"sum_k w_k p_k^y; P_all: (K,n,L), weights: (K,)\"\"\"\n",
    "    return np.tensordot(weights, P_all, axes=(0, 0))\n",
    "\n",
    "def learn_view_weights_from_pvals(pv_train_concat: np.ndarray, y_train: np.ndarray, K: int, L: int, max_iter: int, seed: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Train multinomial LR on p-only features to predict y.\n",
    "    Convert coef_ (L, K*L) -> view weights by Frobenius norm per (L×L) block.\n",
    "    \"\"\"\n",
    "    lr = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=max_iter, random_state=seed)\n",
    "    lr.fit(pv_train_concat, y_train)\n",
    "    B = lr.coef_  # (L, K*L)\n",
    "    imps = []\n",
    "    for k in range(K):\n",
    "        block = B[:, k*L:(k+1)*L]\n",
    "        imps.append(np.linalg.norm(block, ord=\"fro\"))\n",
    "    w = np.array(imps, float)\n",
    "    w = np.maximum(w, 1e-12)\n",
    "    return w / w.sum()\n",
    "\n",
    "# =============================================================================\n",
    "# Conformalization of fused model (MISSING BEFORE — now included)\n",
    "# =============================================================================\n",
    "\n",
    "def fused_class_cal_scores(y_cal: np.ndarray, fused_probs_cal: np.ndarray, L: int) -> Dict[int, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Per-class calibration score sets for fused predictor:\n",
    "    s = 1 - fused_probs[y]; stored classwise for conformal p-values later.\n",
    "    \"\"\"\n",
    "    s = 1 - fused_probs_cal[np.arange(len(y_cal)), y_cal]\n",
    "    out = {c: [] for c in range(L)}\n",
    "    for sc, yy in zip(s, y_cal):\n",
    "        out[int(yy)].append(float(sc))\n",
    "    return {c: np.asarray(v, float) for c, v in out.items()}\n",
    "\n",
    "def fused_p_values_from_cal(fused_probs: np.ndarray, cal_class_scores: Dict[int, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute conformal p-values from fused predictor using per-class calibration scores.\n",
    "    \"\"\"\n",
    "    n, L = fused_probs.shape\n",
    "    out = np.zeros((n, L))\n",
    "    for y in range(L):\n",
    "        cal = cal_class_scores.get(y, np.array([]))\n",
    "        if cal.size == 0:\n",
    "            out[:, y] = 1.0\n",
    "        else:\n",
    "            s_test = 1 - fused_probs[:, y]\n",
    "            counts = np.sum(cal[:, None] >= s_test[None, :], axis=0)\n",
    "            out[:, y] = (1 + counts) / (len(cal) + 1)\n",
    "    return out\n",
    "\n",
    "# =============================================================================\n",
    "# MVCP (their method): multivariate quantile envelope over per-view scores\n",
    "# =============================================================================\n",
    "\n",
    "def _mvcp_sample_directions(K: int, M: int, rng: np.random.RandomState) -> np.ndarray:\n",
    "    \"\"\"Uniform on the positive orthant of the unit (K-1)-sphere.\"\"\"\n",
    "    V = rng.randn(M, K)\n",
    "    V = np.abs(V)\n",
    "    V = V / np.linalg.norm(V, axis=1, keepdims=True)\n",
    "    return V  # shape (M, K)\n",
    "\n",
    "def _mvcp_quantile(a: np.ndarray, q: float) -> float:\n",
    "    \"\"\"Quantile helper with guard.\"\"\"\n",
    "    q = min(max(q, 0.0), 1.0)\n",
    "    # Use method='higher' to avoid deprecated 'interpolation='\n",
    "    return float(np.quantile(a, q, method=\"higher\"))\n",
    "\n",
    "def _mvcp_build_envelope(S1: np.ndarray, alpha: float, M: int, eps: float, rng: np.random.RandomState) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build directions U (M,K) and per-direction thresholds q_e (M,) via beta search\n",
    "    so that intersection of half-spaces covers ~ (1 - alpha) of S1.\n",
    "    \"\"\"\n",
    "    n1, K = S1.shape\n",
    "    U = _mvcp_sample_directions(K, M, rng)  # (M, K)\n",
    "    # search beta in [alpha/M, alpha]\n",
    "    beta_lo = alpha / max(M, 1)\n",
    "    beta_hi = alpha\n",
    "    # Precompute projections per direction\n",
    "    Proj = S1 @ U.T  # (n1, M)\n",
    "\n",
    "    # Binary search beta to hit ~1 - alpha coverage\n",
    "    target = 1.0 - alpha\n",
    "    for _ in range(30):\n",
    "        beta = 0.5 * (beta_lo + beta_hi)\n",
    "        # per-direction (1 - beta) quantiles\n",
    "        q_e = np.array([_mvcp_quantile(Proj[:, m], 1.0 - beta) for m in range(M)])  # (M,)\n",
    "        inside = (Proj <= q_e[None, :]).all(axis=1)\n",
    "        cov = inside.mean()\n",
    "        if cov > target + eps:\n",
    "            beta_lo = beta\n",
    "        elif cov < target - eps:\n",
    "            beta_hi = beta\n",
    "        else:\n",
    "            break\n",
    "    # final thresholds at current beta\n",
    "    q_e = np.array([_mvcp_quantile(Proj[:, m], 1.0 - beta) for m in range(M)])\n",
    "    return U, q_e\n",
    "\n",
    "def _mvcp_adjust_scale(S2: np.ndarray, U: np.ndarray, q_e: np.ndarray, alpha: float) -> float:\n",
    "    \"\"\"Compute t*(s) = max_m u_m^T s / q_e,m on S2, then (1 - alpha)-quantile for scaling.\"\"\"\n",
    "    Proj2 = S2 @ U.T  # (n2, M)\n",
    "    qsafe = np.maximum(q_e, 1e-12)\n",
    "    t_vals = np.max(Proj2 / qsafe[None, :], axis=1)\n",
    "    b_t = _mvcp_quantile(t_vals, 1.0 - alpha)\n",
    "    return float(max(b_t, 1e-12))\n",
    "\n",
    "def mvcp_fit(S_scores: np.ndarray, alpha: float, env_frac: float, M: int, eps: float, seed: int) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Fit MVCP envelope:\n",
    "      S_scores: (n_cal, K) score vectors for TRUE labels on fuse-cal set.\n",
    "    Returns directions U, thresholds q_e, and scale b_t.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    n = S_scores.shape[0]\n",
    "    n1 = max(1, int(np.floor(env_frac * n)))\n",
    "    # shuffle for randomness\n",
    "    idx = rng.permutation(n)\n",
    "    S1 = S_scores[idx[:n1]]\n",
    "    S2 = S_scores[idx[n1:]] if n1 < n else S_scores[idx[:n1]]  # if too small, reuse\n",
    "    U, q_e = _mvcp_build_envelope(S1, alpha, M, eps, rng)\n",
    "    b_t = _mvcp_adjust_scale(S2, U, q_e, alpha)\n",
    "    return U, q_e, b_t\n",
    "\n",
    "def mvcp_predict_sets(pr_views: List[np.ndarray], U: np.ndarray, q_e: np.ndarray, b_t: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given per-view probabilities for TEST set, produce boolean prediction sets C(x):\n",
    "      pr_views: list length K with arrays (n, L)\n",
    "    Returns C_bool: (n, L) where True if class is included by MVCP region.\n",
    "    \"\"\"\n",
    "    K = len(pr_views)\n",
    "    n, L = pr_views[0].shape\n",
    "    PR = np.stack(pr_views, axis=0)  # (K, n, L)\n",
    "    C = np.zeros((n, L), dtype=bool)\n",
    "    U_T = U.T  # (K, M)\n",
    "    thresh = q_e * b_t  # (M,)\n",
    "\n",
    "    for y in range(L):\n",
    "        S_y = 1.0 - PR[:, :, y]        # (K, n)\n",
    "        S_y = S_y.transpose(1, 0)      # (n, K)\n",
    "        Proj = S_y @ U_T               # (n, M)\n",
    "        C[:, y] = (Proj <= thresh[None, :]).all(axis=1)\n",
    "    return C  # (n, L) boolean\n",
    "\n",
    "# =============================================================================\n",
    "# Metrics & tables\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_sets(P: np.ndarray, y_true: np.ndarray, alpha: float) -> Tuple[float, float]:\n",
    "    C = (P > alpha)\n",
    "    cov = float(np.mean(C[np.arange(len(y_true)), y_true]))\n",
    "    size = float(np.mean(C.sum(axis=1)))\n",
    "    return cov, size\n",
    "\n",
    "def evaluate_sets_from_bool(C_bool: np.ndarray, y_true: np.ndarray) -> Tuple[float, float]:\n",
    "    cov = float(np.mean(C_bool[np.arange(len(y_true)), y_true]))\n",
    "    size = float(np.mean(C_bool.sum(axis=1)))\n",
    "    return cov, size\n",
    "\n",
    "def summarize_table(df: pd.DataFrame, methods: List[str], metric_name: str) -> pd.DataFrame:\n",
    "    g = df.groupby(\"K\").agg({m: [\"mean\", \"std\"] for m in methods})\n",
    "    g.columns = [f\"{a}_{b}\" for a, b in g.columns]\n",
    "    g = g.reset_index()\n",
    "    for m in methods:\n",
    "        g[m] = g.apply(lambda r: f\"{r[f'{m}_mean']:.2f} ({r[f'{m}_std']:.2f})\", axis=1)\n",
    "    g.insert(1, \"Metric\", metric_name)\n",
    "    return g[[\"K\", \"Metric\"] + methods]\n",
    "\n",
    "# =============================================================================\n",
    "# Main experiment\n",
    "# =============================================================================\n",
    "\n",
    "def run_experiments(cfg: CIFARConfig):\n",
    "    results_cov, results_size, results_acc = [], [], []\n",
    "\n",
    "    for sim in range(cfg.num_simulations):\n",
    "        print(f\"\\n=== Simulation {sim+1}/{cfg.num_simulations} ===\")\n",
    "        seed = cfg.train_seed_base + sim\n",
    "        rng = np.random.RandomState(seed)\n",
    "\n",
    "        # Splits (train for view predictors, then cal/fusion splits)\n",
    "        X_trP, X_tmp, y_trP, y_tmp = train_test_split(\n",
    "            X_train_full, Y_train_full, test_size=1 - cfg.train_frac, stratify=Y_train_full, random_state=seed\n",
    "        )\n",
    "        # We will split X_tmp into cal and fusion pools\n",
    "        X_cal, X_rest, y_cal, y_rest = train_test_split(\n",
    "            X_tmp, y_tmp, test_size=1 - cfg.cal_frac_of_temp, stratify=y_tmp, random_state=seed\n",
    "        )\n",
    "        X_fuse_tr, X_fuse_cal, y_fuse_tr, y_fuse_cal = train_test_split(\n",
    "            X_rest, y_rest, test_size=1 - cfg.fuse_train_frac_of_rest, stratify=y_rest, random_state=seed\n",
    "        )\n",
    "        X_te, y_te = X_test_full, Y_test_full\n",
    "\n",
    "        for K in cfg.Ks:\n",
    "            print(f\"\\n  -> K = {K}\")\n",
    "            num_views = 4 if K == 4 else K\n",
    "\n",
    "            # Build loaders per view\n",
    "            loaders = {}\n",
    "            for v in range(num_views):\n",
    "                tr_loader   = torch.utils.data.DataLoader(PatchesDataset(X_trP,      y_trP,      K, v), batch_size=cfg.batch_size, shuffle=True)\n",
    "                cal_loader  = torch.utils.data.DataLoader(PatchesDataset(X_cal,      y_cal,      K, v), batch_size=cfg.batch_size, shuffle=False)\n",
    "                ftr_loader  = torch.utils.data.DataLoader(PatchesDataset(X_fuse_tr,  y_fuse_tr,  K, v), batch_size=cfg.batch_size, shuffle=False)\n",
    "                fcal_loader = torch.utils.data.DataLoader(PatchesDataset(X_fuse_cal, y_fuse_cal, K, v), batch_size=cfg.batch_size, shuffle=False)\n",
    "                te_loader   = torch.utils.data.DataLoader(PatchesDataset(X_te,       y_te,       K, v), batch_size=cfg.batch_size, shuffle=False)\n",
    "                loaders[v] = dict(train=tr_loader, cal=cal_loader, ftr=ftr_loader, fcal=fcal_loader, te=te_loader)\n",
    "\n",
    "            # Train per-view CNNs\n",
    "            models, cal_classwise = [], []\n",
    "            for v in range(num_views):\n",
    "                print(f\"    [View {v+1}/{num_views}] training...\")\n",
    "                m = PredictorCNN(num_classes=cfg.num_classes)\n",
    "                m = train_model(m, loaders[v][\"train\"], num_epochs=cfg.epochs_per_view, lr=cfg.lr)\n",
    "                models.append(m)\n",
    "                sc, lab = compute_nonconformity_scores(m, loaders[v][\"cal\"])\n",
    "                cal_classwise.append(classwise_scores(sc, lab, cfg.num_classes))\n",
    "\n",
    "            # Per-view p/probs for fusion train/cal/test\n",
    "            pv_tr, pr_tr = [], []\n",
    "            pv_cal, pr_cal = [], []\n",
    "            pv_te,  pr_te  = [], []\n",
    "            for v in range(num_views):\n",
    "                p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"ftr\"], cfg.num_classes)\n",
    "                pv_tr.append(p); pr_tr.append(pr)\n",
    "                p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"fcal\"], cfg.num_classes)\n",
    "                pv_cal.append(p); pr_cal.append(pr)\n",
    "                p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"te\"],  cfg.num_classes)\n",
    "                pv_te.append(p);  pr_te.append(pr)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Late-fusion LR (yours)\n",
    "            # -----------------------------\n",
    "            X_ftr = build_fusion_features(pv_tr, pr_tr)\n",
    "            fusion_lr = LogisticRegression(max_iter=cfg.max_iter_lr, multi_class=\"multinomial\", solver=\"lbfgs\", random_state=seed)\n",
    "            fusion_lr.fit(X_ftr, y_fuse_tr)\n",
    "\n",
    "            # Fused calibration probs + classwise cal scores\n",
    "            X_fcal = build_fusion_features(pv_cal, pr_cal)\n",
    "            fused_probs_cal = fusion_lr.predict_proba(X_fcal)\n",
    "            fused_cal_scores = fused_class_cal_scores(y_fuse_cal, fused_probs_cal, cfg.num_classes)\n",
    "\n",
    "            # Fused test probs\n",
    "            X_ftest = build_fusion_features(pv_te, pr_te)\n",
    "            fused_probs_test = fusion_lr.predict_proba(X_ftest)\n",
    "\n",
    "            # Our conformal-fused p-values\n",
    "            P_cf = fused_p_values_from_cal(fused_probs_test, fused_cal_scores)\n",
    "\n",
    "            # Baselines (stack per-view p-values)\n",
    "            P_train = np.stack(pv_tr, axis=0)   # (K, n_tr, L)\n",
    "            P_test  = np.stack(pv_te, axis=0)   # (K, n_te, L)\n",
    "\n",
    "            P_min    = min_p_value_fusion(P_test)\n",
    "            P_fisher = fisher_fusion(P_test)\n",
    "            P_adjF   = adjusted_fisher_fusion(P_train, y_fuse_tr, P_test, cfg.num_classes)\n",
    "\n",
    "            # Learned weighted average from p-values-only (K*L features)\n",
    "            pv_tr_concat = np.concatenate(pv_tr, axis=1)  # (n_tr, K*L)\n",
    "            w_learned = learn_view_weights_from_pvals(pv_tr_concat, y_fuse_tr, num_views, cfg.num_classes, cfg.max_iter_lr, seed)\n",
    "            P_wavgL = weighted_average_fusion(P_test, w_learned)\n",
    "\n",
    "            # -----------------------------\n",
    "            # MVCP (their method)\n",
    "            # -----------------------------\n",
    "            # Build TRUE-label multiview scores on fuse-cal set: s_k = 1 - p_k(y_i)\n",
    "            n_cal = pr_cal[0].shape[0]\n",
    "            S_scores = np.zeros((n_cal, num_views))\n",
    "            for i in range(n_cal):\n",
    "                yi = int(y_fuse_cal[i])\n",
    "                for v in range(num_views):\n",
    "                    S_scores[i, v] = 1.0 - pr_cal[v][i, yi]\n",
    "            # Fit envelope & scaling\n",
    "            U, q_e, b_t = mvcp_fit(\n",
    "                S_scores,\n",
    "                alpha=cfg.alpha,\n",
    "                env_frac=cfg.mvcp_env_frac,\n",
    "                M=cfg.mvcp_num_dirs,\n",
    "                eps=cfg.mvcp_eps,\n",
    "                seed=cfg.mvcp_rng_seed + sim + K  # vary across sims/K\n",
    "            )\n",
    "            # Predict MVCP sets on TEST\n",
    "            C_mvcp = mvcp_predict_sets(pr_te, U, q_e, b_t)  # (n_test, L) boolean\n",
    "\n",
    "            # Metrics\n",
    "            cov_cf,   set_cf   = evaluate_sets(P_cf,     y_te, cfg.alpha)\n",
    "            cov_min,  set_min  = evaluate_sets(P_min,    y_te, cfg.alpha)\n",
    "            cov_fi,   set_fi   = evaluate_sets(P_fisher, y_te, cfg.alpha)\n",
    "            cov_afi,  set_afi  = evaluate_sets(P_adjF,   y_te, cfg.alpha)\n",
    "            cov_wl,   set_wl   = evaluate_sets(P_wavgL,  y_te, cfg.alpha)\n",
    "            cov_mv,   set_mv   = evaluate_sets_from_bool(C_mvcp, y_te)\n",
    "\n",
    "            results_cov.append({\n",
    "                \"Sim\": sim, \"K\": K,\n",
    "                \"Conformal Fusion\": cov_cf * 100,\n",
    "                \"Min p-Value\": cov_min * 100,\n",
    "                \"Fisher\": cov_fi * 100,\n",
    "                \"Adjusted Fisher\": cov_afi * 100,\n",
    "                \"Weighted Avg (learned)\": cov_wl * 100,\n",
    "                \"MVCP (theirs)\": cov_mv * 100,\n",
    "            })\n",
    "            results_size.append({\n",
    "                \"Sim\": sim, \"K\": K,\n",
    "                \"Conformal Fusion\": set_cf,\n",
    "                \"Min p-Value\": set_min,\n",
    "                \"Fisher\": set_fi,\n",
    "                \"Adjusted Fisher\": set_afi,\n",
    "                \"Weighted Avg (learned)\": set_wl,\n",
    "                \"MVCP (theirs)\": set_mv,\n",
    "            })\n",
    "\n",
    "            # (Optional) Reference accuracy using simple average of per-view probs\n",
    "            avg_probs = np.mean(np.stack(pr_te, axis=0), axis=0)\n",
    "            acc_ref = accuracy_score(y_te, np.argmax(avg_probs, axis=1)) * 100\n",
    "            results_acc.append({\"Sim\": sim, \"K\": K, \"Reference Acc (avg probs)\": acc_ref})\n",
    "\n",
    "    return pd.DataFrame(results_cov), pd.DataFrame(results_size), pd.DataFrame(results_acc)\n",
    "\n",
    "# =============================================================================\n",
    "# Save/print tables (same style as synthetic)\n",
    "# =============================================================================\n",
    "\n",
    "def save_tables(df_cov: pd.DataFrame, df_size: pd.DataFrame, df_acc: pd.DataFrame):\n",
    "    methods = [\n",
    "        \"Conformal Fusion\",\n",
    "        \"Min p-Value\",\n",
    "        \"Fisher\",\n",
    "        \"Adjusted Fisher\",\n",
    "        \"Weighted Avg (learned)\",\n",
    "        \"MVCP (theirs)\",\n",
    "    ]\n",
    "    sum_cov = summarize_table(df_cov, methods, \"Coverage (%)\")\n",
    "    sum_set = summarize_table(df_size, methods, \"Average Set Size\")\n",
    "\n",
    "    # CSV + LaTeX (CIFAR version)\n",
    "    sum_cov.to_csv(\"cifar_summary_coverage.csv\", index=False)\n",
    "    sum_set.to_csv(\"cifar_summary_setsize.csv\", index=False)\n",
    "    with open(\"cifar_summary_coverage.tex\", \"w\") as f:\n",
    "        f.write(sum_cov.to_latex(index=False, escape=False))\n",
    "    with open(\"cifar_summary_setsize.tex\", \"w\") as f:\n",
    "        f.write(sum_set.to_latex(index=False, escape=False))\n",
    "\n",
    "    # Compact side-by-side\n",
    "    cov_comp = sum_cov.drop(columns=[\"Metric\"]).rename(columns={\n",
    "        \"Conformal Fusion\": \"CF Cov\",\n",
    "        \"Min p-Value\": \"MinPV Cov\",\n",
    "        \"Fisher\": \"Fisher Cov\",\n",
    "        \"Adjusted Fisher\": \"AdjF Cov\",\n",
    "        \"Weighted Avg (learned)\": \"WAvgL Cov\",\n",
    "        \"MVCP (theirs)\": \"MVCP Cov\",\n",
    "    })\n",
    "    set_comp = sum_set.drop(columns=[\"Metric\"]).rename(columns={\n",
    "        \"Conformal Fusion\": \"CF Set\",\n",
    "        \"Min p-Value\": \"MinPV Set\",\n",
    "        \"Fisher\": \"Fisher Set\",\n",
    "        \"Adjusted Fisher\": \"AdjF Set\",\n",
    "        \"Weighted Avg (learned)\": \"WAvgL Set\",\n",
    "        \"MVCP (theirs)\": \"MVCP Set\",\n",
    "    })\n",
    "    final = cov_comp.merge(set_comp, on=\"K\").sort_values(\"K\")\n",
    "    final.to_csv(\"cifar_summary_final.csv\", index=False)\n",
    "    with open(\"cifar_summary_final.tex\", \"w\") as f:\n",
    "        f.write(final.to_latex(index=False, escape=False))\n",
    "\n",
    "    # Acc means (if you want)\n",
    "    acc_means = df_acc.groupby(\"K\").mean(numeric_only=True).reset_index()\n",
    "    acc_means.to_csv(\"cifar_accuracy_summary.csv\", index=False)\n",
    "    with open(\"cifar_accuracy_summary.tex\", \"w\") as f:\n",
    "        f.write(acc_means.to_latex(index=False, float_format=\"%.2f\"))\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\"  cifar_summary_coverage.csv / .tex\")\n",
    "    print(\"  cifar_summary_setsize.csv  / .tex\")\n",
    "    print(\"  cifar_summary_final.csv    / .tex\")\n",
    "    print(\"  cifar_accuracy_summary.csv / .tex\")\n",
    "\n",
    "# =============================================================================\n",
    "# Entry\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    cfg = CIFARConfig()\n",
    "    df_cov, df_size, df_acc = run_experiments(cfg)\n",
    "    print(\"\\n=== Coverage (raw rows) ===\")\n",
    "    print(df_cov.head())\n",
    "    print(\"\\n=== Set Size (raw rows) ===\")\n",
    "    print(df_size.head())\n",
    "    save_tables(df_cov, df_size, df_acc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042173c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
