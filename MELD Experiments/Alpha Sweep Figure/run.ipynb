{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f17a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siahkali/.local/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/siahkali/.local/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA A30\n",
      "Available GPUs: 1\n",
      "Loading RoBERTa tokenizer...\n",
      "\n",
      "Loading data from: /depot/gupta869/data/farbod/preprocessed_data\n",
      "Loading labels for stratification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train labels: 100%|██████████| 11096/11096 [04:47<00:00, 38.59it/s]\n",
      "Loading test labels: 100%|██████████| 2610/2610 [01:01<00:00, 42.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      " Train samples: 11096\n",
      " Test samples: 2610\n",
      "\n",
      "Class distribution (train):\n",
      " neutral   : 5178 (46.7%)\n",
      " joy       : 1906 (17.2%)\n",
      " surprise  : 1355 (12.2%)\n",
      " anger     : 1262 (11.4%)\n",
      " sadness   :  794 (7.2%)\n",
      " disgust   :  293 (2.6%)\n",
      " fear      :  308 (2.8%)\n",
      "\n",
      "Experiment Configuration:\n",
      " Device: cuda\n",
      " Epochs: Text=5, Audio=20, Video=20\n",
      " LRs: Text=5e-05, Audio=0.003, Video=0.0003\n",
      " Batch size: 64 (train), 8 (inf)\n",
      "\n",
      "Experiment Configuration:\n",
      " Device: cuda\n",
      " Epochs: Text=5, Audio=20, Video=20\n",
      " LRs: Text=5e-05, Audio=0.003, Video=0.0003\n",
      " Batch size: 64 (train), 8 (inf)\n",
      "\n",
      " [Text (RoBERTa)] Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  1/5: Loss=1.810, Train Acc=32.7%, F1=0.376 | Val Acc=53.5%, F1=0.546 | Best F1=0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  2/5: Loss=1.488, Train Acc=52.5%, F1=0.556 | Val Acc=46.9%, F1=0.511 | Best F1=0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  3/5: Loss=1.263, Train Acc=58.9%, F1=0.610 | Val Acc=50.1%, F1=0.537 | Best F1=0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  4/5: Loss=1.010, Train Acc=65.8%, F1=0.671 | Val Acc=53.2%, F1=0.559 | Best F1=0.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  5/5: Loss=0.822, Train Acc=70.3%, F1=0.710 | Val Acc=56.5%, F1=0.582 | Best F1=0.582\n",
      " [Text (RoBERTa)] Best Val: Acc=56.54%, F1=0.5823\n",
      "\n",
      " [Audio (CNN)] Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  1/20: Loss=2.043, Train Acc=14.6%, F1=0.170 | Val Acc=2.6%, F1=0.001 | Best F1=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  2/20: Loss=1.999, Train Acc=12.5%, F1=0.137 | Val Acc=20.6%, F1=0.203 | Best F1=0.203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  3/20: Loss=2.013, Train Acc=17.1%, F1=0.169 | Val Acc=17.0%, F1=0.171 | Best F1=0.203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  4/20: Loss=1.998, Train Acc=15.3%, F1=0.164 | Val Acc=27.4%, F1=0.266 | Best F1=0.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  5/20: Loss=2.008, Train Acc=18.6%, F1=0.198 | Val Acc=13.3%, F1=0.059 | Best F1=0.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  6/20: Loss=1.980, Train Acc=15.0%, F1=0.165 | Val Acc=10.0%, F1=0.053 | Best F1=0.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  7/20: Loss=1.969, Train Acc=18.1%, F1=0.189 | Val Acc=14.7%, F1=0.073 | Best F1=0.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  8/20: Loss=1.977, Train Acc=16.6%, F1=0.165 | Val Acc=29.2%, F1=0.295 | Best F1=0.295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  9/20: Loss=1.981, Train Acc=17.0%, F1=0.185 | Val Acc=23.8%, F1=0.239 | Best F1=0.295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 10/20: Loss=1.979, Train Acc=23.0%, F1=0.237 | Val Acc=31.5%, F1=0.301 | Best F1=0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 11/20: Loss=1.964, Train Acc=18.3%, F1=0.181 | Val Acc=16.2%, F1=0.149 | Best F1=0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 12/20: Loss=1.956, Train Acc=18.8%, F1=0.200 | Val Acc=29.1%, F1=0.298 | Best F1=0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 13/20: Loss=1.952, Train Acc=19.4%, F1=0.193 | Val Acc=28.9%, F1=0.291 | Best F1=0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 14/20: Loss=1.965, Train Acc=19.5%, F1=0.210 | Val Acc=12.6%, F1=0.080 | Best F1=0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 15/20: Loss=1.952, Train Acc=18.8%, F1=0.188 | Val Acc=17.9%, F1=0.148 | Best F1=0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 16/20: Loss=1.947, Train Acc=16.8%, F1=0.163 | Val Acc=26.9%, F1=0.265 | Best F1=0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 17/20: Loss=1.941, Train Acc=17.1%, F1=0.165 | Val Acc=20.8%, F1=0.200 | Best F1=0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 18/20: Loss=1.948, Train Acc=20.5%, F1=0.210 | Val Acc=18.1%, F1=0.179 | Best F1=0.301\n",
      " Early stopping at epoch 18\n",
      " [Audio (CNN)] Best Val: Acc=31.47%, F1=0.3010\n",
      "\n",
      " [Video (ResNet34)] Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  1/20: Loss=2.031, Train Acc=14.3%, F1=0.169 | Val Acc=14.3%, F1=0.128 | Best F1=0.128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  2/20: Loss=1.989, Train Acc=15.0%, F1=0.176 | Val Acc=18.5%, F1=0.215 | Best F1=0.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  3/20: Loss=1.986, Train Acc=15.2%, F1=0.174 | Val Acc=22.8%, F1=0.245 | Best F1=0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  4/20: Loss=1.962, Train Acc=17.3%, F1=0.202 | Val Acc=7.8%, F1=0.093 | Best F1=0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  5/20: Loss=1.967, Train Acc=14.9%, F1=0.179 | Val Acc=7.0%, F1=0.052 | Best F1=0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  6/20: Loss=1.971, Train Acc=11.5%, F1=0.128 | Val Acc=13.4%, F1=0.128 | Best F1=0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  7/20: Loss=1.951, Train Acc=11.3%, F1=0.128 | Val Acc=5.6%, F1=0.050 | Best F1=0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  8/20: Loss=1.944, Train Acc=11.5%, F1=0.129 | Val Acc=10.6%, F1=0.122 | Best F1=0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  9/20: Loss=1.977, Train Acc=16.9%, F1=0.192 | Val Acc=13.2%, F1=0.114 | Best F1=0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 10/20: Loss=1.965, Train Acc=13.7%, F1=0.152 | Val Acc=12.1%, F1=0.108 | Best F1=0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 11/20: Loss=1.958, Train Acc=12.3%, F1=0.135 | Val Acc=9.7%, F1=0.095 | Best F1=0.245\n",
      " Early stopping at epoch 11\n",
      " [Video (ResNet34)] Best Val: Acc=22.81%, F1=0.2453\n",
      "\n",
      " Training fusion model...\n",
      "\n",
      "Learned view weights (from p-values): [0.50197745 0.27187713 0.22614542]\n",
      "Alpha sweep (first 5):\n",
      " alpha=0.01 -> coverage=0.994, size=6.893\n",
      " alpha=0.03 -> coverage=0.976, size=6.475\n",
      " alpha=0.05 -> coverage=0.963, size=5.987\n",
      " alpha=0.07 -> coverage=0.940, size=5.416\n",
      " alpha=0.09 -> coverage=0.922, size=5.118\n"
     ]
    }
   ],
   "source": [
    "# ===== multimodal MELD training + conformal fusion =====\n",
    "import os, gc, warnings, math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import chi2\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------- CONFIGURE YOUR DATA PATH --------------------\n",
    "data_root = \"/depot/gupta869/data/farbod/preprocessed_data\"  # <--- change if needed\n",
    "\n",
    "# -------------------- Utils --------------------\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ==================== MODELS ====================\n",
    "class ImprovedAudioModel(nn.Module):\n",
    "    def __init__(self, num_classes=7, hidden_dim=512, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2), nn.Dropout2d(0.2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2), nn.Dropout2d(0.2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2), nn.Dropout2d(0.2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Dropout2d(0.3)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(512, hidden_dim); self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim); self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim // 2); self.bn3 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim // 2, num_classes)\n",
    "    def forward(self, audio_mel, **kwargs):\n",
    "        x = self.conv1(audio_mel); x = self.conv2(x); x = self.conv3(x); x = self.conv4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        identity = x\n",
    "        x = self.dropout(F.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout(F.relu(self.bn2(self.fc2(x))))\n",
    "        if identity.size(1) == x.size(1):\n",
    "            x = x + identity\n",
    "        x = self.dropout(F.relu(self.bn3(self.fc3(x))))\n",
    "        return self.classifier(x)\n",
    "\n",
    "class ImprovedVideoModel(nn.Module):\n",
    "    def __init__(self, num_classes=7, hidden_dim=512, dropout=0.4):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "        for p in resnet.parameters(): p.requires_grad = False\n",
    "        for p in resnet.layer2.parameters(): p.requires_grad = True\n",
    "        for p in resnet.layer3.parameters(): p.requires_grad = True\n",
    "        for p in resnet.layer4.parameters(): p.requires_grad = True\n",
    "        self.frozen_layers = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool, resnet.layer1)\n",
    "        self.trainable_layers = nn.Sequential(resnet.layer2, resnet.layer3, resnet.layer4, resnet.avgpool)\n",
    "        self.fc1 = nn.Linear(512, hidden_dim); self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim); self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim // 2); self.bn3 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim // 2, num_classes)\n",
    "    def forward(self, face, **kwargs):\n",
    "        self.frozen_layers.eval()            # stable BN stats\n",
    "        x = self.frozen_layers(face)         # no torch.no_grad() here\n",
    "        x = self.trainable_layers(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        identity = x\n",
    "        x1 = self.dropout(F.relu(self.bn1(self.fc1(x))))\n",
    "        x2 = self.dropout(F.relu(self.bn2(self.fc2(x1))))\n",
    "        if identity.size(1) == x2.size(1): x2 = x2 + identity\n",
    "        x3 = self.dropout(F.relu(self.bn3(self.fc3(x2))))\n",
    "        return self.classifier(x3)\n",
    "\n",
    "class ImprovedTextModel(nn.Module):\n",
    "    def __init__(self, num_classes=7, hidden_dim=768, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.text_encoder = RobertaModel.from_pretrained('roberta-base')\n",
    "        for p in self.text_encoder.parameters(): p.requires_grad = False\n",
    "        for p in self.text_encoder.encoder.layer[-8:].parameters(): p.requires_grad = True\n",
    "        for p in self.text_encoder.pooler.parameters(): p.requires_grad = True\n",
    "        text_dim = 768\n",
    "        self.multihead_attn = nn.MultiheadAttention(text_dim, num_heads=4, dropout=dropout, batch_first=True)\n",
    "        self.fc1 = nn.Linear(text_dim, hidden_dim); self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim); self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim // 2); self.ln3 = nn.LayerNorm(hidden_dim // 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim // 2, num_classes)\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hs = out.last_hidden_state\n",
    "        attn_output, _ = self.multihead_attn(hs, hs, hs, key_padding_mask=(attention_mask == 0))\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(attn_output.size()).float()\n",
    "        sum_embeddings = torch.sum(attn_output * mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "        feat = sum_embeddings / sum_mask\n",
    "        x1 = self.dropout(F.gelu(self.ln1(self.fc1(feat))))\n",
    "        x2 = self.dropout(F.gelu(self.ln2(self.fc2(x1))))\n",
    "        x3 = self.dropout(F.gelu(self.ln3(self.fc3(x2 + x1))))\n",
    "        return self.classifier(x3)\n",
    "\n",
    "# ==================== Dataset ====================\n",
    "class PreprocessedMELDDataset(Dataset):\n",
    "    def __init__(self, data_dir, tokenizer, max_length=128, files=None, augment=False, target_audio_T=300):\n",
    "        self.data_dir = data_dir; self.tokenizer = tokenizer; self.max_length = max_length\n",
    "        self.augment = augment; self.target_audio_T = target_audio_T\n",
    "        if files is None:\n",
    "            assert data_dir is not None, \"data_dir must be provided if files is None\"\n",
    "            self.files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.pt')])\n",
    "        else:\n",
    "            self.files = files\n",
    "        self.emotion_map = {'neutral':0,'joy':1,'surprise':2,'anger':3,'sadness':4,'disgust':5,'fear':6}\n",
    "    def __len__(self): return len(self.files)\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.load(self.files[idx], map_location='cpu')\n",
    "\n",
    "        # ----- text\n",
    "        text = sample['utterance']\n",
    "        enc = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        # ----- audio (1 x F x T) with padding-aware normalization\n",
    "        audio_mel = sample['audio_mel']\n",
    "        if isinstance(audio_mel, np.ndarray): audio_mel = torch.from_numpy(audio_mel)\n",
    "        if audio_mel.dim() == 2: audio_mel = audio_mel.unsqueeze(0)\n",
    "        T_target = self.target_audio_T\n",
    "        T = audio_mel.shape[-1]\n",
    "        if T < T_target:\n",
    "            pad_val = float(audio_mel.min()); audio_mel = F.pad(audio_mel, (0, T_target - T), value=pad_val)\n",
    "        elif T > T_target:\n",
    "            audio_mel = audio_mel[:, :, :T_target]; T = T_target\n",
    "        orig_T = min(T, T_target)\n",
    "        pad_mask = torch.zeros(T_target, dtype=torch.bool); pad_mask[:orig_T] = True\n",
    "        audio_lp = torch.log1p(audio_mel.clamp(min=0))\n",
    "        valid = audio_lp[:, :, pad_mask]\n",
    "        mean = valid.mean(); std = valid.std() + 1e-8\n",
    "        audio_mel = (audio_lp - mean) / std\n",
    "        audio_mel = torch.clamp(audio_mel, -3, 3)\n",
    "        if self.augment:\n",
    "            max_w = 20; w = np.random.randint(0, max_w+1); t0 = np.random.randint(0, max(1, T_target - w))\n",
    "            audio_mel[:, :, t0:t0+w] = 0\n",
    "\n",
    "        # ----- face -> 3x224x224 normalized\n",
    "        face = torch.from_numpy(sample['face']).float()\n",
    "        if face.ndim == 2: face = face.unsqueeze(-1).repeat(1,1,3)\n",
    "        if face.shape[-1] == 4: face = face[..., :3]\n",
    "        face = (face / 255.0).permute(2,0,1)\n",
    "        face = TF.resize(face, [224,224], antialias=True)\n",
    "        normalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "        face = normalize(face)\n",
    "\n",
    "        # ----- label\n",
    "        emo = sample['emotion']\n",
    "        label = self.emotion_map.get(emo.lower(), 0) if isinstance(emo, str) else int(emo)\n",
    "\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'audio_mel': audio_mel[:, :, :T_target],\n",
    "            'face': face,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "def collate_fn(batch, max_audio_length=300):\n",
    "    input_ids = torch.stack([it['input_ids'] for it in batch])\n",
    "    attention_mask = torch.stack([it['attention_mask'] for it in batch])\n",
    "    labels = torch.tensor([it['label'] for it in batch])\n",
    "    audio_mels = torch.stack([it['audio_mel'] for it in batch])\n",
    "    faces = torch.stack([it['face'] for it in batch])\n",
    "    return {'input_ids':input_ids,'attention_mask':attention_mask,'audio_mel':audio_mels,'face':faces,'label':labels}\n",
    "\n",
    "# ==================== Train/Eval helpers ====================\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, scheduler=None, grad_clip=5.0, view_name=\"\"):\n",
    "    model.train()\n",
    "    total_loss = 0.0; all_preds, all_labels = [], []\n",
    "    pbar = tqdm(dataloader, desc=f'Training {view_name}', leave=False)\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        ids = batch['input_ids'].to(device); mask = batch['attention_mask'].to(device)\n",
    "        mel = batch['audio_mel'].to(device); face = batch['face'].to(device); y = batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids=ids, attention_mask=mask, audio_mel=mel, face=face)\n",
    "        loss = criterion(logits, y)\n",
    "        if torch.isnan(loss): \n",
    "            print(f\"NaN loss @ batch {batch_idx}, skipping\"); continue\n",
    "        loss.backward()\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None: scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(1).detach().cpu().numpy()\n",
    "        all_preds.extend(preds); all_labels.extend(y.detach().cpu().numpy())\n",
    "        if batch_idx % 50 == 0:\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.3f}', 'grad': f'{float(total_norm):.2f}'})\n",
    "    avg_loss = total_loss / max(1, len(dataloader))\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    return avg_loss, acc, f1\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            ids = batch['input_ids'].to(device); mask = batch['attention_mask'].to(device)\n",
    "            mel = batch['audio_mel'].to(device); face = batch['face'].to(device)\n",
    "            y = batch['label'].cpu().numpy()\n",
    "            logits = model(input_ids=ids, attention_mask=mask, audio_mel=mel, face=face)\n",
    "            preds = logits.argmax(1).cpu().numpy()\n",
    "            all_preds.extend(preds); all_labels.extend(y)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    return acc, f1\n",
    "\n",
    "# ==================== Conformal & Fusion ====================\n",
    "def compute_s(probs, y, score_type, params=None):\n",
    "    if params is None: params = {}\n",
    "    if score_type == 'hinge': return 1 - probs[y]\n",
    "    if score_type == 'cross_entropy': return -np.log(probs[y] + 1e-12)\n",
    "    if score_type == 'margin': return np.max(np.delete(probs, y)) - probs[y]\n",
    "    if score_type == 'raps':\n",
    "        u = params.get('u', 0.1); lam = params.get('lam', 0.01); k_reg = params.get('k_reg', 5)\n",
    "        idx = np.argsort(-probs); ranks = np.empty(len(probs), int); ranks[idx] = np.arange(1, len(probs)+1)\n",
    "        R_y = ranks[y]; cumsum = 0.0\n",
    "        for r in range(1, R_y): cumsum += probs[idx[r-1]]\n",
    "        return cumsum + u*probs[y] + lam*max(R_y - k_reg, 0)\n",
    "    raise ValueError(f\"Unknown score_type: {score_type}\")\n",
    "\n",
    "def compute_nonconformity_scores(model, loader, device, score_type='hinge', params=None):\n",
    "    model.eval(); scores, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ids = batch['input_ids'].to(device); mask = batch['attention_mask'].to(device)\n",
    "            mel = batch['audio_mel'].to(device); face = batch['face'].to(device)\n",
    "            yb = batch['label'].cpu().numpy()\n",
    "            probs = F.softmax(model(input_ids=ids, attention_mask=mask, audio_mel=mel, face=face), dim=1).cpu().numpy()\n",
    "            for i in range(len(yb)): scores.append(compute_s(probs[i], yb[i], score_type, params))\n",
    "            labels.extend(yb)\n",
    "    return np.array(scores), np.array(labels)\n",
    "\n",
    "def classwise_scores(scores, labels, L):\n",
    "    out = {c: [] for c in range(L)}\n",
    "    for s, y in zip(scores, labels): out[int(y)].append(float(s))\n",
    "    return {c: np.asarray(v, float) for c, v in out.items()}\n",
    "\n",
    "def per_view_pvalues_and_probs(model, class_scores, loader, L, device, score_type='hinge', params=None):\n",
    "    model.eval(); probs_all = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ids = batch['input_ids'].to(device); mask = batch['attention_mask'].to(device)\n",
    "            mel = batch['audio_mel'].to(device); face = batch['face'].to(device)\n",
    "            probs = F.softmax(model(input_ids=ids, attention_mask=mask, audio_mel=mel, face=face), dim=1).cpu().numpy()\n",
    "            probs_all.append(probs)\n",
    "            del ids, mask, mel, face\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    probs_all = np.vstack(probs_all); n = probs_all.shape[0]; pvals = np.zeros((n, L))\n",
    "    for i in range(n):\n",
    "        for y in range(L):\n",
    "            s_y = compute_s(probs_all[i], y, score_type, params)\n",
    "            cal = class_scores.get(y, np.array([]))\n",
    "            pvals[i, y] = 1.0 if cal.size == 0 else (1 + np.sum(cal >= s_y)) / (len(cal) + 1)\n",
    "    return pvals, probs_all\n",
    "\n",
    "def build_fusion_features(pvals_list, probs_list): return np.hstack([np.hstack([pvals_list[k], probs_list[k]]) for k in range(len(pvals_list))])\n",
    "def min_p_value_fusion(P_all): K = P_all.shape[0]; return K * np.min(P_all, axis=0)\n",
    "def fisher_fusion(P_all):\n",
    "    eps = 1e-12; p = np.clip(P_all, eps, 1.0)\n",
    "    T = -2 * np.sum(np.log(p), axis=0); df = 2 * P_all.shape[0]\n",
    "    return 1 - chi2.cdf(T, df=df)\n",
    "def adjusted_fisher_fusion(P_train, y_train, P_test, L):\n",
    "    K, _, _ = P_train.shape; n_test = P_test.shape[1]; eps = 1e-12; out = np.zeros((n_test, L))\n",
    "    for y in range(L):\n",
    "        idx = np.where(y_train == y)[0]\n",
    "        if idx.size < 5: out[:, y] = fisher_fusion(P_test)[:, y]; continue\n",
    "        P_cls = np.clip(P_train[:, idx, y], eps, 1.0); W = -2 * np.log(P_cls)\n",
    "        Wc = W - W.mean(axis=1, keepdims=True); Sigma = (Wc @ Wc.T) / max(W.shape[1] - 1, 1)\n",
    "        var_T = np.sum(Sigma); \n",
    "        if not np.isfinite(var_T) or var_T <= 0: var_T = 4 * K\n",
    "        f_y = (8.0 * K * K) / var_T; c_y = var_T / (4 * K)\n",
    "        P_t = np.clip(P_test[:, :, y], eps, 1.0); T_t = -2 * np.sum(np.log(P_t), axis=0)\n",
    "        out[:, y] = 1 - chi2.cdf(T_t / c_y, df=f_y)\n",
    "    return out\n",
    "def weighted_average_fusion(P_all, weights): return np.tensordot(weights, P_all, axes=(0, 0))\n",
    "def learn_view_weights_from_pvals(pv_train_concat, y_train, K, L, max_iter, seed):\n",
    "    lr = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=max_iter, random_state=seed, class_weight='balanced')\n",
    "    lr.fit(pv_train_concat, y_train); B = lr.coef_; imps = []\n",
    "    for k in range(K): imps.append(np.linalg.norm(B[:, k*L:(k+1)*L], ord=\"fro\"))\n",
    "    w = np.maximum(np.array(imps, float), 1e-12); return w / w.sum()\n",
    "def fused_class_cal_scores(y_cal, fused_probs_cal, L):\n",
    "    s = 1 - fused_probs_cal[np.arange(len(y_cal)), y_cal]; out = {c: [] for c in range(L)}\n",
    "    for sc, yy in zip(s, y_cal): out[int(yy)].append(float(sc))\n",
    "    return {c: np.asarray(v, float) for c, v in out.items()}\n",
    "def fused_p_values_from_cal(fused_probs, cal_class_scores):\n",
    "    n, L = fused_probs.shape; out = np.zeros((n, L))\n",
    "    for y in range(L):\n",
    "        cal = cal_class_scores.get(y, np.array([]))\n",
    "        if cal.size == 0: out[:, y] = 1.0\n",
    "        else:\n",
    "            s_test = 1 - fused_probs[:, y]\n",
    "            counts = np.sum(cal[:, None] >= s_test[None, :], axis=0)\n",
    "            out[:, y] = (1 + counts) / (len(cal) + 1)\n",
    "    return out\n",
    "def evaluate_sets(P, y_true, alpha):\n",
    "    C = (P > alpha); cov = float(np.mean(C[np.arange(len(y_true)), y_true])); size = float(np.mean(C.sum(axis=1)))\n",
    "    return cov, size\n",
    "\n",
    "# ==================== Config ====================\n",
    "@dataclass\n",
    "class MELDConfig:\n",
    "    alpha: float = 0.1\n",
    "    Ks: Tuple[int, ...] = (3,)\n",
    "    num_classes: int = 7\n",
    "    num_simulations: int = 1\n",
    "    epochs_text: int = 5\n",
    "    epochs_audio: int = 20\n",
    "    epochs_video: int = 20\n",
    "    lr_text: float = 5e-5\n",
    "    lr_audio: float = 3e-3\n",
    "    lr_video: float = 3e-4\n",
    "    batch_size: int = 64\n",
    "    inference_batch_size: int = 8\n",
    "    warmup_ratio: float = 0.15\n",
    "    max_iter_lr: int = 1000\n",
    "    train_seed_base: int = 42\n",
    "    weight_decay_text: float = 0.01\n",
    "    weight_decay_av: float = 0.0\n",
    "    label_smoothing: float = 0.0\n",
    "    grad_clip: float = 5.0\n",
    "    patience: int = 8\n",
    "    train_frac: float = 0.6\n",
    "    cal_frac_of_temp: float = 0.25\n",
    "    fuse_train_frac_of_rest: float = 0.7\n",
    "\n",
    "# ==================== Environment & data ====================\n",
    "device = None\n",
    "tokenizer = None\n",
    "full_train_files = None\n",
    "test_files = None\n",
    "Y_full = None\n",
    "Y_test = None\n",
    "\n",
    "def setup_environment():\n",
    "    global device, tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\"); print(\"Using MPS device\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\"); print(\"Using CPU device\")\n",
    "    print(\"Loading RoBERTa tokenizer...\")\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    return device, tokenizer\n",
    "\n",
    "def load_dataset_labels(data_dir: str):\n",
    "    emotion_map = {'neutral':0,'joy':1,'surprise':2,'anger':3,'sadness':4,'disgust':5,'fear':6}\n",
    "    print(f\"\\nLoading data from: {data_dir}\")\n",
    "    train_dir = os.path.join(data_dir, 'train'); dev_dir = os.path.join(data_dir, 'dev'); test_dir = os.path.join(data_dir, 'test')\n",
    "    train_files = [os.path.join(train_dir, f) for f in os.listdir(train_dir) if f.endswith('.pt')]\n",
    "    dev_files   = [os.path.join(dev_dir,   f) for f in os.listdir(dev_dir)   if f.endswith('.pt')]\n",
    "    test_files_ = [os.path.join(test_dir,  f) for f in os.listdir(test_dir)  if f.endswith('.pt')]\n",
    "    full_train_files_ = sorted(train_files + dev_files); test_files_ = sorted(test_files_)\n",
    "    print(\"Loading labels for stratification...\")\n",
    "    Y_full_ = []\n",
    "    for f in tqdm(full_train_files_, desc=\"Loading train labels\"):\n",
    "        sample = torch.load(f, map_location='cpu')\n",
    "        emo = sample['emotion'].lower() if isinstance(sample['emotion'], str) else str(sample['emotion'])\n",
    "        Y_full_.append(emotion_map.get(emo, 0))\n",
    "    Y_full_ = np.array(Y_full_)\n",
    "    Y_test_ = []\n",
    "    for f in tqdm(test_files_, desc=\"Loading test labels\"):\n",
    "        sample = torch.load(f, map_location='cpu')\n",
    "        emo = sample['emotion'].lower() if isinstance(sample['emotion'], str) else str(sample['emotion'])\n",
    "        Y_test_.append(emotion_map.get(emo, 0))\n",
    "    Y_test_ = np.array(Y_test_)\n",
    "    print(f\"\\nDataset Statistics:\\n Train samples: {len(Y_full_)}\\n Test samples: {len(Y_test_)}\")\n",
    "    names = ['neutral','joy','surprise','anger','sadness','disgust','fear']\n",
    "    print(\"\\nClass distribution (train):\")\n",
    "    for i, n in enumerate(names):\n",
    "        c = int((Y_full_ == i).sum()); print(f\" {n:10s}: {c:4d} ({c/len(Y_full_)*100:.1f}%)\")\n",
    "    return full_train_files_, test_files_, Y_full_, Y_test_\n",
    "\n",
    "# ==================== Runner ====================\n",
    "def run_once_collect_P(cfg: MELDConfig, score_type: str, score_params: dict):\n",
    "    global full_train_files, test_files, Y_full, Y_test, device\n",
    "    seed = cfg.train_seed_base; set_seed(seed)\n",
    "\n",
    "    indices = np.arange(len(full_train_files))\n",
    "    trP_idx, tmp_idx = train_test_split(indices, test_size=1 - cfg.train_frac, stratify=Y_full, random_state=seed)\n",
    "    cal_idx, rest_idx = train_test_split(tmp_idx, test_size=1 - cfg.cal_frac_of_temp, stratify=Y_full[tmp_idx], random_state=seed)\n",
    "    ftr_idx, fcal_idx = train_test_split(rest_idx, test_size=1 - cfg.fuse_train_frac_of_rest, stratify=Y_full[rest_idx], random_state=seed)\n",
    "\n",
    "    X_trP_files = [full_train_files[i] for i in trP_idx]\n",
    "    X_cal_files = [full_train_files[i] for i in cal_idx]\n",
    "    X_fuse_tr_files = [full_train_files[i] for i in ftr_idx]\n",
    "    X_fuse_cal_files = [full_train_files[i] for i in fcal_idx]\n",
    "    X_te_files = test_files\n",
    "    y_fuse_tr = Y_full[ftr_idx]; y_fuse_cal = Y_full[fcal_idx]; y_te = Y_test\n",
    "\n",
    "    train_dataset = PreprocessedMELDDataset(None, tokenizer, files=X_trP_files, augment=True)\n",
    "    cal_dataset   = PreprocessedMELDDataset(None, tokenizer, files=X_cal_files, augment=False)\n",
    "    ftr_dataset   = PreprocessedMELDDataset(None, tokenizer, files=X_fuse_tr_files, augment=False)\n",
    "    fcal_dataset  = PreprocessedMELDDataset(None, tokenizer, files=X_fuse_cal_files, augment=False)\n",
    "    te_dataset    = PreprocessedMELDDataset(None, tokenizer, files=X_te_files, augment=False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
    "                              collate_fn=lambda b: collate_fn(b, 300), num_workers=4, pin_memory=True, drop_last=True)\n",
    "    cal_loader   = DataLoader(cal_dataset, batch_size=cfg.inference_batch_size, shuffle=False,\n",
    "                              collate_fn=lambda b: collate_fn(b, 300), num_workers=4, pin_memory=True)\n",
    "    ftr_loader   = DataLoader(ftr_dataset, batch_size=cfg.inference_batch_size, shuffle=False,\n",
    "                              collate_fn=lambda b: collate_fn(b, 300), num_workers=4, pin_memory=True)\n",
    "    fcal_loader  = DataLoader(fcal_dataset, batch_size=cfg.inference_batch_size, shuffle=False,\n",
    "                              collate_fn=lambda b: collate_fn(b, 300), num_workers=4, pin_memory=True)\n",
    "    te_loader    = DataLoader(te_dataset, batch_size=cfg.inference_batch_size, shuffle=False,\n",
    "                              collate_fn=lambda b: collate_fn(b, 300), num_workers=4, pin_memory=True)\n",
    "\n",
    "    counts = np.bincount(Y_full, minlength=cfg.num_classes).astype(np.float32)\n",
    "    cls_weights = counts.sum() / (counts + 1e-6); cls_weights = cls_weights / cls_weights.mean()\n",
    "    cls_weights_t = torch.tensor(cls_weights, dtype=torch.float32, device=device)\n",
    "\n",
    "    model_classes = [ImprovedTextModel, ImprovedAudioModel, ImprovedVideoModel]\n",
    "    learning_rates = [cfg.lr_text, cfg.lr_audio, cfg.lr_video]\n",
    "    model_names = [\"Text (RoBERTa)\", \"Audio (CNN)\", \"Video (ResNet34)\"]\n",
    "    epochs_per_model = [cfg.epochs_text, cfg.epochs_audio, cfg.epochs_video]\n",
    "\n",
    "    models_local, pr_te = [], []\n",
    "\n",
    "    print(\"\\nExperiment Configuration:\")\n",
    "    print(f\" Device: {device.type}\")\n",
    "    print(f\" Epochs: Text={cfg.epochs_text}, Audio={cfg.epochs_audio}, Video={cfg.epochs_video}\")\n",
    "    print(f\" LRs: Text={cfg.lr_text}, Audio={cfg.lr_audio}, Video={cfg.lr_video}\")\n",
    "    print(f\" Batch size: {cfg.batch_size} (train), {cfg.inference_batch_size} (inf)\")\n",
    "\n",
    "    for v in range(3):\n",
    "        print(f\"\\n [{model_names[v]}] Training...\")\n",
    "        m = model_classes[v](num_classes=cfg.num_classes).to(device)\n",
    "\n",
    "        if v == 0:\n",
    "            criterion = nn.CrossEntropyLoss(weight=cls_weights_t)\n",
    "            base_params = [p for n, p in m.named_parameters() if 'text_encoder.encoder.layer' in n]\n",
    "            other_params = [p for n, p in m.named_parameters() if 'text_encoder.encoder.layer' not in n]\n",
    "            optimizer = torch.optim.AdamW([\n",
    "                {'params': base_params, 'lr': learning_rates[v]},\n",
    "                {'params': other_params, 'lr': learning_rates[v] * 10}\n",
    "            ], weight_decay=cfg.weight_decay_text)\n",
    "            total_steps = epochs_per_model[v] * len(train_loader)\n",
    "            warmup_steps = int(total_steps * cfg.warmup_ratio)\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "        elif v == 1:\n",
    "            criterion = nn.CrossEntropyLoss(weight=cls_weights_t)\n",
    "            optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rates[v], weight_decay=cfg.weight_decay_av)\n",
    "            scheduler = None\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss(weight=cls_weights_t)\n",
    "            trainable = [p for n, p in m.named_parameters() if p.requires_grad]\n",
    "            optimizer = torch.optim.AdamW(trainable, lr=learning_rates[v], weight_decay=cfg.weight_decay_av)\n",
    "            scheduler = None\n",
    "\n",
    "        best_f1, best_acc, best_state, patience_counter = 0.0, 0.0, None, 0\n",
    "\n",
    "        for epoch in range(epochs_per_model[v]):\n",
    "            tr_loss, tr_acc, tr_f1 = train_epoch(m, train_loader, optimizer, criterion, device, scheduler=scheduler, grad_clip=cfg.grad_clip, view_name=model_names[v])\n",
    "            va_acc, va_f1 = evaluate(m, cal_loader, device)\n",
    "            if va_f1 > best_f1:\n",
    "                best_f1, best_acc, patience_counter = va_f1, va_acc, 0\n",
    "                best_state = {k: v_.detach().cpu() for k, v_ in m.state_dict().items()}\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            print(f\" Epoch {epoch+1:2d}/{epochs_per_model[v]}: Loss={tr_loss:.3f}, Train Acc={tr_acc*100:.1f}%, F1={tr_f1:.3f} | Val Acc={va_acc*100:.1f}%, F1={va_f1:.3f} | Best F1={best_f1:.3f}\")\n",
    "            if patience_counter >= cfg.patience:\n",
    "                print(f\" Early stopping at epoch {epoch+1}\"); break\n",
    "\n",
    "        if best_state is not None: m.load_state_dict(best_state)\n",
    "        print(f\" [{model_names[v]}] Best Val: Acc={best_acc*100:.2f}%, F1={best_f1:.4f}\")\n",
    "        models_local.append(m)\n",
    "\n",
    "        _, pr_te_v = per_view_pvalues_and_probs(m, {}, te_loader, cfg.num_classes, device)\n",
    "        pr_te.append(pr_te_v)\n",
    "\n",
    "        del best_state, optimizer, scheduler, criterion\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # --- Conformal per view\n",
    "    score_params_local = score_params\n",
    "    pv_tr, pr_tr, pv_cal, pr_cal, pv_te_list = [], [], [], [], []\n",
    "    for v in range(3):\n",
    "        m = models_local[v]; m.to(device); m.eval()\n",
    "        sc, lab = compute_nonconformity_scores(m, cal_loader, device, score_type=score_type, params=score_params_local)\n",
    "        cal_cls = classwise_scores(sc, lab, cfg.num_classes)\n",
    "        p_tr,  pr_tr_v  = per_view_pvalues_and_probs(m, cal_cls, ftr_loader,  cfg.num_classes, device, score_type=score_type, params=score_params_local)\n",
    "        p_cal, pr_cal_v = per_view_pvalues_and_probs(m, cal_cls, fcal_loader, cfg.num_classes, device, score_type=score_type, params=score_params_local)\n",
    "        p_te,  _        = per_view_pvalues_and_probs(m, cal_cls, te_loader,   cfg.num_classes, device, score_type=score_type, params=score_params_local)\n",
    "        pv_tr.append(p_tr); pr_tr.append(pr_tr_v); pv_cal.append(p_cal); pr_cal.append(pr_cal_v); pv_te_list.append(p_te)\n",
    "        m.to('cpu'); \n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # --- Fusion\n",
    "    print(\"\\n Training fusion model...\")\n",
    "    X_ftr = build_fusion_features(pv_tr, pr_tr)\n",
    "    fusion_lr = LogisticRegression(max_iter=cfg.max_iter_lr, multi_class=\"multinomial\", solver=\"lbfgs\", random_state=seed, C=1.0, class_weight='balanced')\n",
    "    fusion_lr.fit(X_ftr, y_fuse_tr)\n",
    "    X_fcal = build_fusion_features(pv_cal, pr_cal)\n",
    "    fused_probs_cal = fusion_lr.predict_proba(X_fcal)\n",
    "    fused_cal_scores = fused_class_cal_scores(y_fuse_cal, fused_probs_cal, cfg.num_classes)\n",
    "    X_ftest = build_fusion_features(pv_te_list, pr_te)\n",
    "    fused_probs_test = fusion_lr.predict_proba(X_ftest)\n",
    "    P_cf = fused_p_values_from_cal(fused_probs_test, fused_cal_scores)\n",
    "\n",
    "    # --- Baselines\n",
    "    P_train = np.stack(pv_tr, axis=0); P_test = np.stack(pv_te_list, axis=0)\n",
    "    P_min = min_p_value_fusion(P_test)\n",
    "    P_fish = fisher_fusion(P_test)\n",
    "    P_adjF = adjusted_fisher_fusion(P_train, y_fuse_tr, P_test, cfg.num_classes)\n",
    "    pv_tr_concat = np.concatenate(pv_tr, axis=1); w_learned = learn_view_weights_from_pvals(pv_tr_concat, y_fuse_tr, 3, cfg.num_classes, cfg.max_iter_lr, seed)\n",
    "    P_wavgL = weighted_average_fusion(P_test, w_learned)\n",
    "\n",
    "    P_dict = {\"CLF\": P_cf, \"Min p-Value\": P_min, \"Fisher's\": P_fish, \"Adjusted Fisher's\": P_adjF, \"Weighted Average\": P_wavgL}\n",
    "    return P_dict, y_te, w_learned\n",
    "\n",
    "def compute_curves_over_alpha(P: np.ndarray, y_true: np.ndarray, alphas: np.ndarray):\n",
    "    cov, size = [], []\n",
    "    for a in alphas:\n",
    "        c, s = evaluate_sets(P, y_true, a)\n",
    "        cov.append(c); size.append(s)\n",
    "    return np.array(cov), np.array(size)\n",
    "\n",
    "# ==================== RUN (directly in notebook) ====================\n",
    "device, tokenizer = setup_environment()\n",
    "full_train_files, test_files, Y_full, Y_test = load_dataset_labels(data_root)\n",
    "\n",
    "cfg = MELDConfig()\n",
    "print(\"\\nExperiment Configuration:\")\n",
    "print(f\" Device: {device.type}\")\n",
    "print(f\" Epochs: Text={cfg.epochs_text}, Audio={cfg.epochs_audio}, Video={cfg.epochs_video}\")\n",
    "print(f\" LRs: Text={cfg.lr_text}, Audio={cfg.lr_audio}, Video={cfg.lr_video}\")\n",
    "print(f\" Batch size: {cfg.batch_size} (train), {cfg.inference_batch_size} (inf)\")\n",
    "\n",
    "score_type = \"hinge\"\n",
    "score_params = {}\n",
    "\n",
    "P_dict, y_te, w_learned = run_once_collect_P(cfg, score_type, score_params)\n",
    "\n",
    "alphas = np.linspace(0.01, 0.5, 25)\n",
    "P_clf = P_dict[\"CLF\"]\n",
    "cov, size = compute_curves_over_alpha(P_clf, y_te, alphas)\n",
    "print(\"\\nLearned view weights (from p-values):\", w_learned)\n",
    "print(\"Alpha sweep (first 5):\")\n",
    "for a, c, s in list(zip(alphas, cov, size))[:5]:\n",
    "    print(f\" alpha={a:.2f} -> coverage={c:.3f}, size={s:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fea40f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figure to ./meld_alpha_sweep_v2.pdf\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 254.88x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use(['science', 'no-latex'])\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams.update({\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "    \"font.size\": 8,\n",
    "    \"axes.titlesize\": 9,\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"xtick.labelsize\": 7,\n",
    "    \"ytick.labelsize\": 7,\n",
    "    \"legend.fontsize\": 5,\n",
    "    \"lines.linewidth\": 1.0,\n",
    "    \"axes.linewidth\": 0.5,\n",
    "})\n",
    "\n",
    "def plot_alpha_sweep(P_dict: Dict[str, np.ndarray], y_true: np.ndarray, alphas: np.ndarray, outpath: str):\n",
    "    methods_order = [\"CLF\",\"Adjusted Fisher's\",\"Fisher's\",\"Min p-Value\",\"Weighted Average\"]\n",
    "    colors = sns.color_palette(\"tab10\", n_colors=len(methods_order))\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(3.54, 2), constrained_layout=True)\n",
    "\n",
    "    ax = axs[0]\n",
    "    for idx, m in enumerate(methods_order):\n",
    "        cov, _ = compute_curves_over_alpha(P_dict[m], y_true, alphas)\n",
    "        ax.plot(alphas, cov, label=m, color=colors[idx])\n",
    "    ax.plot(alphas, 1 - alphas, linestyle='--', color='k', label=r'Target 1-$\\alpha$')\n",
    "    ax.set_xlabel(r'$\\alpha$')\n",
    "    ax.set_ylabel('Coverage')\n",
    "    ax.set_title('Coverage vs $\\\\,\\\\alpha$')\n",
    "    ax.set_ylim(0.6, 1.01)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='best', ncol=1, frameon=False)\n",
    "\n",
    "    ax = axs[1]\n",
    "    for idx, m in enumerate(methods_order):\n",
    "        _, size = compute_curves_over_alpha(P_dict[m], y_true, alphas)\n",
    "        ax.plot(alphas, size, label=m, color=colors[idx])\n",
    "    ax.set_xlabel(r'$\\alpha$')\n",
    "    ax.set_ylabel('Average set size')\n",
    "    ax.set_title('Set size vs $\\\\,\\\\alpha$')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.savefig(outpath, bbox_inches='tight', dpi=600)\n",
    "    print(f\"Saved figure to {outpath}\")\n",
    "    plt.show()\n",
    "    \n",
    "alphas = np.linspace(0.01, 0.30, 30)\n",
    "plot_alpha_sweep(P_dict, y_te, alphas, outpath=\"./meld_alpha_sweep_v2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac719559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
