{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024207ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90744fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "=== Simulation 1/10 ===\n",
      "\n",
      "  -> K = 2\n",
      "    [View 1/2] training...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 57.56 MiB is free. Process 290455 has 5.71 GiB memory in use. Process 291637 has 4.68 GiB memory in use. Process 295770 has 2.60 GiB memory in use. Including non-PyTorch memory, this process has 10.53 GiB memory in use. Of the allocated memory 10.22 GiB is allocated by PyTorch, and 45.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_295709/600173243.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_295709/600173243.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFARConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0mdf_cov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Coverage (raw rows) ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_cov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_295709/600173243.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"    [View {v+1}/{num_views}] training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictorCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                 \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_per_view\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m                 \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_nonconformity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_295709/600173243.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, num_epochs, lr)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_295709/600173243.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_res_connect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/ops/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 57.56 MiB is free. Process 290455 has 5.71 GiB memory in use. Process 291637 has 4.68 GiB memory in use. Process 295770 has 2.60 GiB memory in use. Including non-PyTorch memory, this process has 10.53 GiB memory in use. Of the allocated memory 10.22 GiB is allocated by PyTorch, and 45.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18, efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# Config\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class CIFARConfig:\n",
    "    # core\n",
    "    alpha: float = 0.1\n",
    "    Ks: Tuple[int, ...] = (2, 3, 4, 6)\n",
    "    num_classes: int = 100\n",
    "    num_simulations: int = 10\n",
    "\n",
    "    # training\n",
    "    epochs_per_view: int = 75      # adjust for runtime; your old code used 200\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 8192\n",
    "    max_iter_lr: int = 1000         # for sklearn LR (fusion & weight-learning)\n",
    "    train_seed_base: int = 41\n",
    "\n",
    "    # data split fractions (similar structure to synthetic)\n",
    "    train_frac: float = 0.5         # predictor training from full train set\n",
    "    cal_frac_of_temp: float = 0.3   # portion of temp used as calibration for per-view\n",
    "    fuse_train_frac_of_rest: float = 0.7  # remaining split into fusion_train/cal\n",
    "\n",
    "# =============================================================================\n",
    "# Torch / Data\n",
    "# =============================================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load once\n",
    "train_dataset = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "X_train_full = train_dataset.data.astype(np.float32) / 255.0   # (50000, 32, 32, 3)\n",
    "Y_train_full = np.array(train_dataset.targets)\n",
    "X_test_full  = test_dataset.data.astype(np.float32) / 255.0    # (10000, 32, 32, 3)\n",
    "Y_test_full  = np.array(test_dataset.targets)\n",
    "\n",
    "# =============================================================================\n",
    "# Multi-view (patch) utilities\n",
    "# =============================================================================\n",
    "\n",
    "def split_image_into_k_patches(image: torch.Tensor, k: int) -> List[torch.Tensor]:\n",
    "    # image: (C, H, W) = (3,32,32)\n",
    "    C, H, W = image.shape\n",
    "    if k == 4:\n",
    "        # 2x2 grid\n",
    "        patches = []\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                patch = image[:, i*16:(i+1)*16, j*16:(j+1)*16]\n",
    "                patches.append(patch)\n",
    "        return patches\n",
    "    else:\n",
    "        # vertical stripes\n",
    "        base_width = W // k\n",
    "        remainder = W % k\n",
    "        patches, start = [], 0\n",
    "        for idx in range(k):\n",
    "            width = base_width + (1 if idx < remainder else 0)\n",
    "            patch = image[:, :, start:start+width]\n",
    "            patches.append(patch)\n",
    "            start += width\n",
    "        return patches\n",
    "\n",
    "class PatchesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images: np.ndarray, labels: np.ndarray, k: int, view: int):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.k = k\n",
    "        self.view = view\n",
    "        self.resize = Resize((32, 32))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img = self.images[idx].transpose((2, 0, 1))   # (3,32,32)\n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "        patches = split_image_into_k_patches(img, self.k)\n",
    "        patch = patches[self.view]\n",
    "        patch = self.resize(patch)\n",
    "        label = int(self.labels[idx])\n",
    "        return patch, label\n",
    "\n",
    "# =============================================================================\n",
    "# ResNet18 per view\n",
    "# =============================================================================\n",
    "\n",
    "# class PredictorCNN(nn.Module):\n",
    "#     def __init__(self, num_classes=100):\n",
    "#         super().__init__()\n",
    "#         self.model = resnet18(pretrained=False, num_classes=num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "    \n",
    "class PredictorCNN(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super().__init__()\n",
    "        # Load pretrained EfficientNet-B0 on ImageNet\n",
    "        self.model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Replace final classification layer\n",
    "        in_features = self.model.classifier[1].in_features\n",
    "        self.model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader, num_epochs=100, lr=1e-3):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt  = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    for ep in range(num_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), torch.tensor(yb, dtype=torch.long, device=device)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if (ep+1) % 25 == 0:\n",
    "            print(f\"  epoch {ep+1}/{num_epochs}\")\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# Conformal utilities (torch models)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_nonconformity_scores(model: nn.Module, loader) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    scores, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs  = F.softmax(logits, dim=1)\n",
    "            idx    = torch.arange(probs.size(0), device=probs.device)\n",
    "            true_p = probs[idx, torch.tensor(yb, dtype=torch.long, device=probs.device)]\n",
    "            s = (1 - true_p).detach().cpu().numpy()\n",
    "            scores.extend(s)\n",
    "            labels.extend(yb.numpy())\n",
    "    return np.asarray(scores, float), np.asarray(labels, int)\n",
    "\n",
    "def classwise_scores(scores: np.ndarray, labels: np.ndarray, L: int) -> Dict[int, np.ndarray]:\n",
    "    out = {c: [] for c in range(L)}\n",
    "    for s, y in zip(scores, labels):\n",
    "        out[int(y)].append(float(s))\n",
    "    return {c: np.asarray(v, float) for c, v in out.items()}\n",
    "\n",
    "def per_view_pvalues_and_probs(\n",
    "    model: nn.Module, class_scores: Dict[int, np.ndarray], loader, L: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return p-values (n,L) and probs (n,L) for a single view.\"\"\"\n",
    "    model.eval()\n",
    "    probs_all = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs  = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "            probs_all.append(probs)\n",
    "    probs_all = np.vstack(probs_all)  # (n, L)\n",
    "\n",
    "    n = probs_all.shape[0]\n",
    "    pvals = np.zeros((n, L))\n",
    "    for y in range(L):\n",
    "        cal = class_scores.get(y, np.array([]))\n",
    "        if cal.size == 0:\n",
    "            pvals[:, y] = 1.0\n",
    "        else:\n",
    "            s_test = 1 - probs_all[:, y]\n",
    "            counts = np.sum(cal[:, None] >= s_test[None, :], axis=0)\n",
    "            pvals[:, y] = (1 + counts) / (len(cal) + 1)\n",
    "    return pvals, probs_all\n",
    "\n",
    "# =============================================================================\n",
    "# Fusion utilities (same as synthetic)\n",
    "# =============================================================================\n",
    "\n",
    "def build_fusion_features(pvals_list: List[np.ndarray], probs_list: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Horizontally stack [pvals, probs] for each view -> (n, K*2L)\"\"\"\n",
    "    blocks = [np.hstack([pvals_list[k], probs_list[k]]) for k in range(len(pvals_list))]\n",
    "    return np.hstack(blocks)\n",
    "\n",
    "def min_p_value_fusion(P_all: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"K * min_k p_k^y. P_all: (K,n,L) -> (n,L)\"\"\"\n",
    "    K = P_all.shape[0]\n",
    "    return K * np.min(P_all, axis=0)\n",
    "\n",
    "def fisher_fusion(P_all: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Standard Fisher.\"\"\"\n",
    "    eps = 1e-12\n",
    "    p = np.clip(P_all, eps, 1.0)\n",
    "    T = -2 * np.sum(np.log(p), axis=0)\n",
    "    df = 2 * P_all.shape[0]\n",
    "    return 1 - chi2.cdf(T, df=df)\n",
    "\n",
    "def adjusted_fisher_fusion(P_train: np.ndarray, y_train: np.ndarray, P_test: np.ndarray, L: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Moment-matched Fisher per class: fit variance of T_y = sum_k -2log p_k^y on per-class train,\n",
    "    then use scaled-chi-square CDF.\n",
    "    \"\"\"\n",
    "    K, _, _ = P_train.shape\n",
    "    n_test = P_test.shape[1]\n",
    "    eps = 1e-12\n",
    "    out = np.zeros((n_test, L))\n",
    "    for y in range(L):\n",
    "        idx = np.where(y_train == y)[0]\n",
    "        if idx.size < 5:\n",
    "            out[:, y] = fisher_fusion(P_test)[:, y]\n",
    "            continue\n",
    "        P_cls = np.clip(P_train[:, idx, y], eps, 1.0)  # (K, n_y)\n",
    "        W = -2 * np.log(P_cls)                          # (K, n_y)\n",
    "        Wc = W - W.mean(axis=1, keepdims=True)\n",
    "        Sigma = (Wc @ Wc.T) / max(W.shape[1] - 1, 1)    # (K, K)\n",
    "        var_T = np.sum(Sigma)\n",
    "        if not np.isfinite(var_T) or var_T <= 0:\n",
    "            var_T = 4 * K\n",
    "        f_y = (8.0 * K * K) / var_T\n",
    "        c_y = var_T / (4 * K)\n",
    "\n",
    "        P_t = np.clip(P_test[:, :, y], eps, 1.0)\n",
    "        T_t = -2 * np.sum(np.log(P_t), axis=0)\n",
    "        out[:, y] = 1 - chi2.cdf(T_t / c_y, df=f_y)\n",
    "    return out\n",
    "\n",
    "def weighted_average_fusion(P_all: np.ndarray, weights: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"sum_k w_k p_k^y; P_all: (K,n,L), weights: (K,)\"\"\"\n",
    "    return np.tensordot(weights, P_all, axes=(0, 0))\n",
    "\n",
    "def learn_view_weights_from_pvals(pv_train_concat: np.ndarray, y_train: np.ndarray, K: int, L: int, max_iter: int, seed: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Train multinomial LR on p-only features to predict y.\n",
    "    Convert coef_ (L, K*L) -> view weights by Frobenius norm per (LÃ—L) block.\n",
    "    \"\"\"\n",
    "    lr = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=max_iter, random_state=seed)\n",
    "    lr.fit(pv_train_concat, y_train)\n",
    "    B = lr.coef_  # (L, K*L)\n",
    "    imps = []\n",
    "    for k in range(K):\n",
    "        block = B[:, k*L:(k+1)*L]\n",
    "        imps.append(np.linalg.norm(block, ord=\"fro\"))\n",
    "    w = np.array(imps, float)\n",
    "    w = np.maximum(w, 1e-12)\n",
    "    return w / w.sum()\n",
    "\n",
    "# Conformalization of fused model\n",
    "def fused_class_cal_scores(y_cal: np.ndarray, fused_probs_cal: np.ndarray, L: int) -> Dict[int, np.ndarray]:\n",
    "    s = 1 - fused_probs_cal[np.arange(len(y_cal)), y_cal]\n",
    "    out = {c: [] for c in range(L)}\n",
    "    for sc, yy in zip(s, y_cal):\n",
    "        out[int(yy)].append(float(sc))\n",
    "    return {c: np.asarray(v, float) for c, v in out.items()}\n",
    "\n",
    "def fused_p_values_from_cal(fused_probs: np.ndarray, cal_class_scores: Dict[int, np.ndarray]) -> np.ndarray:\n",
    "    n, L = fused_probs.shape\n",
    "    out = np.zeros((n, L))\n",
    "    for y in range(L):\n",
    "        cal = cal_class_scores.get(y, np.array([]))\n",
    "        if cal.size == 0:\n",
    "            out[:, y] = 1.0\n",
    "        else:\n",
    "            s_test = 1 - fused_probs[:, y]\n",
    "            counts = np.sum(cal[:, None] >= s_test[None, :], axis=0)\n",
    "            out[:, y] = (1 + counts) / (len(cal) + 1)\n",
    "    return out\n",
    "\n",
    "def evaluate_sets(P: np.ndarray, y_true: np.ndarray, alpha: float) -> Tuple[float, float]:\n",
    "    C = (P > alpha)\n",
    "    cov = float(np.mean(C[np.arange(len(y_true)), y_true]))\n",
    "    size = float(np.mean(C.sum(axis=1)))\n",
    "    return cov, size\n",
    "\n",
    "def summarize_table(df: pd.DataFrame, methods: List[str], metric_name: str) -> pd.DataFrame:\n",
    "    g = df.groupby(\"K\").agg({m: [\"mean\", \"std\"] for m in methods})\n",
    "    g.columns = [f\"{a}_{b}\" for a, b in g.columns]\n",
    "    g = g.reset_index()\n",
    "    for m in methods:\n",
    "        g[m] = g.apply(lambda r: f\"{r[f'{m}_mean']:.2f} ({r[f'{m}_std']:.2f})\", axis=1)\n",
    "    g.insert(1, \"Metric\", metric_name)\n",
    "    return g[[\"K\", \"Metric\"] + methods]\n",
    "\n",
    "# =============================================================================\n",
    "# Main experiment\n",
    "# =============================================================================\n",
    "\n",
    "def run_experiments(cfg: CIFARConfig):\n",
    "    results_cov, results_size, results_acc = [], [], []\n",
    "\n",
    "    for sim in range(cfg.num_simulations):\n",
    "        print(f\"\\n=== Simulation {sim+1}/{cfg.num_simulations} ===\")\n",
    "        seed = cfg.train_seed_base + sim\n",
    "        rng = np.random.RandomState(seed)\n",
    "\n",
    "        # Splits (train for view predictors, then cal/fusion splits)\n",
    "        X_trP, X_tmp, y_trP, y_tmp = train_test_split(\n",
    "            X_train_full, Y_train_full, test_size=1 - cfg.train_frac, stratify=Y_train_full, random_state=seed\n",
    "        )\n",
    "        # We will split X_tmp into cal and fusion pools\n",
    "        X_cal, X_rest, y_cal, y_rest = train_test_split(\n",
    "            X_tmp, y_tmp, test_size=1 - cfg.cal_frac_of_temp, stratify=y_tmp, random_state=seed\n",
    "        )\n",
    "        X_fuse_tr, X_fuse_cal, y_fuse_tr, y_fuse_cal = train_test_split(\n",
    "            X_rest, y_rest, test_size=1 - cfg.fuse_train_frac_of_rest, stratify=y_rest, random_state=seed\n",
    "        )\n",
    "        X_te, y_te = X_test_full, Y_test_full\n",
    "\n",
    "        for K in cfg.Ks:\n",
    "            print(f\"\\n  -> K = {K}\")\n",
    "            num_views = 4 if K == 4 else K\n",
    "\n",
    "            # Build loaders per view\n",
    "            loaders = {}\n",
    "            for v in range(num_views):\n",
    "                tr_loader   = torch.utils.data.DataLoader(PatchesDataset(X_trP,      y_trP,      K, v), batch_size=cfg.batch_size, shuffle=True)\n",
    "                cal_loader  = torch.utils.data.DataLoader(PatchesDataset(X_cal,      y_cal,      K, v), batch_size=cfg.batch_size, shuffle=False)\n",
    "                ftr_loader  = torch.utils.data.DataLoader(PatchesDataset(X_fuse_tr,  y_fuse_tr,  K, v), batch_size=cfg.batch_size, shuffle=False)\n",
    "                fcal_loader = torch.utils.data.DataLoader(PatchesDataset(X_fuse_cal, y_fuse_cal, K, v), batch_size=cfg.batch_size, shuffle=False)\n",
    "                te_loader   = torch.utils.data.DataLoader(PatchesDataset(X_te,       y_te,       K, v), batch_size=cfg.batch_size, shuffle=False)\n",
    "                loaders[v] = dict(train=tr_loader, cal=cal_loader, ftr=ftr_loader, fcal=fcal_loader, te=te_loader)\n",
    "\n",
    "            # Train per-view CNNs\n",
    "            models, cal_classwise = [], []\n",
    "            for v in range(num_views):\n",
    "                print(f\"    [View {v+1}/{num_views}] training...\")\n",
    "                m = PredictorCNN(num_classes=cfg.num_classes)\n",
    "                m = train_model(m, loaders[v][\"train\"], num_epochs=cfg.epochs_per_view, lr=cfg.lr)\n",
    "                models.append(m)\n",
    "                sc, lab = compute_nonconformity_scores(m, loaders[v][\"cal\"])\n",
    "                cal_classwise.append(classwise_scores(sc, lab, cfg.num_classes))\n",
    "\n",
    "            # Per-view p/probs for fusion train/cal/test\n",
    "            pv_tr, pr_tr = [], []\n",
    "            pv_cal, pr_cal = [], []\n",
    "            pv_te,  pr_te  = [], []\n",
    "            for v in range(num_views):\n",
    "                p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"ftr\"], cfg.num_classes)\n",
    "                pv_tr.append(p); pr_tr.append(pr)\n",
    "                p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"fcal\"], cfg.num_classes)\n",
    "                pv_cal.append(p); pr_cal.append(pr)\n",
    "                p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"te\"],  cfg.num_classes)\n",
    "                pv_te.append(p);  pr_te.append(pr)\n",
    "\n",
    "            # Build fusion LR on [p, prob] from fusion-train\n",
    "            X_ftr = build_fusion_features(pv_tr, pr_tr)\n",
    "            fusion_lr = LogisticRegression(max_iter=cfg.max_iter_lr, multi_class=\"multinomial\", solver=\"lbfgs\", random_state=seed)\n",
    "            fusion_lr.fit(X_ftr, y_fuse_tr)\n",
    "\n",
    "            # Fused calibration probs + classwise cal scores\n",
    "            X_fcal = build_fusion_features(pv_cal, pr_cal)\n",
    "            fused_probs_cal = fusion_lr.predict_proba(X_fcal)\n",
    "            fused_cal_scores = fused_class_cal_scores(y_fuse_cal, fused_probs_cal, cfg.num_classes)\n",
    "\n",
    "            # Fused test probs\n",
    "            X_ftest = build_fusion_features(pv_te, pr_te)\n",
    "            fused_probs_test = fusion_lr.predict_proba(X_ftest)\n",
    "\n",
    "            # Our conformal-fused p-values\n",
    "            P_cf = fused_p_values_from_cal(fused_probs_test, fused_cal_scores)\n",
    "\n",
    "            # Baselines (stack per-view p-values)\n",
    "            P_train = np.stack(pv_tr, axis=0)   # (K, n_tr, L)\n",
    "            P_test  = np.stack(pv_te, axis=0)   # (K, n_te, L)\n",
    "\n",
    "            P_min    = min_p_value_fusion(P_test)\n",
    "            P_fisher = fisher_fusion(P_test)\n",
    "            P_adjF   = adjusted_fisher_fusion(P_train, y_fuse_tr, P_test, cfg.num_classes)\n",
    "\n",
    "            # Learned weighted average from p-values-only (K*L features)\n",
    "            pv_tr_concat = np.concatenate(pv_tr, axis=1)  # (n_tr, K*L)\n",
    "            w_learned = learn_view_weights_from_pvals(pv_tr_concat, y_fuse_tr, num_views, cfg.num_classes, cfg.max_iter_lr, seed)\n",
    "            P_wavgL = weighted_average_fusion(P_test, w_learned)\n",
    "\n",
    "            # Metrics\n",
    "            cov_cf,   set_cf   = evaluate_sets(P_cf,     y_te, cfg.alpha)\n",
    "            cov_min,  set_min  = evaluate_sets(P_min,    y_te, cfg.alpha)\n",
    "            cov_fi,   set_fi   = evaluate_sets(P_fisher, y_te, cfg.alpha)\n",
    "            cov_afi,  set_afi  = evaluate_sets(P_adjF,   y_te, cfg.alpha)\n",
    "            cov_wl,   set_wl   = evaluate_sets(P_wavgL,  y_te, cfg.alpha)\n",
    "\n",
    "            results_cov.append({\n",
    "                \"Sim\": sim, \"K\": K,\n",
    "                \"Conformal Fusion\": cov_cf * 100,\n",
    "                \"Min p-Value\": cov_min * 100,\n",
    "                \"Fisher\": cov_fi * 100,\n",
    "                \"Adjusted Fisher\": cov_afi * 100,\n",
    "                \"Weighted Averaging\": cov_wl * 100,\n",
    "            })\n",
    "            results_size.append({\n",
    "                \"Sim\": sim, \"K\": K,\n",
    "                \"Conformal Fusion\": set_cf,\n",
    "                \"Min p-Value\": set_min,\n",
    "                \"Fisher\": set_fi,\n",
    "                \"Adjusted Fisher\": set_afi,\n",
    "                \"Weighted Averaging\": set_wl,\n",
    "            })\n",
    "\n",
    "            # (Optional) Reference accuracy using simple average of per-view probs\n",
    "            avg_probs = np.mean(np.stack(pr_te, axis=0), axis=0)\n",
    "            acc_ref = accuracy_score(y_te, np.argmax(avg_probs, axis=1)) * 100\n",
    "            results_acc.append({\"Sim\": sim, \"K\": K, \"Reference Acc (avg probs)\": acc_ref})\n",
    "\n",
    "    return pd.DataFrame(results_cov), pd.DataFrame(results_size), pd.DataFrame(results_acc)\n",
    "\n",
    "# =============================================================================\n",
    "# Save/print tables (same style as synthetic)\n",
    "# =============================================================================\n",
    "\n",
    "def save_tables(df_cov: pd.DataFrame, df_size: pd.DataFrame, df_acc: pd.DataFrame):\n",
    "    methods = [\n",
    "        \"Conformal Fusion\",\n",
    "        \"Min p-Value\",\n",
    "        \"Fisher\",\n",
    "        \"Adjusted Fisher\",\n",
    "        \"Weighted Averaging\",\n",
    "    ]\n",
    "    sum_cov = summarize_table(df_cov, methods, \"Coverage (%)\")\n",
    "    sum_set = summarize_table(df_size, methods, \"Average Set Size\")\n",
    "\n",
    "    # CSV + LaTeX (CIFAR version)\n",
    "    sum_cov.to_csv(\"cifar100_summary_coverage.csv\", index=False)\n",
    "    sum_set.to_csv(\"cifar100_summary_setsize.csv\", index=False)\n",
    "    with open(\"cifar100_summary_coverage.tex\", \"w\") as f:\n",
    "        f.write(sum_cov.to_latex(index=False, escape=False))\n",
    "    with open(\"cifar100_summary_setsize.tex\", \"w\") as f:\n",
    "        f.write(sum_set.to_latex(index=False, escape=False))\n",
    "\n",
    "    # Compact side-by-side\n",
    "    cov_comp = sum_cov.drop(columns=[\"Metric\"]).rename(columns={\n",
    "        \"Conformal Fusion\": \"CF Cov\",\n",
    "        \"Min p-Value\": \"MinPV Cov\",\n",
    "        \"Fisher\": \"Fisher Cov\",\n",
    "        \"Adjusted Fisher\": \"AdjF Cov\",\n",
    "        \"Weighted Averaging\": \"WAvgL Cov\",\n",
    "    })\n",
    "    set_comp = sum_set.drop(columns=[\"Metric\"]).rename(columns={\n",
    "        \"Conformal Fusion\": \"CF Set\",\n",
    "        \"Min p-Value\": \"MinPV Set\",\n",
    "        \"Fisher\": \"Fisher Set\",\n",
    "        \"Adjusted Fisher\": \"AdjF Set\",\n",
    "        \"Weighted Averaging\": \"WAvgL Set\",\n",
    "    })\n",
    "    final = cov_comp.merge(set_comp, on=\"K\").sort_values(\"K\")\n",
    "    final.to_csv(\"cifar100_summary_final.csv\", index=False)\n",
    "    with open(\"cifar100_summary_final.tex\", \"w\") as f:\n",
    "        f.write(final.to_latex(index=False, escape=False))\n",
    "\n",
    "    # Acc means (if you want)\n",
    "    acc_means = df_acc.groupby(\"K\").mean(numeric_only=True).reset_index()\n",
    "    acc_means.to_csv(\"cifar100_accuracy_summary.csv\", index=False)\n",
    "    with open(\"cifar100_accuracy_summary.tex\", \"w\") as f:\n",
    "        f.write(acc_means.to_latex(index=False, float_format=\"%.2f\"))\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\"  cifar100_summary_coverage.csv / .tex\")\n",
    "    print(\"  cifar100_summary_setsize.csv  / .tex\")\n",
    "    print(\"  cifar100_summary_final.csv    / .tex\")\n",
    "    print(\"  cifar100_accuracy_summary.csv / .tex\")\n",
    "\n",
    "# =============================================================================\n",
    "# Entry\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    cfg = CIFARConfig()\n",
    "    df_cov, df_size, df_acc = run_experiments(cfg)\n",
    "    print(\"\\n=== Coverage (raw rows) ===\")\n",
    "    print(df_cov.head())\n",
    "    print(\"\\n=== Set Size (raw rows) ===\")\n",
    "    print(df_size.head())\n",
    "    save_tables(df_cov, df_size, df_acc)\n",
    "\n",
    "    # Plotting\n",
    "    plt.style.use(['science','ieee', 'no-latex'])\n",
    "\n",
    "    # Avoid Type 3 fonts\n",
    "    matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "    matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "    sns.set_context(\"paper\", font_scale=1.2)\n",
    "\n",
    "    # Consistent method order/colors\n",
    "    method_order = ['Conformal Fusion', 'Min p-Value', \"Fisher\", 'Adjusted Fisher', 'Weighted Averaging']\n",
    "    palette = {'Conformal Fusion': 'blue', 'Min p-Value': 'red', \"Fisher\": 'green', 'Adjusted Fisher': 'cyan', 'Weighted Averaging': 'orange'}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5.9, 4.4))\n",
    "\n",
    "    # Melt df_cov for plotting\n",
    "    df_cov_melt = pd.melt(df_cov, id_vars=['K', 'Sim'], value_vars=method_order, var_name='Method', value_name='Coverage')\n",
    "\n",
    "    # Boxplot: Coverage vs K\n",
    "    sns.boxplot(x='K', y='Coverage', hue='Method',\n",
    "                data=df_cov_melt, hue_order=method_order, palette=palette, ax=ax)\n",
    "    ax.set_title('Coverage vs. Number of Views on CIFAR-100', fontsize=15)\n",
    "    ax.set_xlabel('Number of Views (K)', fontsize=13)\n",
    "    ax.set_ylabel('Coverage (%)', fontsize=13)\n",
    "    ax.legend(loc='lower left')\n",
    "\n",
    "    sns.despine(right=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cifar100_coverage_boxplots.png', dpi=600)\n",
    "    plt.savefig('cifar100_coverage_boxplots.pdf')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0bfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "=== Simulation 1/2 ===\n",
      "\n",
      "  -> K = 2\n",
      "    [View 1/2] training...\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/2] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [Fusion MLP] epoch 5/100  loss=2.1790\n",
      "    [Fusion MLP] epoch 10/100  loss=1.8050\n",
      "    [Fusion MLP] epoch 15/100  loss=1.5435\n",
      "    [Fusion MLP] epoch 20/100  loss=1.3156\n",
      "    [Fusion MLP] epoch 25/100  loss=1.1286\n",
      "    [Fusion MLP] epoch 30/100  loss=0.9805\n",
      "    [Fusion MLP] epoch 35/100  loss=0.8706\n",
      "    [Fusion MLP] epoch 40/100  loss=0.7870\n",
      "    [Fusion MLP] epoch 45/100  loss=0.7344\n",
      "    [Fusion MLP] epoch 50/100  loss=0.6967\n",
      "    [Fusion MLP] epoch 55/100  loss=0.6672\n",
      "    [Fusion MLP] epoch 60/100  loss=0.6485\n",
      "    [Fusion MLP] epoch 65/100  loss=0.6353\n",
      "    [Fusion MLP] epoch 70/100  loss=0.6265\n",
      "    [Fusion MLP] epoch 75/100  loss=0.6185\n",
      "    [Fusion MLP] epoch 80/100  loss=0.6127\n",
      "    [Fusion MLP] epoch 85/100  loss=0.6105\n",
      "    [Fusion MLP] epoch 90/100  loss=0.6081\n",
      "    [Fusion MLP] epoch 95/100  loss=0.6043\n",
      "    [Fusion MLP] epoch 100/100  loss=0.6078\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/3] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [Fusion MLP] epoch 5/100  loss=2.2408\n",
      "    [Fusion MLP] epoch 10/100  loss=1.7450\n",
      "    [Fusion MLP] epoch 15/100  loss=1.3974\n",
      "    [Fusion MLP] epoch 20/100  loss=1.1388\n",
      "    [Fusion MLP] epoch 25/100  loss=0.9368\n",
      "    [Fusion MLP] epoch 30/100  loss=0.7969\n",
      "    [Fusion MLP] epoch 35/100  loss=0.7059\n",
      "    [Fusion MLP] epoch 40/100  loss=0.6510\n",
      "    [Fusion MLP] epoch 45/100  loss=0.6171\n",
      "    [Fusion MLP] epoch 50/100  loss=0.5962\n",
      "    [Fusion MLP] epoch 55/100  loss=0.5850\n",
      "    [Fusion MLP] epoch 60/100  loss=0.5720\n",
      "    [Fusion MLP] epoch 65/100  loss=0.5649\n",
      "    [Fusion MLP] epoch 70/100  loss=0.5609\n",
      "    [Fusion MLP] epoch 75/100  loss=0.5576\n",
      "    [Fusion MLP] epoch 80/100  loss=0.5544\n",
      "    [Fusion MLP] epoch 85/100  loss=0.5538\n",
      "    [Fusion MLP] epoch 90/100  loss=0.5515\n",
      "    [Fusion MLP] epoch 95/100  loss=0.5505\n",
      "    [Fusion MLP] epoch 100/100  loss=0.5512\n",
      "\n",
      "  -> K = 4\n",
      "    [View 1/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 4/4] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [Fusion MLP] epoch 5/100  loss=2.0666\n",
      "    [Fusion MLP] epoch 10/100  loss=1.5338\n",
      "    [Fusion MLP] epoch 15/100  loss=1.1778\n",
      "    [Fusion MLP] epoch 20/100  loss=0.9230\n",
      "    [Fusion MLP] epoch 25/100  loss=0.7567\n",
      "    [Fusion MLP] epoch 30/100  loss=0.6605\n",
      "    [Fusion MLP] epoch 35/100  loss=0.6078\n",
      "    [Fusion MLP] epoch 40/100  loss=0.5793\n",
      "    [Fusion MLP] epoch 45/100  loss=0.5622\n",
      "    [Fusion MLP] epoch 50/100  loss=0.5486\n",
      "    [Fusion MLP] epoch 55/100  loss=0.5417\n",
      "    [Fusion MLP] epoch 60/100  loss=0.5357\n",
      "    [Fusion MLP] epoch 65/100  loss=0.5316\n",
      "    [Fusion MLP] epoch 70/100  loss=0.5293\n",
      "    [Fusion MLP] epoch 75/100  loss=0.5272\n",
      "    [Fusion MLP] epoch 80/100  loss=0.5246\n",
      "    [Fusion MLP] epoch 85/100  loss=0.5237\n",
      "    [Fusion MLP] epoch 90/100  loss=0.5233\n",
      "    [Fusion MLP] epoch 95/100  loss=0.5217\n",
      "    [Fusion MLP] epoch 100/100  loss=0.5226\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 2/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n",
      "  epoch 100/100\n",
      "    [View 3/6] training...\n",
      "  epoch 25/100\n",
      "  epoch 50/100\n",
      "  epoch 75/100\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18, efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# Config\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class CIFARConfig:\n",
    "    # core\n",
    "    alpha: float = 0.1\n",
    "    Ks: Tuple[int, ...] = (2, 3, 4, 6)\n",
    "    num_classes: int = 100\n",
    "    num_simulations: int = 2\n",
    "\n",
    "    # training\n",
    "    epochs_per_view: int = 100      # adjust for runtime; your old code used 200\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 8192\n",
    "    max_iter_lr: int = 300         # for sklearn LR (baselines/weight-learning)\n",
    "    train_seed_base: int = 41\n",
    "\n",
    "    # data split fractions (similar structure to synthetic)\n",
    "    train_frac: float = 0.5         # predictor training from full train set\n",
    "    cal_frac_of_temp: float = 0.3   # portion of temp used as calibration for per-view\n",
    "    fuse_train_frac_of_rest: float = 0.7  # remaining split into fusion_train/cal\n",
    "\n",
    "# =============================================================================\n",
    "# Torch / Data\n",
    "# =============================================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load once\n",
    "train_dataset = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "X_train_full = train_dataset.data.astype(np.float32) / 255.0   # (50000, 32, 32, 3)\n",
    "Y_train_full = np.array(train_dataset.targets)\n",
    "X_test_full  = test_dataset.data.astype(np.float32) / 255.0    # (10000, 32, 32, 3)\n",
    "Y_test_full  = np.array(test_dataset.targets)\n",
    "\n",
    "# =============================================================================\n",
    "# Multi-view (patch) utilities\n",
    "# =============================================================================\n",
    "\n",
    "def split_image_into_k_patches(image: torch.Tensor, k: int) -> List[torch.Tensor]:\n",
    "    # image: (C, H, W) = (3,32,32)\n",
    "    C, H, W = image.shape\n",
    "    if k == 4:\n",
    "        # 2x2 grid\n",
    "        patches = []\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                patch = image[:, i*16:(i+1)*16, j*16:(j+1)*16]\n",
    "                patches.append(patch)\n",
    "        return patches\n",
    "    else:\n",
    "        # vertical stripes\n",
    "        base_width = W // k\n",
    "        remainder = W % k\n",
    "        patches, start = [], 0\n",
    "        for idx in range(k):\n",
    "            width = base_width + (1 if idx < remainder else 0)\n",
    "            patch = image[:, :, start:start+width]\n",
    "            patches.append(patch)\n",
    "            start += width\n",
    "        return patches\n",
    "\n",
    "class PatchesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images: np.ndarray, labels: np.ndarray, k: int, view: int):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.k = k\n",
    "        self.view = view\n",
    "        self.resize = Resize((32, 32))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img = self.images[idx].transpose((2, 0, 1))   # (3,32,32)\n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "        patches = split_image_into_k_patches(img, self.k)\n",
    "        patch = patches[self.view]\n",
    "        patch = self.resize(patch)\n",
    "        label = int(self.labels[idx])\n",
    "        return patch, label\n",
    "\n",
    "# =============================================================================\n",
    "# Per-view CNN (EfficientNet-B0 head)\n",
    "# =============================================================================\n",
    "\n",
    "# class PredictorCNN(nn.Module):\n",
    "#     def __init__(self, num_classes=100):\n",
    "#         super().__init__()\n",
    "#         self.model = resnet18(pretrained=False, num_classes=num_classes)\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "class PredictorCNN(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super().__init__()\n",
    "        # Load pretrained EfficientNet-B0 on ImageNet\n",
    "        self.model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        # Replace final classification layer\n",
    "        in_features = self.model.classifier[1].in_features\n",
    "        self.model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader, num_epochs=100, lr=1e-3):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt  = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    for ep in range(num_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), torch.tensor(yb, dtype=torch.long, device=device)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if (ep+1) % 25 == 0:\n",
    "            print(f\"  epoch {ep+1}/{num_epochs}\")\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# Conformal utilities (torch models)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_nonconformity_scores(model: nn.Module, loader) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    scores, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs  = F.softmax(logits, dim=1)\n",
    "            idx    = torch.arange(probs.size(0), device=probs.device)\n",
    "            true_p = probs[idx, torch.tensor(yb, dtype=torch.long, device=probs.device)]\n",
    "            s = (1 - true_p).detach().cpu().numpy()\n",
    "            scores.extend(s)\n",
    "            labels.extend(yb.numpy())\n",
    "    return np.asarray(scores, float), np.asarray(labels, int)\n",
    "\n",
    "def classwise_scores(scores: np.ndarray, labels: np.ndarray, L: int) -> Dict[int, np.ndarray]:\n",
    "    out = {c: [] for c in range(L)}\n",
    "    for s, y in zip(scores, labels):\n",
    "        out[int(y)].append(float(s))\n",
    "    return {c: np.asarray(v, float) for c, v in out.items()}\n",
    "\n",
    "def per_view_pvalues_and_probs(\n",
    "    model: nn.Module, class_scores: Dict[int, np.ndarray], loader, L: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return p-values (n,L) and probs (n,L) for a single view.\"\"\"\n",
    "    model.eval()\n",
    "    probs_all = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs  = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "            probs_all.append(probs)\n",
    "    probs_all = np.vstack(probs_all)  # (n, L)\n",
    "\n",
    "    n = probs_all.shape[0]\n",
    "    pvals = np.zeros((n, L))\n",
    "    for y in range(L):\n",
    "        cal = class_scores.get(y, np.array([]))\n",
    "        if cal.size == 0:\n",
    "            pvals[:, y] = 1.0\n",
    "        else:\n",
    "            s_test = 1 - probs_all[:, y]\n",
    "            counts = np.sum(cal[:, None] >= s_test[None, :], axis=0)\n",
    "            pvals[:, y] = (1 + counts) / (len(cal) + 1)\n",
    "    return pvals, probs_all\n",
    "\n",
    "# =============================================================================\n",
    "# Fusion utilities (baseline + new richer fusion)\n",
    "# =============================================================================\n",
    "\n",
    "def build_fusion_features(pvals_list: List[np.ndarray], probs_list: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Horizontally stack [pvals, probs] for each view -> (n, K*2L)\"\"\"\n",
    "    blocks = [np.hstack([pvals_list[k], probs_list[k]]) for k in range(len(pvals_list))]\n",
    "    return np.hstack(blocks)\n",
    "\n",
    "def build_fusion_features_extended(pvals_list: List[np.ndarray], probs_list: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each view, stack [pvals, probs, log(probs+eps)] horizontally.\n",
    "    Output shape: (n, K * 3L)\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "    blocks = []\n",
    "    for k in range(len(pvals_list)):\n",
    "        pvals = pvals_list[k]\n",
    "        probs = probs_list[k]\n",
    "        logp  = np.log(np.clip(probs, eps, 1.0))\n",
    "        blocks.append(np.hstack([pvals, probs, logp]))\n",
    "    return np.hstack(blocks)\n",
    "\n",
    "class FusionMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A 3-layer MLP with GELU, dropout, batchnorm, and a light residual connection.\n",
    "    Input: concatenated per-view features (p, prob, logprob). Output: logits over L classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_classes: int, hidden1: int = 2048, hidden2: int = 1024, p_drop: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, num_classes)\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.act  = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        h = self.fc2(x)\n",
    "        h = self.bn2(h)\n",
    "        h = self.act(h)\n",
    "        h = self.drop(h)\n",
    "\n",
    "        # Residual within hidden space\n",
    "        x_proj = x\n",
    "        if x_proj.shape[1] != h.shape[1]:\n",
    "            x_proj = h\n",
    "        h = h + x_proj\n",
    "\n",
    "        out = self.fc3(h)\n",
    "        return out\n",
    "\n",
    "def train_fusion_mlp(X: np.ndarray, y: np.ndarray, L: int, seed: int,\n",
    "                     lr: float = 3e-4, batch_size: int = 4096, epochs: int = 30,\n",
    "                     weight_decay: float = 1e-4, label_smoothing: float = 0.05) -> FusionMLP:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X_t = torch.tensor(X, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    ds = TensorDataset(X_t, y_t)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    model = FusionMLP(input_dim=X.shape[1], num_classes=L).to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
    "    crit = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        total = 0.0\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            opt.step()\n",
    "            total += float(loss.item()) * xb.size(0)\n",
    "        sched.step()\n",
    "        if (ep + 1) % 5 == 0:\n",
    "            print(f\"    [Fusion MLP] epoch {ep+1}/{epochs}  loss={total/len(ds):.4f}\")\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def fusion_mlp_predict_proba(model: FusionMLP, X: np.ndarray) -> np.ndarray:\n",
    "    model.eval()\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    logits = model(X_t)\n",
    "    probs  = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "# ---- Baseline fusion operations (unchanged) ----\n",
    "\n",
    "def min_p_value_fusion(P_all: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"K * min_k p_k^y. P_all: (K,n,L) -> (n,L)\"\"\"\n",
    "    K = P_all.shape[0]\n",
    "    return K * np.min(P_all, axis=0)\n",
    "\n",
    "def fisher_fusion(P_all: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Standard Fisher.\"\"\"\n",
    "    eps = 1e-12\n",
    "    p = np.clip(P_all, eps, 1.0)\n",
    "    T = -2 * np.sum(np.log(p), axis=0)\n",
    "    df = 2 * P_all.shape[0]\n",
    "    return 1 - chi2.cdf(T, df=df)\n",
    "\n",
    "def adjusted_fisher_fusion(P_train: np.ndarray, y_train: np.ndarray, P_test: np.ndarray, L: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Moment-matched Fisher per class: fit variance of T_y = sum_k -2log p_k^y on per-class train,\n",
    "    then use scaled-chi-square CDF.\n",
    "    \"\"\"\n",
    "    K, _, _ = P_train.shape\n",
    "    n_test = P_test.shape[1]\n",
    "    eps = 1e-12\n",
    "    out = np.zeros((n_test, L))\n",
    "    for y in range(L):\n",
    "        idx = np.where(y_train == y)[0]\n",
    "        if idx.size < 5:\n",
    "            out[:, y] = fisher_fusion(P_test)[:, y]\n",
    "            continue\n",
    "        P_cls = np.clip(P_train[:, idx, y], eps, 1.0)  # (K, n_y)\n",
    "        W = -2 * np.log(P_cls)                          # (K, n_y)\n",
    "        Wc = W - W.mean(axis=1, keepdims=True)\n",
    "        Sigma = (Wc @ Wc.T) / max(W.shape[1] - 1, 1)    # (K, K)\n",
    "        var_T = np.sum(Sigma)\n",
    "        if not np.isfinite(var_T) or var_T <= 0:\n",
    "            var_T = 4 * K\n",
    "        f_y = (8.0 * K * K) / var_T\n",
    "        c_y = var_T / (4 * K)\n",
    "\n",
    "        P_t = np.clip(P_test[:, :, y], eps, 1.0)\n",
    "        T_t = -2 * np.sum(np.log(P_t), axis=0)\n",
    "        out[:, y] = 1 - chi2.cdf(T_t / c_y, df=f_y)\n",
    "    return out\n",
    "\n",
    "def weighted_average_fusion(P_all: np.ndarray, weights: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"sum_k w_k p_k^y; P_all: (K,n,L), weights: (K,)\"\"\"\n",
    "    return np.tensordot(weights, P_all, axes=(0, 0))\n",
    "\n",
    "def learn_view_weights_from_pvals(pv_train_concat: np.ndarray, y_train: np.ndarray, K: int, L: int, max_iter: int, seed: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Train multinomial LR on p-only features to predict y.\n",
    "    Convert coef_ (L, K*L) -> view weights by Frobenius norm per (LÃ—L) block.\n",
    "    \"\"\"\n",
    "    lr = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=max_iter, random_state=seed)\n",
    "    lr.fit(pv_train_concat, y_train)\n",
    "    B = lr.coef_  # (L, K*L)\n",
    "    imps = []\n",
    "    for k in range(K):\n",
    "        block = B[:, k*L:(k+1)*L]\n",
    "        imps.append(np.linalg.norm(block, ord=\"fro\"))\n",
    "    w = np.array(imps, float)\n",
    "    w = np.maximum(w, 1e-12)\n",
    "    return w / w.sum()\n",
    "\n",
    "# Conformalization of fused model\n",
    "def fused_class_cal_scores(y_cal: np.ndarray, fused_probs_cal: np.ndarray, L: int) -> Dict[int, np.ndarray]:\n",
    "    s = 1 - fused_probs_cal[np.arange(len(y_cal)), y_cal]\n",
    "    out = {c: [] for c in range(L)}\n",
    "    for sc, yy in zip(s, y_cal):\n",
    "        out[int(yy)].append(float(sc))\n",
    "    return {c: np.asarray(v, float) for c, v in out.items()}\n",
    "\n",
    "def fused_p_values_from_cal(fused_probs: np.ndarray, cal_class_scores: Dict[int, np.ndarray]) -> np.ndarray:\n",
    "    n, L = fused_probs.shape\n",
    "    out = np.zeros((n, L))\n",
    "    for y in range(L):\n",
    "        cal = cal_class_scores.get(y, np.array([]))\n",
    "        if cal.size == 0:\n",
    "            out[:, y] = 1.0\n",
    "        else:\n",
    "            s_test = 1 - fused_probs[:, y]\n",
    "            counts = np.sum(cal[:, None] >= s_test[None, :], axis=0)\n",
    "            out[:, y] = (1 + counts) / (len(cal) + 1)\n",
    "    return out\n",
    "\n",
    "def evaluate_sets(P: np.ndarray, y_true: np.ndarray, alpha: float) -> Tuple[float, float]:\n",
    "    C = (P > alpha)\n",
    "    cov = float(np.mean(C[np.arange(len(y_true)), y_true]))\n",
    "    size = float(np.mean(C.sum(axis=1)))\n",
    "    return cov, size\n",
    "\n",
    "def summarize_table(df: pd.DataFrame, methods: List[str], metric_name: str) -> pd.DataFrame:\n",
    "    g = df.groupby(\"K\").agg({m: [\"mean\", \"std\"] for m in methods})\n",
    "    g.columns = [f\"{a}_{b}\" for a, b in g.columns]\n",
    "    g = g.reset_index()\n",
    "    for m in methods:\n",
    "        g[m] = g.apply(lambda r: f\"{r[f'{m}_mean']:.2f} ({r[f'{m}_std']:.2f})\", axis=1)\n",
    "    g.insert(1, \"Metric\", metric_name)\n",
    "    return g[[\"K\", \"Metric\"] + methods]\n",
    "\n",
    "# =============================================================================\n",
    "# Main experiment\n",
    "# =============================================================================\n",
    "\n",
    "def run_experiments(cfg: CIFARConfig):\n",
    "    results_cov, results_size, results_acc = [], [], []\n",
    "\n",
    "    for sim in range(cfg.num_simulations):\n",
    "        print(f\"\\n=== Simulation {sim+1}/{cfg.num_simulations} ===\")\n",
    "        seed = cfg.train_seed_base + sim\n",
    "        rng = np.random.RandomState(seed)\n",
    "\n",
    "        # Splits (train for view predictors, then cal/fusion splits)\n",
    "        X_trP, X_tmp, y_trP, y_tmp = train_test_split(\n",
    "            X_train_full, Y_train_full, test_size=1 - cfg.train_frac, stratify=Y_train_full, random_state=seed\n",
    "        )\n",
    "        # We will split X_tmp into cal and fusion pools\n",
    "        X_cal, X_rest, y_cal, y_rest = train_test_split(\n",
    "            X_tmp, y_tmp, test_size=1 - cfg.cal_frac_of_temp, stratify=y_tmp, random_state=seed\n",
    "        )\n",
    "        X_fuse_tr, X_fuse_cal, y_fuse_tr, y_fuse_cal = train_test_split(\n",
    "            X_rest, y_rest, test_size=1 - cfg.fuse_train_frac_of_rest, stratify=y_rest, random_state=seed\n",
    "        )\n",
    "        X_te, y_te = X_test_full, Y_test_full\n",
    "\n",
    "        for K in cfg.Ks:\n",
    "            print(f\"\\n  -> K = {K}\")\n",
    "            num_views = 4 if K == 4 else K\n",
    "\n",
    "            # Build loaders per view\n",
    "            loaders = {}\n",
    "            for v in range(num_views):\n",
    "                tr_loader   = torch.utils.data.DataLoader(PatchesDataset(X_trP,      y_trP,      K, v), batch_size=cfg.batch_size, shuffle=True)\n",
    "                cal_loader  = torch.utils.data.DataLoader(PatchesDataset(X_cal,      y_cal,      K, v), batch_size=cfg.batch_size, shuffle=False)\n",
    "                ftr_loader  = torch.utils.data.DataLoader(PatchesDataset(X_fuse_tr,  y_fuse_tr,  K, v), batch_size=cfg.batch_size, shuffle=False)\n",
    "                fcal_loader = torch.utils.data.DataLoader(PatchesDataset(X_fuse_cal, y_fuse_cal, K, v), batch_size=cfg.batch_size, shuffle=False)\n",
    "                te_loader   = torch.utils.data.DataLoader(PatchesDataset(X_te,       y_te,       K, v), batch_size=cfg.batch_size, shuffle=False)\n",
    "                loaders[v] = dict(train=tr_loader, cal=cal_loader, ftr=ftr_loader, fcal=fcal_loader, te=te_loader)\n",
    "\n",
    "            # Train per-view CNNs\n",
    "            models, cal_classwise = [], []\n",
    "            for v in range(num_views):\n",
    "                print(f\"    [View {v+1}/{num_views}] training...\")\n",
    "                m = PredictorCNN(num_classes=cfg.num_classes)\n",
    "                m = train_model(m, loaders[v][\"train\"], num_epochs=cfg.epochs_per_view, lr=cfg.lr)\n",
    "                models.append(m)\n",
    "                sc, lab = compute_nonconformity_scores(m, loaders[v][\"cal\"])\n",
    "                cal_classwise.append(classwise_scores(sc, lab, cfg.num_classes))\n",
    "\n",
    "            # Per-view p/probs for fusion train/cal/test\n",
    "            pv_tr, pr_tr = [], []\n",
    "            pv_cal, pr_cal = [], []\n",
    "            pv_te,  pr_te  = [], []\n",
    "            for v in range(num_views):\n",
    "                p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"ftr\"], cfg.num_classes)\n",
    "                pv_tr.append(p); pr_tr.append(pr)\n",
    "                p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"fcal\"], cfg.num_classes)\n",
    "                pv_cal.append(p); pr_cal.append(pr)\n",
    "                p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"te\"],  cfg.num_classes)\n",
    "                pv_te.append(p);  pr_te.append(pr)\n",
    "\n",
    "            # ------------------ NEW: richer fusion model (MLP) ------------------\n",
    "            # Build richer fusion features: [p, prob, log(prob)]\n",
    "            X_ftr   = build_fusion_features_extended(pv_tr, pr_tr)\n",
    "            X_fcal  = build_fusion_features_extended(pv_cal, pr_cal)\n",
    "            X_ftest = build_fusion_features_extended(pv_te,  pr_te)\n",
    "\n",
    "            # Train a deeper Fusion-MLP on fusion-train\n",
    "            fusion_mlp = train_fusion_mlp(\n",
    "                X_ftr, y_fuse_tr, L=cfg.num_classes, seed=seed,\n",
    "                lr=3e-4, batch_size=min(cfg.batch_size, 4096), epochs=100,\n",
    "                weight_decay=1e-4, label_smoothing=0.05\n",
    "            )\n",
    "\n",
    "            # Fused calibration/test probabilities from the MLP\n",
    "            fused_probs_cal  = fusion_mlp_predict_proba(fusion_mlp, X_fcal)\n",
    "            fused_cal_scores = fused_class_cal_scores(y_fuse_cal, fused_probs_cal, cfg.num_classes)\n",
    "            fused_probs_test = fusion_mlp_predict_proba(fusion_mlp, X_ftest)\n",
    "            # -------------------------------------------------------------------\n",
    "\n",
    "            # Our conformal-fused p-values\n",
    "            P_cf = fused_p_values_from_cal(fused_probs_test, fused_cal_scores)\n",
    "\n",
    "            # Baselines (stack per-view p-values) -- unchanged\n",
    "            P_train = np.stack(pv_tr, axis=0)   # (K, n_tr, L)\n",
    "            P_test  = np.stack(pv_te, axis=0)   # (K, n_te, L)\n",
    "\n",
    "            P_min    = min_p_value_fusion(P_test)\n",
    "            P_fisher = fisher_fusion(P_test)\n",
    "            P_adjF   = adjusted_fisher_fusion(P_train, y_fuse_tr, P_test, cfg.num_classes)\n",
    "\n",
    "            # Learned weighted average from p-values-only (K*L features)\n",
    "            pv_tr_concat = np.concatenate(pv_tr, axis=1)  # (n_tr, K*L)\n",
    "            w_learned = learn_view_weights_from_pvals(pv_tr_concat, y_fuse_tr, num_views, cfg.num_classes, cfg.max_iter_lr, seed)\n",
    "            P_wavgL = weighted_average_fusion(P_test, w_learned)\n",
    "\n",
    "            # Metrics\n",
    "            cov_cf,   set_cf   = evaluate_sets(P_cf,     y_te, cfg.alpha)\n",
    "            cov_min,  set_min  = evaluate_sets(P_min,    y_te, cfg.alpha)\n",
    "            cov_fi,   set_fi   = evaluate_sets(P_fisher, y_te, cfg.alpha)\n",
    "            cov_afi,  set_afi  = evaluate_sets(P_adjF,   y_te, cfg.alpha)\n",
    "            cov_wl,   set_wl   = evaluate_sets(P_wavgL,  y_te, cfg.alpha)\n",
    "\n",
    "            results_cov.append({\n",
    "                \"Sim\": sim, \"K\": K,\n",
    "                \"Conformal Fusion\": cov_cf * 100,\n",
    "                \"Min p-Value\": cov_min * 100,\n",
    "                \"Fisher\": cov_fi * 100,\n",
    "                \"Adjusted Fisher\": cov_afi * 100,\n",
    "                \"Weighted Averaging\": cov_wl * 100,\n",
    "            })\n",
    "            results_size.append({\n",
    "                \"Sim\": sim, \"K\": K,\n",
    "                \"Conformal Fusion\": set_cf,\n",
    "                \"Min p-Value\": set_min,\n",
    "                \"Fisher\": set_fi,\n",
    "                \"Adjusted Fisher\": set_afi,\n",
    "                \"Weighted Averaging\": set_wl,\n",
    "            })\n",
    "\n",
    "            # (Optional) Reference accuracy using simple average of per-view probs\n",
    "            avg_probs = np.mean(np.stack(pr_te, axis=0), axis=0)\n",
    "            acc_ref = accuracy_score(y_te, np.argmax(avg_probs, axis=1)) * 100\n",
    "            results_acc.append({\"Sim\": sim, \"K\": K, \"Reference Acc (avg probs)\": acc_ref})\n",
    "\n",
    "    return pd.DataFrame(results_cov), pd.DataFrame(results_size), pd.DataFrame(results_acc)\n",
    "\n",
    "# =============================================================================\n",
    "# Save/print tables (same style as synthetic)\n",
    "# =============================================================================\n",
    "\n",
    "def save_tables(df_cov: pd.DataFrame, df_size: pd.DataFrame, df_acc: pd.DataFrame):\n",
    "    methods = [\n",
    "        \"Conformal Fusion\",\n",
    "        \"Min p-Value\",\n",
    "        \"Fisher\",\n",
    "        \"Adjusted Fisher\",\n",
    "        \"Weighted Averaging\",\n",
    "    ]\n",
    "    sum_cov = summarize_table(df_cov, methods, \"Coverage (%)\")\n",
    "    sum_set = summarize_table(df_size, methods, \"Average Set Size\")\n",
    "\n",
    "    # CSV + LaTeX (CIFAR version)\n",
    "    sum_cov.to_csv(\"cifar100_summary_coverage.csv\", index=False)\n",
    "    sum_set.to_csv(\"cifar100_summary_setsize.csv\", index=False)\n",
    "    with open(\"cifar100_summary_coverage.tex\", \"w\") as f:\n",
    "        f.write(sum_cov.to_latex(index=False, escape=False))\n",
    "    with open(\"cifar100_summary_setsize.tex\", \"w\") as f:\n",
    "        f.write(sum_set.to_latex(index=False, escape=False))\n",
    "\n",
    "    # Compact side-by-side\n",
    "    cov_comp = sum_cov.drop(columns=[\"Metric\"]).rename(columns={\n",
    "        \"Conformal Fusion\": \"CF Cov\",\n",
    "        \"Min p-Value\": \"MinPV Cov\",\n",
    "        \"Fisher\": \"Fisher Cov\",\n",
    "        \"Adjusted Fisher\": \"AdjF Cov\",\n",
    "        \"Weighted Averaging\": \"WAvgL Cov\",\n",
    "    })\n",
    "    set_comp = sum_set.drop(columns=[\"Metric\"]).rename(columns={\n",
    "        \"Conformal Fusion\": \"CF Set\",\n",
    "        \"Min p-Value\": \"MinPV Set\",\n",
    "        \"Fisher\": \"Fisher Set\",\n",
    "        \"Adjusted Fisher\": \"AdjF Set\",\n",
    "        \"Weighted Averaging\": \"WAvgL Set\",\n",
    "    })\n",
    "    final = cov_comp.merge(set_comp, on=\"K\").sort_values(\"K\")\n",
    "    final.to_csv(\"cifar100_summary_final.csv\", index=False)\n",
    "    with open(\"cifar100_summary_final.tex\", \"w\") as f:\n",
    "        f.write(final.to_latex(index=False, escape=False))\n",
    "\n",
    "    # Acc means (if you want)\n",
    "    acc_means = df_acc.groupby(\"K\").mean(numeric_only=True).reset_index()\n",
    "    acc_means.to_csv(\"cifar100_accuracy_summary.csv\", index=False)\n",
    "    with open(\"cifar100_accuracy_summary.tex\", \"w\") as f:\n",
    "        f.write(acc_means.to_latex(index=False, float_format=\"%.2f\"))\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\"  cifar100_summary_coverage.csv / .tex\")\n",
    "    print(\"  cifar100_summary_setsize.csv  / .tex\")\n",
    "    print(\"  cifar100_summary_final.csv    / .tex\")\n",
    "    print(\"  cifar100_accuracy_summary.csv / .tex\")\n",
    "\n",
    "# =============================================================================\n",
    "# Entry\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    cfg = CIFARConfig()\n",
    "    df_cov, df_size, df_acc = run_experiments(cfg)\n",
    "    print(\"\\n=== Coverage (raw rows) ===\")\n",
    "    print(df_cov.head())\n",
    "    print(\"\\n=== Set Size (raw rows) ===\")\n",
    "    print(df_size.head())\n",
    "    save_tables(df_cov, df_size, df_acc)\n",
    "\n",
    "    # Plotting\n",
    "    plt.style.use(['science','ieee', 'no-latex'])\n",
    "\n",
    "    # Avoid Type 3 fonts\n",
    "    matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "    matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "    sns.set_context(\"paper\", font_scale=1.2)\n",
    "\n",
    "    # Consistent method order/colors\n",
    "    method_order = ['Conformal Fusion', 'Min p-Value', \"Fisher\", 'Adjusted Fisher', 'Weighted Averaging']\n",
    "    palette = {'Conformal Fusion': 'blue', 'Min p-Value': 'red', \"Fisher\": 'green', 'Adjusted Fisher': 'cyan', 'Weighted Averaging': 'orange'}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5.9, 4.4))\n",
    "\n",
    "    # Melt df_cov for plotting\n",
    "    df_cov_melt = pd.melt(df_cov, id_vars=['K', 'Sim'], value_vars=method_order, var_name='Method', value_name='Coverage')\n",
    "\n",
    "    # Boxplot: Coverage vs K\n",
    "    sns.boxplot(x='K', y='Coverage', hue='Method',\n",
    "                data=df_cov_melt, hue_order=method_order, palette=palette, ax=ax)\n",
    "    ax.set_title('Coverage vs. Number of Views on CIFAR-100', fontsize=15)\n",
    "    ax.set_xlabel('Number of Views (K)', fontsize=13)\n",
    "    ax.set_ylabel('Coverage (%)', fontsize=13)\n",
    "    ax.legend(loc='lower left')\n",
    "\n",
    "    sns.despine(right=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cifar100_coverage_boxplots.png', dpi=600)\n",
    "    plt.savefig('cifar100_coverage_boxplots.pdf')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8408fb96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
