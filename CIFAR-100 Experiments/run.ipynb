{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32cff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== Simulation 1/4 ===\n",
      "\n",
      "  -> K = 2\n",
      "    [View 1/2] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/2] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.911, size=5.42 | MinPV: cov=0.926, size=11.54 | Fisher: cov=0.886, size=4.77 | AdjF: cov=0.892, size=5.06 | WAvgL(p): cov=0.959, size=11.84\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/3] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 3/3] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.904, size=7.72 | MinPV: cov=0.929, size=17.85 | Fisher: cov=0.875, size=6.30 | AdjF: cov=0.896, size=7.30 | WAvgL(p): cov=0.978, size=23.44\n",
      "\n",
      "  -> K = 4\n",
      "    [View 1/4] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/4] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 3/4] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 4/4] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.905, size=7.78 | MinPV: cov=0.957, size=38.93 | Fisher: cov=0.870, size=6.61 | AdjF: cov=0.903, size=8.15 | WAvgL(p): cov=0.992, size=34.21\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 3/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 4/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 5/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 6/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.902, size=10.02 | MinPV: cov=0.940, size=29.46 | Fisher: cov=0.848, size=6.19 | AdjF: cov=0.902, size=8.87 | WAvgL(p): cov=0.995, size=44.16\n",
      "\n",
      "=== Simulation 2/4 ===\n",
      "\n",
      "  -> K = 2\n",
      "    [View 1/2] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/2] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.910, size=5.50 | MinPV: cov=0.924, size=11.91 | Fisher: cov=0.885, size=4.48 | AdjF: cov=0.896, size=4.63 | WAvgL(p): cov=0.957, size=10.56\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/3] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 3/3] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.906, size=7.02 | MinPV: cov=0.925, size=16.96 | Fisher: cov=0.869, size=5.33 | AdjF: cov=0.895, size=6.19 | WAvgL(p): cov=0.977, size=22.08\n",
      "\n",
      "  -> K = 4\n",
      "    [View 1/4] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/4] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 3/4] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "    [View 4/4] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.907, size=8.38 | MinPV: cov=0.954, size=37.86 | Fisher: cov=0.866, size=6.51 | AdjF: cov=0.902, size=8.03 | WAvgL(p): cov=0.989, size=33.55\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 3/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 4/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 5/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 6/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.911, size=10.62 | MinPV: cov=0.935, size=31.18 | Fisher: cov=0.845, size=5.74 | AdjF: cov=0.904, size=8.35 | WAvgL(p): cov=0.994, size=43.68\n",
      "\n",
      "=== Simulation 3/4 ===\n",
      "\n",
      "  -> K = 2\n",
      "    [View 1/2] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/2] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.901, size=5.28 | MinPV: cov=0.927, size=12.10 | Fisher: cov=0.884, size=4.70 | AdjF: cov=0.890, size=4.70 | WAvgL(p): cov=0.960, size=11.47\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/3] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 3/3] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.907, size=6.64 | MinPV: cov=0.925, size=18.17 | Fisher: cov=0.868, size=5.32 | AdjF: cov=0.890, size=6.07 | WAvgL(p): cov=0.981, size=22.11\n",
      "\n",
      "  -> K = 4\n",
      "    [View 1/4] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/4] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 3/4] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 4/4] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.904, size=8.71 | MinPV: cov=0.947, size=36.14 | Fisher: cov=0.858, size=6.29 | AdjF: cov=0.896, size=7.78 | WAvgL(p): cov=0.990, size=32.45\n",
      "\n",
      "  -> K = 6\n",
      "    [View 1/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 3/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 4/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 5/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 6/6] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.900, size=9.05 | MinPV: cov=0.939, size=31.93 | Fisher: cov=0.844, size=5.95 | AdjF: cov=0.900, size=8.52 | WAvgL(p): cov=0.993, size=44.83\n",
      "\n",
      "=== Simulation 4/4 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  -> K = 2\n",
      "    [View 1/2] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/2] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.909, size=5.38 | MinPV: cov=0.922, size=11.72 | Fisher: cov=0.881, size=4.23 | AdjF: cov=0.889, size=4.40 | WAvgL(p): cov=0.955, size=10.14\n",
      "\n",
      "  -> K = 3\n",
      "    [View 1/3] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 2/3] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    [View 3/3] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n",
      "  epoch 75/150\n",
      "  epoch 100/150\n",
      "  epoch 125/150\n",
      "  epoch 150/150\n",
      "    nt_frac=1.00 (n=12249) | CF: cov=0.911, size=8.54 | MinPV: cov=0.927, size=17.31 | Fisher: cov=0.876, size=6.09 | AdjF: cov=0.897, size=7.17 | WAvgL(p): cov=0.979, size=24.56\n",
      "\n",
      "  -> K = 4\n",
      "    [View 1/4] training ResNet-18...\n",
      "  epoch 25/150\n",
      "  epoch 50/150\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Matched version of your \"gcifar100\" script — UPDATED to match the FIRST code's\n",
    "choices for: CNN model, preprocess, normalization, resize->patch, LR config, dataloaders, etc.\n",
    "\n",
    "✅ Change in THIS revision (requested):\n",
    "- WAvgL baseline is now implemented like your CIFAR-10/MVCP script:\n",
    "    * Learn view weights from per-view p-values using multinomial LR on p-only features\n",
    "    * Convert LR coefficients -> per-view importance via Frobenius norm per (L×L) block\n",
    "    * Use weights to form weighted average of per-view p-values: P_wavg(x,y)=sum_k w_k p_k(x,y)\n",
    "    * (No extra stage-3 recalibration for WAvgL, matching the other script’s baseline style)\n",
    "\n",
    "Everything else remains as in your matched version:\n",
    "- Backbone: ResNet-18 (ImageNet-pretrained optional)\n",
    "- Preprocess: resize FULL image first, then split into patches (resize->patch)\n",
    "- Normalization: ImageNet (default) or CIFAR-100\n",
    "- Dataloader knobs: batch_size/num_workers/pin_memory/persistent_workers\n",
    "- Optimizer for per-view CNN: Adam with cfg.lr\n",
    "- LogisticRegression config: multinomial + lbfgs + max_iter=cfg.max_iter_lr + random_state=seed\n",
    "- Still runs: CF / MinPV / Fisher / AdjF / WAvgL\n",
    "- Still outputs raw CSV + aggregate CSV + old-style LaTeX mean(std) table(s)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# IMPORTANT: match ablation script default behavior\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Config (MATCH first code's training + preprocess defaults)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class MatchedCIFARConfig:\n",
    "    # core\n",
    "    alpha: float = 0.1\n",
    "    Ks: Tuple[int, ...] = (2, 3, 4, 6)  # experiments over K\n",
    "    num_classes: int = 100\n",
    "    num_simulations: int = 4\n",
    "\n",
    "    # training (match your first code)\n",
    "    epochs_per_view: int = 150\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 1024\n",
    "    max_iter_lr: int = 2000\n",
    "    train_seed_base: int = 41\n",
    "\n",
    "    # dataloader (match first code pattern)\n",
    "    num_workers: int = 4\n",
    "\n",
    "    # data split fractions (match first code)\n",
    "    train_frac: float = 0.5\n",
    "    cal_frac_of_temp: float = 0.3\n",
    "    fuse_train_frac_of_rest: float = 0.7\n",
    "\n",
    "    # ablation-style nt sweep\n",
    "    nt_fracs: Tuple[float, ...] = (1.0,)\n",
    "\n",
    "    # ResNet / preprocessing (match first code)\n",
    "    pretrained_backbone: bool = True\n",
    "    normalization: str = \"imagenet\"  # \"imagenet\" or \"cifar100\"\n",
    "    full_image_size: int = 128       # resize BEFORE patching (resize->patch)\n",
    "\n",
    "    # outputs\n",
    "    out_dir: str = \"matched_outputs\"\n",
    "    out_csv: str = \"matched_cifar100_results.csv\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Seeds / device\n",
    "# =============================================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def set_all_seeds(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Data (match first code: numpy in [0,1])\n",
    "# =============================================================================\n",
    "\n",
    "# keep CIFAR as raw tensors/numpy; normalize in PatchesDataset (like first code)\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "X_train_full = train_dataset.data.astype(np.float32) / 255.0\n",
    "Y_train_full = np.array(train_dataset.targets, dtype=int)\n",
    "\n",
    "X_test_full  = test_dataset.data.astype(np.float32) / 255.0\n",
    "Y_test_full  = np.array(test_dataset.targets, dtype=int)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Multi-view (patch) utilities (MATCH first code)\n",
    "# =============================================================================\n",
    "\n",
    "def split_image_into_k_patches(image: torch.Tensor, k: int) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    image: (C,H,W)\n",
    "    For k=4, split into a 2x2 grid using the CURRENT H,W.\n",
    "    Otherwise, split along width into k vertical strips with remainder handling.\n",
    "    \"\"\"\n",
    "    _, H, W = image.shape\n",
    "    if k == 4:\n",
    "        h2 = H // 2\n",
    "        w2 = W // 2\n",
    "        patches = []\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                patch = image[:, i * h2 : (i + 1) * h2, j * w2 : (j + 1) * w2]\n",
    "                patches.append(patch)\n",
    "        return patches\n",
    "    else:\n",
    "        base_width = W // k\n",
    "        remainder = W % k\n",
    "        patches, start = [], 0\n",
    "        for idx in range(k):\n",
    "            width = base_width + (1 if idx < remainder else 0)\n",
    "            patch = image[:, :, start : start + width]\n",
    "            patches.append(patch)\n",
    "            start += width\n",
    "        return patches\n",
    "\n",
    "\n",
    "class PatchesDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    MATCH first code:\n",
    "      - input images are numpy float in [0,1] with shape (H,W,3)\n",
    "      - convert to torch (3,H,W)\n",
    "      - Resize FULL image to (full_image_size, full_image_size)\n",
    "      - split into patches\n",
    "      - normalize patch (ImageNet or CIFAR-100)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        images: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "        k: int,\n",
    "        view: int,\n",
    "        normalization: str = \"imagenet\",\n",
    "        full_image_size: int = 128,\n",
    "    ):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.k = k\n",
    "        self.view = view\n",
    "\n",
    "        self.resize = Resize((full_image_size, full_image_size))\n",
    "\n",
    "        if normalization.lower() == \"cifar100\":\n",
    "            mean = (0.5071, 0.4867, 0.4408)\n",
    "            std  = (0.2675, 0.2565, 0.2761)\n",
    "        else:\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std  = (0.229, 0.224, 0.225)\n",
    "\n",
    "        self.normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img = torch.from_numpy(self.images[idx]).permute(2, 0, 1).contiguous()  # (3,32,32)\n",
    "        img = self.resize(img)  # (3, full_image_size, full_image_size)\n",
    "        patch = split_image_into_k_patches(img, self.k)[self.view]\n",
    "        patch = self.normalize(patch)\n",
    "        return patch, int(self.labels[idx])\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ResNet-18 per view (MATCH first code)\n",
    "# =============================================================================\n",
    "\n",
    "class PredictorCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int = 100, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        weights = ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        self.model = resnet18(weights=weights)\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader, num_epochs: int, lr: float):\n",
    "    # MATCH first code: Adam, no weight_decay, no schedule\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    for ep in range(num_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True) if torch.is_tensor(yb) else torch.tensor(\n",
    "                yb, dtype=torch.long, device=device\n",
    "            )\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss = crit(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if (ep + 1) % 25 == 0:\n",
    "            print(f\"  epoch {ep+1}/{num_epochs}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Conformal utilities (MATCH first code)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_nonconformity_scores(model: nn.Module, loader) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    scores, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb_t = yb.to(device, non_blocking=True) if torch.is_tensor(yb) else torch.tensor(\n",
    "                yb, dtype=torch.long, device=device\n",
    "            )\n",
    "\n",
    "            logits = model(xb)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx = torch.arange(probs.size(0), device=probs.device)\n",
    "            true_p = probs[idx, yb_t]\n",
    "            s = (1.0 - true_p).detach().cpu().numpy()\n",
    "            scores.extend(s)\n",
    "            labels.extend(yb.cpu().numpy() if torch.is_tensor(yb) else np.asarray(yb))\n",
    "\n",
    "    return np.asarray(scores, float), np.asarray(labels, int)\n",
    "\n",
    "\n",
    "def classwise_scores(scores: np.ndarray, labels: np.ndarray, L: int) -> Dict[int, np.ndarray]:\n",
    "    out = {c: [] for c in range(L)}\n",
    "    for s, y in zip(scores, labels):\n",
    "        out[int(y)].append(float(s))\n",
    "    return {c: np.asarray(v, float) for c, v in out.items()}\n",
    "\n",
    "\n",
    "def per_view_pvalues_and_probs(\n",
    "    model: nn.Module, class_scores: Dict[int, np.ndarray], loader, L: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      pvals: (n, L)\n",
    "      probs: (n, L)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    probs_all = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            logits = model(xb)\n",
    "            probs = F.softmax(logits, dim=1).detach().cpu().numpy().astype(np.float32)\n",
    "            probs_all.append(probs)\n",
    "    probs_all = np.vstack(probs_all)  # (n, L)\n",
    "\n",
    "    n = probs_all.shape[0]\n",
    "    pvals = np.zeros((n, L), dtype=np.float32)\n",
    "    for y in range(L):\n",
    "        cal = class_scores.get(y, np.array([], dtype=np.float32))\n",
    "        if cal.size == 0:\n",
    "            pvals[:, y] = 1.0\n",
    "        else:\n",
    "            s_test = 1.0 - probs_all[:, y]\n",
    "            counts = np.sum(cal[:, None] >= s_test[None, :], axis=0)\n",
    "            pvals[:, y] = (1.0 + counts) / (len(cal) + 1.0)\n",
    "\n",
    "    return pvals, probs_all\n",
    "\n",
    "\n",
    "def fused_class_cal_scores(y_cal: np.ndarray, fused_probs_cal: np.ndarray, L: int) -> Dict[int, np.ndarray]:\n",
    "    s = 1.0 - fused_probs_cal[np.arange(len(y_cal)), y_cal]\n",
    "    out = {c: [] for c in range(L)}\n",
    "    for sc, yy in zip(s, y_cal):\n",
    "        out[int(yy)].append(float(sc))\n",
    "    return {c: np.asarray(v, float) for c, v in out.items()}\n",
    "\n",
    "\n",
    "def fused_p_values_from_cal(fused_probs: np.ndarray, cal_class_scores: Dict[int, np.ndarray]) -> np.ndarray:\n",
    "    n, L = fused_probs.shape\n",
    "    out = np.zeros((n, L), dtype=np.float32)\n",
    "    for y in range(L):\n",
    "        cal = cal_class_scores.get(y, np.array([], dtype=np.float32))\n",
    "        if cal.size == 0:\n",
    "            out[:, y] = 1.0\n",
    "        else:\n",
    "            s_test = 1.0 - fused_probs[:, y]\n",
    "            counts = np.sum(cal[:, None] >= s_test[None, :], axis=0)\n",
    "            out[:, y] = (1.0 + counts) / (len(cal) + 1.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def evaluate_sets(P: np.ndarray, y_true: np.ndarray, alpha: float) -> Tuple[float, float]:\n",
    "    C = (P > alpha)\n",
    "    cov = float(np.mean(C[np.arange(len(y_true)), y_true]))\n",
    "    size = float(np.mean(C.sum(axis=1)))\n",
    "    return cov, size\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Fusion feature builders / p-value fusion baselines\n",
    "# =============================================================================\n",
    "\n",
    "def build_fusion_features(pvals_list: List[np.ndarray], probs_list: List[np.ndarray]) -> np.ndarray:\n",
    "    # concat [pvals, probs] per view, then concat across views\n",
    "    blocks = [np.hstack([pvals_list[k], probs_list[k]]) for k in range(len(pvals_list))]\n",
    "    return np.hstack(blocks)\n",
    "\n",
    "\n",
    "def fisher_fusion(P_all: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    P_all: (K, n, L) -> (n, L)\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "    p = np.clip(P_all, eps, 1.0)\n",
    "    T = -2 * np.sum(np.log(p), axis=0)\n",
    "    df = 2 * P_all.shape[0]\n",
    "    return 1 - chi2.cdf(T, df=df)\n",
    "\n",
    "\n",
    "def adjusted_fisher_fusion(P_train: np.ndarray, y_train: np.ndarray, P_test: np.ndarray, L: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Moment-matched (class-conditional) adjusted Fisher.\n",
    "    P_train: (K, n_train, L), P_test: (K, n_test, L) -> (n_test, L)\n",
    "    \"\"\"\n",
    "    K, _, _ = P_train.shape\n",
    "    n_test = P_test.shape[1]\n",
    "    eps = 1e-12\n",
    "    out = np.zeros((n_test, L), dtype=np.float32)\n",
    "\n",
    "    for y in range(L):\n",
    "        idx = np.where(y_train == y)[0]\n",
    "        if idx.size < 5:\n",
    "            out[:, y] = fisher_fusion(P_test)[:, y]\n",
    "            continue\n",
    "\n",
    "        P_cls = np.clip(P_train[:, idx, y], eps, 1.0)  # (K, n_y)\n",
    "        W = -2 * np.log(P_cls)\n",
    "        Wc = W - W.mean(axis=1, keepdims=True)\n",
    "        Sigma = (Wc @ Wc.T) / max(W.shape[1] - 1, 1)\n",
    "        var_T = np.sum(Sigma)\n",
    "        if not np.isfinite(var_T) or var_T <= 0:\n",
    "            var_T = 4 * K\n",
    "\n",
    "        f_y = (8.0 * K * K) / var_T\n",
    "        c_y = var_T / (4 * K)\n",
    "\n",
    "        P_t = np.clip(P_test[:, :, y], eps, 1.0)\n",
    "        T_t = -2 * np.sum(np.log(P_t), axis=0)\n",
    "        out[:, y] = 1 - chi2.cdf(T_t / c_y, df=f_y)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Subsampling helper (stratified) for nt_fracs\n",
    "# =============================================================================\n",
    "\n",
    "def stratified_subsample_indices(y: np.ndarray, n_sub: int, seed: int) -> np.ndarray:\n",
    "    n = len(y)\n",
    "    n_sub = int(max(1, min(n_sub, n)))\n",
    "    idx_all = np.arange(n, dtype=int)\n",
    "    if n_sub >= n:\n",
    "        return idx_all\n",
    "    idx_sub, _ = train_test_split(idx_all, train_size=n_sub, stratify=y, random_state=seed)\n",
    "    return np.asarray(idx_sub, dtype=int)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# WAvgL baseline (UPDATED): learn weights from p-values via multinomial LR (CIFAR-10 style)\n",
    "# =============================================================================\n",
    "\n",
    "def weighted_average_fusion(P_all: np.ndarray, weights: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    sum_k w_k p_k^y\n",
    "    P_all: (K, n, L), weights: (K,) -> (n, L)\n",
    "    \"\"\"\n",
    "    return np.tensordot(weights.astype(np.float32), P_all.astype(np.float32), axes=(0, 0))\n",
    "\n",
    "\n",
    "def learn_view_weights_from_pvals(\n",
    "    pv_train_concat: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    K: int,\n",
    "    L: int,\n",
    "    max_iter: int,\n",
    "    seed: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Train multinomial LR on p-only features to predict y.\n",
    "    Convert coef_ (L, K*L) -> view weights by Frobenius norm per (L×L) block.\n",
    "    \"\"\"\n",
    "    lr = LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=max_iter,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    lr.fit(pv_train_concat, y_train)\n",
    "\n",
    "    B = lr.coef_  # (L, K*L)\n",
    "    imps = []\n",
    "    for k in range(K):\n",
    "        block = B[:, k * L : (k + 1) * L]     # (L, L)\n",
    "        imps.append(np.linalg.norm(block, ord=\"fro\"))\n",
    "\n",
    "    w = np.asarray(imps, dtype=np.float64)\n",
    "    w = np.maximum(w, 1e-12)\n",
    "    w = w / w.sum()\n",
    "    return w.astype(np.float32)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Old-style LaTeX table writer (mean(std))\n",
    "# =============================================================================\n",
    "\n",
    "def _fmt_mean_std(mean: float, std: float, scale: float = 1.0) -> str:\n",
    "    if not np.isfinite(std):\n",
    "        std = 0.0\n",
    "    return f\"{mean*scale:.2f} ({std*scale:.2f})\"\n",
    "\n",
    "\n",
    "def save_old_style_latex_table(df_raw: pd.DataFrame, out_path: str, nt_frac: float) -> str:\n",
    "    \"\"\"\n",
    "    columns:\n",
    "      K | CF Cov | MinPV Cov | Fisher Cov | AdjF Cov | WAvgL Cov\n",
    "        | CF Set | MinPV Set | Fisher Set | AdjF Set | WAvgL Set\n",
    "    \"\"\"\n",
    "    d = df_raw[df_raw[\"nt_frac\"] == float(nt_frac)].copy()\n",
    "    if d.empty:\n",
    "        raise ValueError(f\"No rows found for nt_frac={nt_frac}\")\n",
    "\n",
    "    cols_cov = [\"cov_cf\", \"cov_minpv\", \"cov_fisher\", \"cov_adjF\", \"cov_wavgl\"]\n",
    "    cols_set = [\"size_cf\", \"size_minpv\", \"size_fisher\", \"size_adjF\", \"size_wavgl\"]\n",
    "\n",
    "    g_mean = d.groupby(\"K\")[cols_cov + cols_set].mean()\n",
    "    g_std  = d.groupby(\"K\")[cols_cov + cols_set].std()\n",
    "\n",
    "    rows = []\n",
    "    for K in sorted(g_mean.index.tolist()):\n",
    "        r = {\n",
    "            \"K\": int(K),\n",
    "            \"CF Cov\":      _fmt_mean_std(g_mean.loc[K, \"cov_cf\"],      g_std.loc[K, \"cov_cf\"],      scale=100.0),\n",
    "            \"MinPV Cov\":   _fmt_mean_std(g_mean.loc[K, \"cov_minpv\"],   g_std.loc[K, \"cov_minpv\"],   scale=100.0),\n",
    "            \"Fisher Cov\":  _fmt_mean_std(g_mean.loc[K, \"cov_fisher\"],  g_std.loc[K, \"cov_fisher\"],  scale=100.0),\n",
    "            \"AdjF Cov\":    _fmt_mean_std(g_mean.loc[K, \"cov_adjF\"],    g_std.loc[K, \"cov_adjF\"],    scale=100.0),\n",
    "            \"WAvgL Cov\":   _fmt_mean_std(g_mean.loc[K, \"cov_wavgl\"],   g_std.loc[K, \"cov_wavgl\"],   scale=100.0),\n",
    "            \"CF Set\":      _fmt_mean_std(g_mean.loc[K, \"size_cf\"],     g_std.loc[K, \"size_cf\"],     scale=1.0),\n",
    "            \"MinPV Set\":   _fmt_mean_std(g_mean.loc[K, \"size_minpv\"],  g_std.loc[K, \"size_minpv\"],  scale=1.0),\n",
    "            \"Fisher Set\":  _fmt_mean_std(g_mean.loc[K, \"size_fisher\"], g_std.loc[K, \"size_fisher\"], scale=1.0),\n",
    "            \"AdjF Set\":    _fmt_mean_std(g_mean.loc[K, \"size_adjF\"],   g_std.loc[K, \"size_adjF\"],   scale=1.0),\n",
    "            \"WAvgL Set\":   _fmt_mean_std(g_mean.loc[K, \"size_wavgl\"],  g_std.loc[K, \"size_wavgl\"],  scale=1.0),\n",
    "        }\n",
    "        rows.append(r)\n",
    "\n",
    "    table_df = pd.DataFrame(rows, columns=[\n",
    "        \"K\", \"CF Cov\", \"MinPV Cov\", \"Fisher Cov\", \"AdjF Cov\", \"WAvgL Cov\",\n",
    "        \"CF Set\", \"MinPV Set\", \"Fisher Set\", \"AdjF Set\", \"WAvgL Set\"\n",
    "    ])\n",
    "\n",
    "    latex = table_df.to_latex(\n",
    "        index=False,\n",
    "        escape=False,\n",
    "        column_format=\"r\" + \"l\"*10,\n",
    "        booktabs=True,\n",
    "    )\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(latex)\n",
    "\n",
    "    return latex\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main experiment (same experiment logic; now matches first code's model+preproc)\n",
    "# =============================================================================\n",
    "\n",
    "def run_matched(cfg: MatchedCIFARConfig) -> pd.DataFrame:\n",
    "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "    rows = []\n",
    "\n",
    "    pin = bool(torch.cuda.is_available())\n",
    "\n",
    "    for sim in range(cfg.num_simulations):\n",
    "        print(f\"\\n=== Simulation {sim+1}/{cfg.num_simulations} ===\")\n",
    "        seed = cfg.train_seed_base + sim\n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        # EXACT split structure (match first code)\n",
    "        X_trP, X_tmp, y_trP, y_tmp = train_test_split(\n",
    "            X_train_full,\n",
    "            Y_train_full,\n",
    "            test_size=1 - cfg.train_frac,\n",
    "            stratify=Y_train_full,\n",
    "            random_state=seed,\n",
    "        )\n",
    "        X_cal, X_rest, y_cal, y_rest = train_test_split(\n",
    "            X_tmp,\n",
    "            y_tmp,\n",
    "            test_size=1 - cfg.cal_frac_of_temp,\n",
    "            stratify=y_tmp,\n",
    "            random_state=seed,\n",
    "        )\n",
    "        X_fuse_tr_full, X_fuse_cal, y_fuse_tr_full, y_fuse_cal = train_test_split(\n",
    "            X_rest,\n",
    "            y_rest,\n",
    "            test_size=1 - cfg.fuse_train_frac_of_rest,\n",
    "            stratify=y_rest,\n",
    "            random_state=seed,\n",
    "        )\n",
    "\n",
    "        X_te, y_te = X_test_full, Y_test_full\n",
    "\n",
    "        for K in cfg.Ks:\n",
    "            print(f\"\\n  -> K = {K}\")\n",
    "            num_views = 4 if K == 4 else K\n",
    "\n",
    "            # loaders per view (MATCH first code dataloader knobs)\n",
    "            loaders = {}\n",
    "            for v in range(num_views):\n",
    "                tr_loader = torch.utils.data.DataLoader(\n",
    "                    PatchesDataset(\n",
    "                        X_trP, y_trP, K, v,\n",
    "                        normalization=cfg.normalization,\n",
    "                        full_image_size=cfg.full_image_size,\n",
    "                    ),\n",
    "                    batch_size=cfg.batch_size,\n",
    "                    shuffle=True,\n",
    "                    num_workers=cfg.num_workers,\n",
    "                    pin_memory=pin,\n",
    "                    persistent_workers=(cfg.num_workers > 0),\n",
    "                )\n",
    "                cal_loader = torch.utils.data.DataLoader(\n",
    "                    PatchesDataset(\n",
    "                        X_cal, y_cal, K, v,\n",
    "                        normalization=cfg.normalization,\n",
    "                        full_image_size=cfg.full_image_size,\n",
    "                    ),\n",
    "                    batch_size=cfg.batch_size,\n",
    "                    shuffle=False,\n",
    "                    num_workers=cfg.num_workers,\n",
    "                    pin_memory=pin,\n",
    "                    persistent_workers=(cfg.num_workers > 0),\n",
    "                )\n",
    "                ftr_loader = torch.utils.data.DataLoader(\n",
    "                    PatchesDataset(\n",
    "                        X_fuse_tr_full, y_fuse_tr_full, K, v,\n",
    "                        normalization=cfg.normalization,\n",
    "                        full_image_size=cfg.full_image_size,\n",
    "                    ),\n",
    "                    batch_size=cfg.batch_size,\n",
    "                    shuffle=False,\n",
    "                    num_workers=cfg.num_workers,\n",
    "                    pin_memory=pin,\n",
    "                    persistent_workers=(cfg.num_workers > 0),\n",
    "                )\n",
    "                fcal_loader = torch.utils.data.DataLoader(\n",
    "                    PatchesDataset(\n",
    "                        X_fuse_cal, y_fuse_cal, K, v,\n",
    "                        normalization=cfg.normalization,\n",
    "                        full_image_size=cfg.full_image_size,\n",
    "                    ),\n",
    "                    batch_size=cfg.batch_size,\n",
    "                    shuffle=False,\n",
    "                    num_workers=cfg.num_workers,\n",
    "                    pin_memory=pin,\n",
    "                    persistent_workers=(cfg.num_workers > 0),\n",
    "                )\n",
    "                te_loader = torch.utils.data.DataLoader(\n",
    "                    PatchesDataset(\n",
    "                        X_te, y_te, K, v,\n",
    "                        normalization=cfg.normalization,\n",
    "                        full_image_size=cfg.full_image_size,\n",
    "                    ),\n",
    "                    batch_size=cfg.batch_size,\n",
    "                    shuffle=False,\n",
    "                    num_workers=cfg.num_workers,\n",
    "                    pin_memory=pin,\n",
    "                    persistent_workers=(cfg.num_workers > 0),\n",
    "                )\n",
    "                loaders[v] = dict(train=tr_loader, cal=cal_loader, ftr=ftr_loader, fcal=fcal_loader, te=te_loader)\n",
    "\n",
    "            # Train per-view predictors\n",
    "            models: List[PredictorCNN] = []\n",
    "            cal_classwise: List[Dict[int, np.ndarray]] = []\n",
    "            for v in range(num_views):\n",
    "                print(f\"    [View {v+1}/{num_views}] training ResNet-18...\")\n",
    "                m = PredictorCNN(num_classes=cfg.num_classes, pretrained=cfg.pretrained_backbone)\n",
    "                m = train_model(m, loaders[v][\"train\"], num_epochs=cfg.epochs_per_view, lr=cfg.lr)\n",
    "                models.append(m)\n",
    "\n",
    "                sc, lab = compute_nonconformity_scores(m, loaders[v][\"cal\"])\n",
    "                cal_classwise.append(classwise_scores(sc, lab, cfg.num_classes))\n",
    "\n",
    "            # Per-view p/probs on fusion-train FULL, fusion-cal, and test\n",
    "            pv_tr_full, pr_tr_full = [], []\n",
    "            pv_fcal,    pr_fcal    = [], []\n",
    "            pv_te,      pr_te      = [], []\n",
    "\n",
    "            for v in range(num_views):\n",
    "                p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"ftr\"], cfg.num_classes)\n",
    "                pv_tr_full.append(p); pr_tr_full.append(pr)\n",
    "\n",
    "                p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"fcal\"], cfg.num_classes)\n",
    "                pv_fcal.append(p); pr_fcal.append(pr)\n",
    "\n",
    "                p, pr = per_view_pvalues_and_probs(models[v], cal_classwise[v], loaders[v][\"te\"], cfg.num_classes)\n",
    "                pv_te.append(p); pr_te.append(pr)\n",
    "\n",
    "            P_test = np.stack(pv_te, axis=0).astype(np.float32)  # (num_views, n_test, L)\n",
    "\n",
    "            # Fixed features for CF (LR) on fusion-cal and test\n",
    "            X_fcal_feat = build_fusion_features(pv_fcal, pr_fcal)\n",
    "            X_test_feat = build_fusion_features(pv_te, pr_te)\n",
    "\n",
    "            n_t_full = len(y_fuse_tr_full)\n",
    "\n",
    "            # free GPU (same idea as first code)\n",
    "            for m in models:\n",
    "                m.to(\"cpu\")\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            for frac in cfg.nt_fracs:\n",
    "                n_sub = int(round(frac * n_t_full))\n",
    "                idx_sub = stratified_subsample_indices(\n",
    "                    y_fuse_tr_full, n_sub,\n",
    "                    seed=seed + int(frac * 1000)\n",
    "                )\n",
    "\n",
    "                y_tr_sub = y_fuse_tr_full[idx_sub]\n",
    "                pv_tr_sub = [pv[idx_sub] for pv in pv_tr_full]\n",
    "                pr_tr_sub = [pr[idx_sub] for pr in pr_tr_full]\n",
    "\n",
    "                # =========================\n",
    "                # CF (LR on [p;prob]) + stage-3 recalibration\n",
    "                # =========================\n",
    "                X_ftr_sub = build_fusion_features(pv_tr_sub, pr_tr_sub)\n",
    "\n",
    "                fusion_lr = LogisticRegression(\n",
    "                    max_iter=cfg.max_iter_lr,\n",
    "                    multi_class=\"multinomial\",\n",
    "                    solver=\"lbfgs\",\n",
    "                    random_state=seed,\n",
    "                )\n",
    "                fusion_lr.fit(X_ftr_sub, y_tr_sub)\n",
    "\n",
    "                fused_probs_cal = fusion_lr.predict_proba(X_fcal_feat)\n",
    "                fused_cal_scores = fused_class_cal_scores(y_fuse_cal, fused_probs_cal, cfg.num_classes)\n",
    "\n",
    "                fused_probs_test = fusion_lr.predict_proba(X_test_feat)\n",
    "                P_cf = fused_p_values_from_cal(fused_probs_test, fused_cal_scores)\n",
    "                cov_cf, size_cf = evaluate_sets(P_cf, y_te, cfg.alpha)\n",
    "\n",
    "                # =========================\n",
    "                # MinPV (Bonferroni min-p): min(1, K * min_k p_k)\n",
    "                # =========================\n",
    "                P_min = np.min(P_test, axis=0)  # (n_test, L)\n",
    "                P_minpv = np.minimum(1.0, float(num_views) * P_min)\n",
    "                cov_minpv, size_minpv = evaluate_sets(P_minpv, y_te, cfg.alpha)\n",
    "\n",
    "                # =========================\n",
    "                # Fisher (standard)\n",
    "                # =========================\n",
    "                P_fisher = fisher_fusion(P_test)\n",
    "                cov_fisher, size_fisher = evaluate_sets(P_fisher, y_te, cfg.alpha)\n",
    "\n",
    "                # =========================\n",
    "                # AdjF (moment-matched Fisher baseline) — depends on subset\n",
    "                # =========================\n",
    "                P_train_sub = np.stack(pv_tr_sub, axis=0).astype(np.float32)  # (num_views, n_sub, L)\n",
    "                P_adjF = adjusted_fisher_fusion(P_train_sub, y_tr_sub, P_test, cfg.num_classes)\n",
    "                cov_adjF, size_adjF = evaluate_sets(P_adjF, y_te, cfg.alpha)\n",
    "\n",
    "                # =========================\n",
    "                # WAvgL (UPDATED): learn weights from p-values only, then weighted avg of p-values\n",
    "                # =========================\n",
    "                pv_tr_concat = np.concatenate(pv_tr_sub, axis=1).astype(np.float32)  # (n_sub, num_views*L)\n",
    "                w = learn_view_weights_from_pvals(\n",
    "                    pv_train_concat=pv_tr_concat,\n",
    "                    y_train=y_tr_sub,\n",
    "                    K=num_views,\n",
    "                    L=cfg.num_classes,\n",
    "                    max_iter=cfg.max_iter_lr,\n",
    "                    seed=seed + 777 + int(frac * 1000),\n",
    "                )\n",
    "                P_wavgL = weighted_average_fusion(P_test, w)  # (n_test, L)\n",
    "                P_wavgL = np.clip(P_wavgL, 0.0, 1.0)\n",
    "                cov_wavgL, size_wavgL = evaluate_sets(P_wavgL, y_te, cfg.alpha)\n",
    "\n",
    "                rows.append(dict(\n",
    "                    sim=int(sim),\n",
    "                    seed=int(seed),\n",
    "                    K=int(K),\n",
    "                    alpha=float(cfg.alpha),\n",
    "                    nt_frac=float(frac),\n",
    "                    nt_sub=int(len(idx_sub)),\n",
    "                    nt_full=int(n_t_full),\n",
    "\n",
    "                    cov_cf=float(cov_cf),\n",
    "                    size_cf=float(size_cf),\n",
    "\n",
    "                    cov_minpv=float(cov_minpv),\n",
    "                    size_minpv=float(size_minpv),\n",
    "\n",
    "                    cov_fisher=float(cov_fisher),\n",
    "                    size_fisher=float(size_fisher),\n",
    "\n",
    "                    cov_adjF=float(cov_adjF),\n",
    "                    size_adjF=float(size_adjF),\n",
    "\n",
    "                    cov_wavgl=float(cov_wavgL),\n",
    "                    size_wavgl=float(size_wavgL),\n",
    "                ))\n",
    "\n",
    "                print(\n",
    "                    f\"    nt_frac={frac:.2f} (n={len(idx_sub):5d}) | \"\n",
    "                    f\"CF: cov={cov_cf:.3f}, size={size_cf:.2f} | \"\n",
    "                    f\"MinPV: cov={cov_minpv:.3f}, size={size_minpv:.2f} | \"\n",
    "                    f\"Fisher: cov={cov_fisher:.3f}, size={size_fisher:.2f} | \"\n",
    "                    f\"AdjF: cov={cov_adjF:.3f}, size={size_adjF:.2f} | \"\n",
    "                    f\"WAvgL(p): cov={cov_wavgL:.3f}, size={size_wavgL:.2f}\"\n",
    "                )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Save raw rows\n",
    "    out_path = os.path.join(cfg.out_dir, cfg.out_csv)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"\\nSaved raw rows: {out_path}\")\n",
    "\n",
    "    # Save numeric aggregate CSV (mean/std)\n",
    "    agg = df.groupby([\"K\", \"nt_frac\"]).agg(\n",
    "        cov_cf_mean=(\"cov_cf\", \"mean\"), cov_cf_std=(\"cov_cf\", \"std\"),\n",
    "        size_cf_mean=(\"size_cf\", \"mean\"), size_cf_std=(\"size_cf\", \"std\"),\n",
    "\n",
    "        cov_minpv_mean=(\"cov_minpv\", \"mean\"), cov_minpv_std=(\"cov_minpv\", \"std\"),\n",
    "        size_minpv_mean=(\"size_minpv\", \"mean\"), size_minpv_std=(\"size_minpv\", \"std\"),\n",
    "\n",
    "        cov_fisher_mean=(\"cov_fisher\", \"mean\"), cov_fisher_std=(\"cov_fisher\", \"std\"),\n",
    "        size_fisher_mean=(\"size_fisher\", \"mean\"), size_fisher_std=(\"size_fisher\", \"std\"),\n",
    "\n",
    "        cov_adjF_mean=(\"cov_adjF\", \"mean\"), cov_adjF_std=(\"cov_adjF\", \"std\"),\n",
    "        size_adjF_mean=(\"size_adjF\", \"mean\"), size_adjF_std=(\"size_adjF\", \"std\"),\n",
    "\n",
    "        cov_wavgl_mean=(\"cov_wavgl\", \"mean\"), cov_wavgl_std=(\"cov_wavgl\", \"std\"),\n",
    "        size_wavgl_mean=(\"size_wavgl\", \"mean\"), size_wavgl_std=(\"size_wavgl\", \"std\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    agg_path = os.path.join(cfg.out_dir, \"matched_cifar100_agg.csv\")\n",
    "    agg.to_csv(agg_path, index=False)\n",
    "    print(f\"Saved aggregate CSV: {agg_path}\")\n",
    "\n",
    "    # Save + print old-style LaTeX tables (one per nt_frac)\n",
    "    for frac in cfg.nt_fracs:\n",
    "        frac_tag = str(frac).replace(\".\", \"p\")\n",
    "        tex_path = os.path.join(cfg.out_dir, f\"matched_cifar100_table_nt{frac_tag}.tex\")\n",
    "        latex = save_old_style_latex_table(df, tex_path, nt_frac=float(frac))\n",
    "        print(f\"\\nSaved LaTeX table: {tex_path}\\n\")\n",
    "        print(latex)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "cfg = MatchedCIFARConfig(\n",
    "    Ks=(2, 3, 4, 6),\n",
    "    nt_fracs=(1.0,),\n",
    "    # defaults match your FIRST code style\n",
    ")\n",
    "df = run_matched(cfg)\n",
    "print(\"\\nHead:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5d95e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
